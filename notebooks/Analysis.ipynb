{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "\n",
    "- [Links of Interest](#links)\n",
    "\n",
    "- [Part 1. Loading and Filtering Data](#paths)\n",
    "    - [Setting input and output paths](#paths)\n",
    "    - [Reading in datasets of phenotype descriptions](#read_this_data_)\n",
    "    - [Reading in datasets of groupings](#read_other_data) (biochemical pathways, functional groups, etc.)\n",
    "    - [Relating the datasets](#relating)\n",
    "    - [Filtering the datasets](#filtering)\n",
    "    \n",
    "- [Part 2. NLP Models](#word2vec_doc2vec)\n",
    "    - [Word2Vec and Doc2Vec](#word2vec_doc2vec)\n",
    "    - [BERT and BioBERT](#bert_biobert)\n",
    "    - [Loading models](#load_models)\n",
    "\n",
    "- [Part 3. NLP Choices]()\n",
    "    - [Preprocessing descriptions](#preprocessing)\n",
    "    - [POS Tagging](#pos_tagging)\n",
    "    - [Reducing vocabulary size](#vocab)\n",
    "    - [Annotating with biological ontologies](#annotation)\n",
    "    - [Splitting into phene descriptions](#phenes)\n",
    "    \n",
    "- [Part 4. Generating Vectors and Distance Matrices](#matrix)\n",
    "    - [Defining methods to use](#methods)\n",
    "    - [Running all methods](#running)\n",
    "    - [Merging distances into an edgelist](#merging)\n",
    "    - [Adding edge information](#merging)\n",
    "\n",
    "- [Part 5. Clustering Analysis]()\n",
    "    - [Topic modeling](#topic_modeling)\n",
    "    - [Agglomerative clustering](#clustering)\n",
    "    - [Phenologs for OMIM disease phenotypes](#phenologs)\n",
    "    \n",
    "- [Part 6. Supervised Tasks](#supervised)\n",
    "    - [Distributions of distance values](#ks)\n",
    "    - [Within-group distance values](#within)\n",
    "    - [Predictions and AUC for shared pathways or interactions](#auc)\n",
    "    - [Tests for querying to recover related genes](#y)\n",
    "    - [Producing output summary table](#output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction: Text Mining Analysis of Phenotype Descriptions in Plants\n",
    "The purpose of this notebook is to evaluate what can be learned from a natural language processing approach to analyzing free-text descriptions of phenotype descriptions of plants. The approach is to generate pairwise distances matrices between a set of plant phenotype descriptions across different species, sourced from academic papers and online model organism databases. These pairwise distance matrices can be constructed using any vectorization method that can be applied to natural language. In this notebook, we specifically evaluate the use of n-gram and bag-of-words techniques, word and document embedding using Word2Vec and Doc2Vec, context-dependent word-embeddings using BERT and BioBERT, and ontology term annotations with automated annotation tools such as NOBLE Coder.\n",
    "\n",
    "Loading, manipulation, and filtering of the dataset of phenotype descriptions associated with genes across different plant species is largely handled through a Python package created for this purpose called OATS (Ontology Annotation and Text Similarity) which is available [here](https://github.com/irbraun/oats). Preprocessing of the descriptions, mapping the dataset to additional resources such as protein-protein interaction databases and biochemical pathway databases are handled in this notebook using that package as well. In the evaluation of each of these natural language processing approaches to analyzing this dataset of descriptions, we compare performance against a dataset generated through manual annotation of a similar dataset in Oellrich Walls et al. (2015) and against manual annotations with experimentally determined terms from the Gene Ontology (PO) and the Plant Ontology (PO).\n",
    "\n",
    "<a id=\"links\"></a>\n",
    "### Relevant links of interest:\n",
    "- Paper describing comparison of NLP and ontology annotation approaches to curation: [Braun, Lawrence-Dill (2019)](https://doi.org/10.3389/fpls.2019.01629)\n",
    "- Paper describing results of manual phenotype description curation: [Oellrich, Walls et al. (2015](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0053-y)\n",
    "- Plant databases with phenotype description text data available: [TAIR](https://www.arabidopsis.org/), [SGN](https://solgenomics.net/), [MaizeGDB](https://www.maizegdb.org/)\n",
    "- Python package for working with phenotype descriptions: [OATS](https://github.com/irbraun/oats)\n",
    "- Python package used for general NLP functions: [NLTK](https://www.nltk.org/), [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "- Python package used for working with biological ontologies: [Pronto](https://pronto.readthedocs.io/en/latest/)\n",
    "- Python package for loading pretrained BERT models: [PyTorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/)\n",
    "- For BERT Models pretrained on PubMed and PMC: [BioBERT Paper](https://arxiv.org/abs/1901.08746), [BioBERT Models](https://github.com/naver/biobert-pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp, hypergeom\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.datasets.edges import Edges\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.graphs import pairwise as pw\n",
    "from oats.graphs.editing import merge_edgelists, make_undirected, remove_self_loops, subset_edgelist_with_ids\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.graphs.weighting import train_logistic_regression_model, apply_logistic_regression_model\n",
    "from oats.graphs.weighting import train_random_forest_model, apply_random_forest_model\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocabulary_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocabulary_connected_components, reduce_vocabulary_linares_pontes\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "from oats.nlp.preprocess import concatenate_with_bar_delim\n",
    "\n",
    "from _utils import Method\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Loading and Filtering Data\n",
    "<a id=\"paths\"></a>\n",
    "### Setting up the input and output paths and summarizing output table\n",
    "This section defines some constants which are used for creating a uniquely named directory to contain all the outputs from running this instance of this notebook. The naming scheme is based on the time that the notebook is run. The other constants are used for specifying information in the output table about what the topic was for this notebook when it was run, such as looking at KEGG biochemical pathways or STRING protein-protein interaction data some other type of gene function grouping or hierarchy. These values are arbitrary and are just for keeping better notes about what the output of the notebook corresponds to. All the input and output file paths for loading datasets or models are also contained within this cell, so that if anything is moved the directories and file names should only have to be changed at this point and nowhere else further into the notebook. If additional files are added to the notebook cells they should be put here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The summarizing output dictionary has the shape TABLE[method][metric] --> value.\n",
    "NOTEBOOK_TAGS = {\"kegg\":False, \"pmn\":False, \"classes\":False, \"subsets\":True}\n",
    "TOPIC = \"Biochemical Pathways\"\n",
    "DATA = \"Filtered\"\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_filename = \"../data/pickles/text_plus_annotations_dataset.pickle\"                            # The full dataset pickle.\n",
    "kegg_pathways_filename = \"../data/pickles/kegg_pathways.pickle\"                                      # The pathway groupings from KEGG.\n",
    "pmn_pathways_filename = \"../data/pickles/pmn_pathways.pickle\"                                        # The pahway groupings from Plant Metabolic Network.\n",
    "lloyd_subsets_filename = \"../data/pickles/lloyd_subsets.pickle\"                                      # The functional subsets defined by Lloyd and Meinke (2012).\n",
    "lloyd_classes_filename = \"../data/pickles/lloyd_classes.pickle\"                                      # The functional classes defined by Lloyd and Meinke (2012).\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"     # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/pubmed_dbow/doc2vec_2.bin\"                                      # File holding saved Doc2Vec model.\n",
    "doc2vec_wikipedia_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                                     # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                                           # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                                           # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                                                # Jar for NOBLE Coder tool.\n",
    "biobert_pmc_path = \"../gensim/biobert_v1.0_pmc/pytorch_model\"                                        # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_path = \"../gensim/biobert_v1.0_pubmed/pytorch_model\"                                  # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_pmc_path = \"../gensim/biobert_v1.0_pubmed_pmc/pytorch_model\"                          # Path for PyTorch BioBERT model.\n",
    "panther_to_omim_filename = \"../data/orthology_related_files/pantherdb_omim_df.csv\"                   # File with mappings to human orthologs and disease phenotypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_this_data\"></a>\n",
    "### Reading in the dataset of genes and their associated phenotype descriptions and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 30169\n",
      "Number of unique IDs:            30169\n",
      "Number of unique descriptions:   4566\n",
      "Number of unique gene name sets: 30169\n",
      "Number of species represented:   6\n",
      "Number of rows in the dataframe: 5615\n",
      "Number of unique IDs:            5615\n",
      "Number of unique descriptions:   3378\n",
      "Number of unique gene name sets: 5615\n",
      "Number of species represented:   1\n",
      "Number of rows in the dataframe: 200\n",
      "Number of unique IDs:            200\n",
      "Number of unique descriptions:   181\n",
      "Number of unique gene name sets: 200\n",
      "Number of species represented:   1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>description</th>\n",
       "      <th>term_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1147</td>\n",
       "      <td>ath</td>\n",
       "      <td>At3g01460|MBD9|AT3G01460|ATMBD9|methyl-CPG-bin...</td>\n",
       "      <td>Early flowering. Increased branching. Early fl...</td>\n",
       "      <td>GO:0006355|GO:0016573|GO:0010223|GO:0043966|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>4480</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT2G21385|CGLD11|AtCGLD11|BFA3|CONSERVED IN TH...</td>\n",
       "      <td>When grown in soil under low light conditions ...</td>\n",
       "      <td>GO:0009941|GO:0009507|GO:0009570|GO:0008150|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>5480</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G14620|DRM2|DMT7|domains rearranged methylt...</td>\n",
       "      <td>No visible phenotype. Plants show developmenta...</td>\n",
       "      <td>GO:0006306|GO:0016458|GO:0051567|GO:0050832|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>1344</td>\n",
       "      <td>ath</td>\n",
       "      <td>At2g35510|SRO1|AT2G35510|similar to RCD one 1|...</td>\n",
       "      <td>Increased lateral root number. Late flowering....</td>\n",
       "      <td>GO:0009651|GO:0006979|GO:0048573|GO:0006970|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>1625</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g02910|LPA1|AT1G02910|LOW PSII ACCUMULATION...</td>\n",
       "      <td>Dwarf. Pale green leaves. Increased sensitivit...</td>\n",
       "      <td>GO:0010270|GO:0009507|PO:0000013|PO:0000037|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>1802</td>\n",
       "      <td>ath</td>\n",
       "      <td>At4g25470|CBF2|AT4G25470|DREB1C|FTQ4|ATCBF2|C-...</td>\n",
       "      <td>Resistant to drought. Resistant to freezing. R...</td>\n",
       "      <td>GO:0009409|GO:0009631|PO:0000037|PO:0009005|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1246</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g01480|ACS2|At2g22810|ACS4|AT1G01480|AT2G22...</td>\n",
       "      <td>Large cotyledons. Long hypocotyl. Slow growth....</td>\n",
       "      <td>GO:0016847|GO:0009733|GO:0042802|GO:0005515|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>4490</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT2G22670|IAA8|indoleacetic acid-induced prote...</td>\n",
       "      <td>No visible phenotype.</td>\n",
       "      <td>GO:0005634|GO:0045892|GO:0010311|GO:0005737|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>4076</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT1G23030|PUB11|Plant U-Box Protein 11|F19G10....</td>\n",
       "      <td>On control medium, displayed shorter roots tha...</td>\n",
       "      <td>GO:0010200|PO:0000013|PO:0000037|PO:0000230|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>780</td>\n",
       "      <td>ath</td>\n",
       "      <td>At2g01860|EMB975|AT2G01860|EMBRYO DEFECTIVE 97...</td>\n",
       "      <td>Embryo defective-Globular.</td>\n",
       "      <td>GO:0003674|GO:0008150|PO:0000013|PO:0000037|PO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id species                                         gene_names                                        description                                           term_ids\n",
       "1147  1147     ath  At3g01460|MBD9|AT3G01460|ATMBD9|methyl-CPG-bin...  Early flowering. Increased branching. Early fl...  GO:0006355|GO:0016573|GO:0010223|GO:0043966|GO...\n",
       "4480  4480     ath  AT2G21385|CGLD11|AtCGLD11|BFA3|CONSERVED IN TH...  When grown in soil under low light conditions ...  GO:0009941|GO:0009507|GO:0009570|GO:0008150|GO...\n",
       "5480  5480     ath  AT5G14620|DRM2|DMT7|domains rearranged methylt...  No visible phenotype. Plants show developmenta...  GO:0006306|GO:0016458|GO:0051567|GO:0050832|GO...\n",
       "1344  1344     ath  At2g35510|SRO1|AT2G35510|similar to RCD one 1|...  Increased lateral root number. Late flowering....  GO:0009651|GO:0006979|GO:0048573|GO:0006970|GO...\n",
       "1625  1625     ath  At1g02910|LPA1|AT1G02910|LOW PSII ACCUMULATION...  Dwarf. Pale green leaves. Increased sensitivit...  GO:0010270|GO:0009507|PO:0000013|PO:0000037|PO...\n",
       "1802  1802     ath  At4g25470|CBF2|AT4G25470|DREB1C|FTQ4|ATCBF2|C-...  Resistant to drought. Resistant to freezing. R...  GO:0009409|GO:0009631|PO:0000037|PO:0009005|PO...\n",
       "1246  1246     ath  At1g01480|ACS2|At2g22810|ACS4|AT1G01480|AT2G22...  Large cotyledons. Long hypocotyl. Slow growth....  GO:0016847|GO:0009733|GO:0042802|GO:0005515|GO...\n",
       "4490  4490     ath  AT2G22670|IAA8|indoleacetic acid-induced prote...                              No visible phenotype.  GO:0005634|GO:0045892|GO:0010311|GO:0005737|GO...\n",
       "4076  4076     ath  AT1G23030|PUB11|Plant U-Box Protein 11|F19G10....  On control medium, displayed shorter roots tha...  GO:0010200|PO:0000013|PO:0000037|PO:0000230|PO...\n",
       "780    780     ath  At2g01860|EMB975|AT2G01860|EMBRYO DEFECTIVE 97...                         Embryo defective-Globular.  GO:0003674|GO:0008150|PO:0000013|PO:0000037|PO..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_pickle(dataset_filename)\n",
    "dataset.describe()\n",
    "dataset.filter_by_species(\"ath\",\"zma\",\"sly\")\n",
    "dataset.filter_has_description()\n",
    "dataset.filter_has_annotation()\n",
    "dataset.describe()\n",
    "dataset.filter_has_annotation(\"GO\")\n",
    "dataset.filter_has_annotation(\"PO\")\n",
    "dataset.filter_random_k(200)\n",
    "dataset.describe()\n",
    "dataset.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_other_data\"></a>\n",
    "### Reading in the dataset of groupings from KEGG, Plant Metabolic Network, and other hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_id</th>\n",
       "      <th>gene_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>FSM</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB|FSM|OVP|SRF</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>CDR|LIT</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|WAT</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP|SRF</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>CHS</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>NLS|GRS|IST</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>LEF|FSM</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species                         group_id                                         gene_names\n",
       "0     ath                              FSM                         at1g01030|nga3|top1|ngatha\n",
       "1     ath                  EMB|FSM|OVP|SRF    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2     ath                          CDR|LIT             at1g01060|lhy|late elongated hypocotyl\n",
       "3     ath                          IST|WAT   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4     ath                          OVP|SRF                 at1g01280|cyp703a2|cytochrome p450\n",
       "5     ath                              EMB        at1g01370|cenh3|centromere-specific histone\n",
       "6     ath                              CHS  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7     ath                      NLS|GRS|IST  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8     ath                          LEF|FSM                          at1g01510|an|angustifolia\n",
       "9     ath  SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP                              at1g01550|bps1|bypass"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupings_filename = \"\"\n",
    "groupings_filename = kegg_pathways_filename if NOTEBOOK_TAGS[\"kegg\"] else groupings_filename\n",
    "groupings_filename = pmn_pathways_filename if NOTEBOOK_TAGS[\"pmn\"] else groupings_filename\n",
    "groupings_filename = lloyd_subsets_filename if NOTEBOOK_TAGS[\"subsets\"] else groupings_filename\n",
    "groupings_filename = lloyd_classes_filename if NOTEBOOK_TAGS[\"classes\"] else groupings_filename\n",
    "groups = load_from_pickle(groupings_filename)\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "groups.to_csv(os.path.join(OUTPUT_DIR,\"part_1_groupings.csv\"))\n",
    "groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relating\"></a>\n",
    "### Relating the dataset of genes to the dataset of groupings or categories\n",
    "This section generates tables that indicate how the genes present in the dataset were mapped to the defined pathways or groups. This includes a summary table that indicates how many genes by species were succcessfully mapped to atleast one pathway or group, as well as a more detailed table describing how many genes from each species were mapped to each particular pathway or group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "summary = defaultdict(dict)\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "for species in dataset.get_species():\n",
    "    summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "    summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table.loc[\"total\"]= table.sum()\n",
    "table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"species\"}, axis=\"columns\")\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"part_1_mappings_summary.csv\"), index=False)\n",
    "\n",
    "# Generate a table describing how many genes from each species map to which particular group.\n",
    "summary = defaultdict(dict)\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "    summary[group_id][\"total\"] = len([x for x in ids])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table = table.sort_values(by=\"total\", ascending=False)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "table.loc[\"total\"] = table.sum()\n",
    "table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"part_1_mappings_by_group.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>\n",
    "### Option 1: Filtering the dataset based on presence in the curated Oellrich, Walls et al. (2015) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 93\n",
      "Number of unique IDs:            93\n",
      "Number of unique descriptions:   89\n",
      "Number of unique gene name sets: 93\n",
      "Number of species represented:   1\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset based on whether or not the genes were in the curated dataset.\n",
    "# This is similar to filtering based on protein interaction data because the dataset is a list of edge values.\n",
    "pppn_edgelist_path = \"../data/supplemental_files_oellrich_walls/13007_2015_53_MOESM9_ESM.txt\"\n",
    "pppn_edgelist = Edges(dataset.get_name_to_id_dictionary(), pppn_edgelist_path)\n",
    "dataset.filter_with_ids(pppn_edgelist.ids)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Filtering the dataset based on protein-protein interactions\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mentioned atleast one time in the STRING database for a given species. If a gene is not mentioned at all in STRING, there is no information available for whether or not it interacts with any other proteins in the dataset so choose to not include it in the analysis. Only genes that have atleast one true positive are included because these are the only ones for which the missing information (negatives) is meaningful. This should be run instead of the subsequent cell, or the other way around, based on whether or not protein-protein interactions is the prediction goal for the current analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Filter the dataset based on whether or not the genes were successfully mapped to an interaction.\n",
    "# Reduce size of the dataset by removing genes not mentioned in the STRING.\n",
    "naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis thaliana\n",
    "    \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\", # maize\n",
    "    \"../data/group_related_files/string/4530.protein.links.detailed.v11.0.txt\", # tomato \n",
    "    \"../data/group_related_files/string/4081.protein.links.detailed.v11.0.txt\", # medicago\n",
    "    \"../data/group_related_files/string/3880.protein.links.detailed.v11.0.txt\", # rice \n",
    "    \"../data/group_related_files/string/3847.protein.links.detailed.v11.0.txt\", # soybean\n",
    "]\n",
    "genes = dataset.get_gene_dictionary()\n",
    "string_data = String(genes, naming_file, *interaction_files)\n",
    "dataset.filter_with_ids(string_data.ids)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Filtering the dataset based on membership in pathways or phenotype category\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mapped to atleast one pathway in whatever the source of pathway membership we are using is (KEGG, Plant Metabolic Network, etc). This is because for these genes, it will be impossible to correctly predict their pathway membership, and we have no evidence that they belong or do not belong in certain pathways so they can not be identified as being true or false negatives in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 93\n",
      "Number of unique IDs:            93\n",
      "Number of unique descriptions:   89\n",
      "Number of unique gene name sets: 93\n",
      "Number of species represented:   1\n"
     ]
    }
   ],
   "source": [
    "# Filter based on succcessful mappings to groups or pathways.\n",
    "dataset.filter_with_ids(group_mapped_ids)\n",
    "dataset.describe()\n",
    "# Get the mappings in each direction again now that the dataset has been subset.\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. NLP Models\n",
    "\n",
    "\n",
    "<a id=\"word2vec_doc2vec\"></a>\n",
    "### Word2Vec and Doc2Vec\n",
    "Word2Vec is a word embedding technique using a neural network trained on a so-called *false task*, namely either predicting a missing word from within a sequence of context words drawn from a sentence or phrase, or predicting which contexts words surround some given input word drawn from a sentence or phrase. Each of these tasks are supervised (the correct answer is fixed and known), but can be generated from unlabelled text data such as a collection of books or wikipedia articles, meaning that even though the task itself is supervised the training data can be generated automatically, enabling the creation of enormous training sets. The internal representation for particular words learned during the training process contain semantically informative features related to that given word, and can therefore be used as embeddings used downstream for tasks such as finding similarity between words or as input into additional models. Doc2Vec is an extension of this technique that determines vector embeddings for entire documents (strings containing multiple words, could be sentences, paragraphs, or documents).\n",
    "\n",
    "\n",
    "<a id=\"bert_biobert\"></a>\n",
    "### BERT and BioBERT\n",
    "BERT ('Bidirectional Encoder Representations from Transformers') is another neueral network-based model trained on two different false tasks, namely predicting the subsequent sentence given some input sentence, or predicting the identity of a set of words masked from an input sentence. Like Word2Vec, this architecture can be used to generate vector embeddings for a particular input word by extracting values from a subset of the encoder layers that correspond to that input word. Practically, a major difference is that because the input word is input in the context of its surrounding sentence, the embedding reflects the meaning of a particular word in a particular context (such as the difference in the meaning of *root* in the phrases *plant root* and *root of the problem*. BioBERT refers to a set of BERT models which have been finetuned on the PubMed and PMC corpora. See the list of relevant links for the publications and pages associated with these models.\n",
    "\n",
    "<a id=\"load_models\"></a>\n",
    "### Loading trained and saved models\n",
    "Versions of the architectures discussed above which have been saved as trained models are loaded here. Some of these models are loaded as pretrained models from the work of other groups, and some were trained on data specific to this notebook and loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Files and models related to the machine learning text embedding methods used here.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. NLP Choices\n",
    "\n",
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing text descriptions\n",
    "The preprocessing methods applied to the phenotype descriptions are a choice which impacts the subsequent vectorization and similarity methods which construct the pairwise distance matrix from each of these descriptions. The preprocessing methods that make sense are also highly dependent on the vectorization method or embedding method that is to be applied. For example, stemming (which is part of the full proprocessing done below using the Gensim preprocessing function) is useful for the n-grams and bag-of-words methods but not for the document embeddings methods which need each token to be in the vocabulary that was constructed and used when the model was trained. For this reason, embedding methods with pretrained models where the vocabulary is fixed should have a lighter degree of preprocessing not involving stemming or lemmatization but should involve things like removal of non-alphanumerics and normalizing case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain a mapping between IDs and the raw text descriptions associated with that ID from the dataset.\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "\n",
    "# Preprocessing of the text descriptions. Different methods are necessary for different approaches.\n",
    "descriptions_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "descriptions_no_stopwords = {i:remove_stopwords(d) for i,d in descriptions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_tagging\"></a>\n",
    "### POS tagging the phenotype descriptions for nouns and adjectives\n",
    "Note that preprocessing of the descriptions should be done after part-of-speech tagging, because tokens that are removed during preprocessing before n-gram analysis contain information that the parser needs to accurately call parts-of-speech. This step should be done on the raw descriptions and then the resulting bags of words can be subset using additional preprocesssing steps before input in one of the vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "descriptions_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "descriptions_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "descriptions_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_noun_adj = {i:\"{} {}\".format(descriptions_noun_only[i],descriptions_adj_only[i]) for i in descriptions.keys()}\n",
    "descriptions_noun_adj_full_preprocessing = {i:\"{} {}\".format(descriptions_noun_only_full_preprocessing[i],descriptions_adj_only_full_preprocessing[i]) for i in descriptions.keys()}\n",
    "descriptions_noun_adj_simple_preprocessing = {i:\"{} {}\".format(descriptions_noun_only_simple_preprocessing[i],descriptions_adj_only_simple_preprocessing[i]) for i in descriptions.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vocab\"></a>\n",
    "### Reducing the vocabulary size using a word distance matrix\n",
    "These approaches for reducing the vocabulary size of the dataset work by replacing multiple words that occur throughout the dataset of descriptions with an identical word that is representative of this larger group of words. The total number of unique words across all descriptions is therefore reduced, and when observing n-gram overlaps between vector representations of these descriptions, overlaps will now occur between descriptions that included different but similar words. These methods work by actually generating versions of these descriptions that have the word replacements present. The returned objects for these methods are the revised description dictionary, a dictionary mapping tokens in the full vocabulary to tokens in the reduced vocabulary, and a dictionary mapping tokens in the reduced vocabulary to a list of tokens in the full vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reducing the size of the vocabulary for descriptions treated with simple preprocessing.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in descriptions_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "\n",
    "# Make sure that the tokens list is in the same order as the indices representing each word in the distance matrix.\n",
    "# This is only trivial here because the IDs used are ordered integers 0 to n, but this might not always be the case.\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "threshold = 0.2\n",
    "descriptions_linares_pontes, reduce_lp, unreduce_lp = reduce_vocabulary_linares_pontes(descriptions_simple_preprocessing, tokens, distance_matrix, n)\n",
    "descriptions_connected_components, reduce_cc, unreduce_cc = reduce_vocabulary_connected_components(descriptions_simple_preprocessing, tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing vocabulary size based on identifying important words\n",
    "These approcahes for reducing the vocabulary size of the dataset work by identifying which words in the descriptions are likely to be the most important for identifying differences between the phenotypes and meaning of the descriptions. One approach is to determine which words occur at a higher rate in text of interest such as articles about plant phenotypes as compared to their rates in more general texts such as a corpus of news articles. These approaches do not create modified versions of the descriptions but rather provide vocabulary objects that can be passed to the sklearn vectorizer or constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing a vocabulary by looking at what words are overrepresented in domain specific text.\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "tokens = get_overrepresented_tokens(phenotypes_corpus, background_corpus, max_features=5000)\n",
    "vocabulary_from_text = get_vocabulary_from_tokens(tokens)\n",
    "\n",
    "# Constructing a vocabulary by assuming all words present in a given ontology are important.\n",
    "ontology = Ontology(ontology_filename)\n",
    "vocabulary_from_ontology = get_vocabulary_from_tokens(ontology.get_tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annotation\"></a>\n",
    "### Annotating descriptions with ontology terms\n",
    "This section generates dictionaries that map gene IDs from the dataset to lists of strings, where those strings are ontology term IDs. How the term IDs are found for each gene entry with its corresponding phenotype description depends on the cell below. Firstly, the terms are found by using the NOBLE Coder annotation tool through these wrapper functions to identify the terms by looking for instances of the term's label or synonyms in the actual text of the phenotype descriptions. Secondly, the next cell just draws the terms directly from the dataset itself. In this case, these are high-confidence annotations done by curators for a comparison against what can be accomplished through computational analysis of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the ontology term annotators over the raw input text descriptions. NOBLE-Coder handles simple issues like case\n",
    "# normalization so preprocessed descriptions are not used for this step.\n",
    "ontology = Ontology(ontology_filename)\n",
    "annotations_noblecoder_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=1)\n",
    "annotations_noblecoder_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the ID to term list annotation dictionaries for each ontology in the dataset.\n",
    "annotations = dataset.get_annotations_dictionary()\n",
    "go_annotations = {k:[term for term in v if term[0:2]==\"GO\"] for k,v in annotations.items()}\n",
    "po_annotations = {k:[term for term in v if term[0:2]==\"PO\"] for k,v in annotations.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phenes\"></a>\n",
    "### Splitting the descriptions into individual phenes\n",
    "As a preprocessing step, split into a new set of descriptions that's larger. Note that phenotypes are split into phenes, and the phenes that are identical are retained as separate entries in the dataset. This makes the distance matrix calculation more needlessly expensive, because vectors need to be found for the same string more than once, but it simplifies converting the edgelist back to having IDs that reference the genes (full phenotypes) instead of the smaller phenes. If anything, that problem should be addressed in the pairwise functions, not here. (The package should handle it, not when creating input data for those methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of phene descriptions and a dictionary to convert back to the phenotype/gene IDs.\n",
    "phenes = {}\n",
    "phene_id_to_id = {}\n",
    "phene_id = 0\n",
    "for i,phene_list in {i:sent_tokenize(d) for i,d in descriptions.items()}.items():\n",
    "    for phene in phene_list:\n",
    "        phenes[phene_id] = phene\n",
    "        phene_id_to_id[phene_id] = i\n",
    "        phene_id = phene_id+1\n",
    "        \n",
    "# Repeating the reprocessing options for the individual phenes instead of the full phenotype descriptions.\n",
    "phenes_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes.items()}\n",
    "phenes_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes.items()}\n",
    "phenes_no_stopwords = {i:remove_stopwords(d) for i,d in phenes.items()}\n",
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "phenes_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in phenes.items()}\n",
    "phenes_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes_noun_only.items()}\n",
    "phenes_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes_noun_only.items()}\n",
    "phenes_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in phenes.items()}\n",
    "phenes_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes_adj_only.items()}\n",
    "phenes_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes_adj_only.items()}\n",
    "phenes_noun_adj = {i:\"{} {}\".format(phenes_noun_only[i],phenes_adj_only[i]) for i in phenes.keys()}\n",
    "phenes_noun_adj_full_preprocessing = {i:\"{} {}\".format(phenes_noun_only_full_preprocessing[i],phenes_adj_only_full_preprocessing[i]) for i in phenes.keys()}\n",
    "phenes_noun_adj_simple_preprocessing = {i:\"{} {}\".format(phenes_noun_only_simple_preprocessing[i],phenes_adj_only_simple_preprocessing[i]) for i in phenes.keys()}\n",
    "\n",
    "# Repeating the vocbulary reduction step using the individual phenes instead of full phenotype descriptions.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in phenes_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "threshold = 0.2\n",
    "phenes_linares_pontes, reduce_lp, unreduce_lp = reduce_vocabulary_linares_pontes(phenes_simple_preprocessing, tokens, distance_matrix, n)\n",
    "phenes_connected_components, reduce_cc, unreduce_cc = reduce_vocabulary_connected_components(phenes_simple_preprocessing, tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matrix\"></a>\n",
    "# Part 4. Generating vector representations and pairwise distances matrices\n",
    "This section uses the text descriptions, preprocessed text descriptions, or ontology term annotations created or read in the previous sections to generate a vector representation for each gene and build a pairwise distance matrix for the whole dataset. Each method specified is a unique combination of a method of vectorization (bag-of-words, n-grams, document embedding model, etc) and distance metric (Euclidean, Jaccard, cosine, etc) applied to those vectors in constructing the pairwise matrix. The method of vectorization here is equivalent to feature selection, so the task is to figure out which type of vectors will encode features that are useful (n-grams, full words, only words from a certain vocabulary, etc).\n",
    "\n",
    "<a id=\"methods\"></a>\n",
    "### Specifying a list of NLP methods to use\n",
    "Something here if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = [\n",
    "\n",
    "    # Full phenotype descriptions\n",
    "\n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    Method(\"Doc2Vec\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Doc2Vec\", \"PubMed,Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "\n",
    "    #Method(\"BERT\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    Method(\"BERT\", \" Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach selecting for specific parts-of-speech (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with a reduced vocabulary size and simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,Linares Pontes,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,Linares Pontes,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,Linares Pontes,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "\n",
    "\n",
    "    # Methods that use terms inferred from automated annotation of the text.\n",
    "    Method(\"NOBLE Coder\", \"Precise\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"NOBLE Coder\", \"Partial\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"NOBLE Coder\", \"Precise,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use terms assigned by humans that are present in the dataset.\n",
    "    Method(\"GO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":go_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"PO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":po_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Individual phenes from those larger phenotypes.\n",
    "\n",
    "    # Approaches were the phenotype descriptions were split into individual phenes first (computationally expensive).\n",
    "    #Method(\"Doc2Vec (Phenes)\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"Doc2Vec (Phenes)\", \"PubMed,Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with full preprocessing (includes stemming).\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with simple preprocessing (no stemming).\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach selecting for specific parts-of-speech (includes stemming).\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with a reduced vocabulary size and simple preprocessing (no stemming).\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,Linares Pontes,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,Linares Pontes,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,Linares Pontes,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Full,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running\"></a>\n",
    "### Running all of the methods to generate distance matrices\n",
    "Something here if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec:Wikipedia,Size=300                                   00:00:00\n",
      "Doc2Vec:PubMed,Size=100                                      00:00:00\n",
      "Word2Vec:Wikipedia,Size=300,Mean                             00:00:00\n",
      "Word2Vec:Wikipedia,Size=300,Max                              00:00:00\n",
      "BERT: Base:Layers=4,Concatenated                             00:00:25\n",
      "BioBERT:PubMed,PMC,Layers=4,Concatenated                     00:00:26\n",
      "N-Grams:Full,Words,1-grams,2-grams                           00:00:00\n",
      "N-Grams:Full,Words,1-grams,2-grams,Binary                    00:00:00\n",
      "N-Grams:Full,Words,1-grams                                   00:00:00\n",
      "N-Grams:Full,Words,1-grams,Binary                            00:00:00\n",
      "N-Grams:Full,Words,1-grams,2-grams,TFIDF                     00:00:00\n",
      "N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF              00:00:00\n",
      "N-Grams:Full,Words,1-grams,TFIDF                             00:00:00\n",
      "N-Grams:Full,Words,1-grams,Binary,TFIDF                      00:00:00\n",
      "N-Grams:Simple,Words,1-grams,2-grams                         00:00:00\n",
      "N-Grams:Simple,Words,1-grams,2-grams,Binary                  00:00:00\n",
      "N-Grams:Simple,Words,1-grams                                 00:00:00\n",
      "N-Grams:Simple,Words,1-grams,Binary                          00:00:00\n",
      "N-Grams:Simple,Words,1-grams,2-grams,TFIDF                   00:00:00\n",
      "N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF            00:00:00\n",
      "N-Grams:Simple,Words,1-grams,TFIDF                           00:00:00\n",
      "N-Grams:Simple,Words,1-grams,Binary,TFIDF                    00:00:00\n",
      "N-Grams:Full,Nouns,1-grams                                   00:00:00\n",
      "N-Grams:Full,Nouns,1-grams,Binary                            00:00:00\n",
      "N-Grams:Full,Nouns,1-grams,TFIDF                             00:00:00\n",
      "N-Grams:Full,Nouns,1-grams,Binary,TFIDF                      00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams                              00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,Binary                       00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,TFIDF                        00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,Binary,TFIDF                 00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams                              00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,Binary                       00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,TFIDF                        00:00:00\n",
      "N-Grams:Full,Adjectives,1-grams,Binary,TFIDF                 00:00:00\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams                    00:00:00\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,Binary             00:00:00\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF              00:00:00\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF       00:00:00\n",
      "NOBLE Coder:Precise                                          00:00:00\n",
      "NOBLE Coder:Partial                                          00:00:00\n",
      "NOBLE Coder:Precise,TFIDF                                    00:00:00\n",
      "NOBLE Coder:Partial,TFIDF                                    00:00:00\n",
      "GO:                                                          00:00:00\n",
      "PO:                                                          00:00:00\n",
      "Word2Vec (Phenes):Wikipedia,Size=300,Mean                    00:00:00\n",
      "Word2Vec (Phenes):Wikipedia,Size=300,Max                     00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Generate all the pairwise distance matrices (not in parallel).\n",
    "graphs = {}\n",
    "names = []\n",
    "durations = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    names.append(method.name_with_hyperparameters)\n",
    "    durations.append(to_hms(duration))\n",
    "    print(\"{:60} {}\".format(method.name_with_hyperparameters,to_hms(duration)))\n",
    "durations_df = pd.DataFrame({\"method\":names,\"duration\":durations})\n",
    "durations_df.to_csv(os.path.join(OUTPUT_DIR,\"part_4_durations.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "### Merging all of the distance matrices into a single dataframe specifying edges\n",
    "This section also handles replacing IDs from the individual methods that are references individual phenes that are part of a larger phenotype, and replacing those IDs with IDs referencing the full phenotypes (one-to-one relationship between phenotypes and genes). In this case, the minimum distance found between any two phenes from those two phenotypes represents the distance between that pair of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Doc2Vec:PubMed,Size=100</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Max</th>\n",
       "      <th>BERT: Base:Layers=4,Concatenated</th>\n",
       "      <th>BioBERT:PubMed,PMC,Layers=4,Concatenated</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>NOBLE Coder:Precise</th>\n",
       "      <th>NOBLE Coder:Partial</th>\n",
       "      <th>NOBLE Coder:Precise,TFIDF</th>\n",
       "      <th>NOBLE Coder:Partial,TFIDF</th>\n",
       "      <th>GO:</th>\n",
       "      <th>PO:</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1147</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.318656</td>\n",
       "      <td>0.783955</td>\n",
       "      <td>0.254395</td>\n",
       "      <td>0.125013</td>\n",
       "      <td>0.180123</td>\n",
       "      <td>0.045611</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.860314</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.947206</td>\n",
       "      <td>0.885984</td>\n",
       "      <td>0.825208</td>\n",
       "      <td>0.939746</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.898071</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.958517</td>\n",
       "      <td>0.955076</td>\n",
       "      <td>0.901777</td>\n",
       "      <td>0.854852</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831901</td>\n",
       "      <td>0.863162</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.832013</td>\n",
       "      <td>0.817598</td>\n",
       "      <td>0.575666</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.765539</td>\n",
       "      <td>0.767976</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.566112</td>\n",
       "      <td>0.454574</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.127756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.396891</td>\n",
       "      <td>0.579636</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>0.081193</td>\n",
       "      <td>0.089893</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>0.688221</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.603559</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.859334</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>0.766539</td>\n",
       "      <td>0.871447</td>\n",
       "      <td>0.495387</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.391184</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.726561</td>\n",
       "      <td>0.901916</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.856017</td>\n",
       "      <td>0.908713</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.918856</td>\n",
       "      <td>0.874575</td>\n",
       "      <td>0.425402</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.639406</td>\n",
       "      <td>0.860050</td>\n",
       "      <td>0.204231</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.503658</td>\n",
       "      <td>0.824208</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.801673</td>\n",
       "      <td>0.778646</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.077525</td>\n",
       "      <td>0.035615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1147</td>\n",
       "      <td>1802</td>\n",
       "      <td>0.410111</td>\n",
       "      <td>0.310422</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.104128</td>\n",
       "      <td>0.171168</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.853905</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.788815</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.897332</td>\n",
       "      <td>0.834516</td>\n",
       "      <td>0.781316</td>\n",
       "      <td>0.912277</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.939530</td>\n",
       "      <td>0.950194</td>\n",
       "      <td>0.860583</td>\n",
       "      <td>0.832986</td>\n",
       "      <td>0.894591</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.871852</td>\n",
       "      <td>0.831785</td>\n",
       "      <td>0.946162</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.873567</td>\n",
       "      <td>0.327407</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.786568</td>\n",
       "      <td>0.853335</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.201439</td>\n",
       "      <td>0.198501</td>\n",
       "      <td>0.123670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1147</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.296903</td>\n",
       "      <td>0.361018</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.088694</td>\n",
       "      <td>0.141595</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.784613</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.702634</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.880703</td>\n",
       "      <td>0.942025</td>\n",
       "      <td>0.786124</td>\n",
       "      <td>0.874559</td>\n",
       "      <td>0.827566</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.761716</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.911975</td>\n",
       "      <td>0.948572</td>\n",
       "      <td>0.846235</td>\n",
       "      <td>0.886290</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>0.865594</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.873753</td>\n",
       "      <td>0.391990</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.650102</td>\n",
       "      <td>0.841460</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.686311</td>\n",
       "      <td>0.707151</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.265734</td>\n",
       "      <td>0.165720</td>\n",
       "      <td>0.127851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>780</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.454307</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.593264</td>\n",
       "      <td>0.806261</td>\n",
       "      <td>0.249255</td>\n",
       "      <td>0.083882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.580809</td>\n",
       "      <td>0.734558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1147</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.347951</td>\n",
       "      <td>0.537185</td>\n",
       "      <td>0.111751</td>\n",
       "      <td>0.081102</td>\n",
       "      <td>0.117849</td>\n",
       "      <td>0.038699</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.812342</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.865344</td>\n",
       "      <td>0.877684</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.917632</td>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>0.879322</td>\n",
       "      <td>0.919678</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>0.919291</td>\n",
       "      <td>0.732397</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.839397</td>\n",
       "      <td>0.867478</td>\n",
       "      <td>0.327220</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.657274</td>\n",
       "      <td>0.857608</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.680644</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.117356</td>\n",
       "      <td>0.104615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>624</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.315342</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.088554</td>\n",
       "      <td>0.115708</td>\n",
       "      <td>0.042031</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.904363</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.994146</td>\n",
       "      <td>0.943096</td>\n",
       "      <td>0.980544</td>\n",
       "      <td>0.876983</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.801708</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.954288</td>\n",
       "      <td>0.986565</td>\n",
       "      <td>0.893232</td>\n",
       "      <td>0.955711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.886071</td>\n",
       "      <td>0.968397</td>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.759421</td>\n",
       "      <td>0.975639</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.821110</td>\n",
       "      <td>0.787983</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.118129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>466</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.562618</td>\n",
       "      <td>0.152418</td>\n",
       "      <td>0.079348</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.877468</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.952113</td>\n",
       "      <td>0.973533</td>\n",
       "      <td>0.909717</td>\n",
       "      <td>0.933976</td>\n",
       "      <td>0.807824</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.721499</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.932896</td>\n",
       "      <td>0.968816</td>\n",
       "      <td>0.875956</td>\n",
       "      <td>0.928069</td>\n",
       "      <td>0.854905</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.909442</td>\n",
       "      <td>0.938419</td>\n",
       "      <td>0.713921</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.884276</td>\n",
       "      <td>0.379826</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.755266</td>\n",
       "      <td>0.887271</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.728723</td>\n",
       "      <td>0.771921</td>\n",
       "      <td>0.700281</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.110231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1147</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.294981</td>\n",
       "      <td>0.686566</td>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.285016</td>\n",
       "      <td>0.098114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.788820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1147</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.375218</td>\n",
       "      <td>0.423950</td>\n",
       "      <td>0.254019</td>\n",
       "      <td>0.390414</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691393</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.976124</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.423548</td>\n",
       "      <td>0.249664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>883</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.408397</td>\n",
       "      <td>0.608083</td>\n",
       "      <td>0.429039</td>\n",
       "      <td>0.218154</td>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.064519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973909</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.954825</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.993808</td>\n",
       "      <td>0.992422</td>\n",
       "      <td>0.984967</td>\n",
       "      <td>0.977448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.748024</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.924830</td>\n",
       "      <td>0.982862</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.672499</td>\n",
       "      <td>0.643330</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.394503</td>\n",
       "      <td>0.252882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1147</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.508925</td>\n",
       "      <td>0.820118</td>\n",
       "      <td>0.325950</td>\n",
       "      <td>0.184269</td>\n",
       "      <td>0.241163</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475858</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.909557</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.306003</td>\n",
       "      <td>0.278729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1147</td>\n",
       "      <td>1705</td>\n",
       "      <td>0.339283</td>\n",
       "      <td>0.619024</td>\n",
       "      <td>0.205217</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.134488</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.884337</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.866815</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.948515</td>\n",
       "      <td>0.967754</td>\n",
       "      <td>0.916412</td>\n",
       "      <td>0.934309</td>\n",
       "      <td>0.821616</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.922926</td>\n",
       "      <td>0.951583</td>\n",
       "      <td>0.871747</td>\n",
       "      <td>0.920932</td>\n",
       "      <td>0.888197</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.943064</td>\n",
       "      <td>0.935776</td>\n",
       "      <td>0.748522</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.866198</td>\n",
       "      <td>0.914052</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.755659</td>\n",
       "      <td>0.887348</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.673786</td>\n",
       "      <td>0.830268</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.214572</td>\n",
       "      <td>0.169860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>902</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>0.573403</td>\n",
       "      <td>0.224005</td>\n",
       "      <td>0.110161</td>\n",
       "      <td>0.204125</td>\n",
       "      <td>0.062694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942617</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.902410</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.979291</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.978509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.828323</td>\n",
       "      <td>0.988980</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>0.716691</td>\n",
       "      <td>0.634404</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>0.120674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>454</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.380159</td>\n",
       "      <td>0.125680</td>\n",
       "      <td>0.103629</td>\n",
       "      <td>0.081487</td>\n",
       "      <td>0.114439</td>\n",
       "      <td>0.038235</td>\n",
       "      <td>0.803949</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.732409</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.901806</td>\n",
       "      <td>0.952914</td>\n",
       "      <td>0.829198</td>\n",
       "      <td>0.863940</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.944785</td>\n",
       "      <td>0.581539</td>\n",
       "      <td>0.895652</td>\n",
       "      <td>0.846196</td>\n",
       "      <td>0.933957</td>\n",
       "      <td>0.752721</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.802646</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.881487</td>\n",
       "      <td>0.925618</td>\n",
       "      <td>0.741404</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.840375</td>\n",
       "      <td>0.827387</td>\n",
       "      <td>0.307233</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.627167</td>\n",
       "      <td>0.830118</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.755869</td>\n",
       "      <td>0.762962</td>\n",
       "      <td>0.727495</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.043165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.287311</td>\n",
       "      <td>0.355538</td>\n",
       "      <td>0.235820</td>\n",
       "      <td>0.142752</td>\n",
       "      <td>0.186015</td>\n",
       "      <td>0.052153</td>\n",
       "      <td>0.958297</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.930920</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.982215</td>\n",
       "      <td>0.991542</td>\n",
       "      <td>0.957400</td>\n",
       "      <td>0.970776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849812</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.908851</td>\n",
       "      <td>0.880243</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.837452</td>\n",
       "      <td>0.719198</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.270086</td>\n",
       "      <td>0.181986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>660</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.436745</td>\n",
       "      <td>0.541221</td>\n",
       "      <td>0.085788</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.087911</td>\n",
       "      <td>0.028616</td>\n",
       "      <td>0.850592</td>\n",
       "      <td>0.969582</td>\n",
       "      <td>0.781072</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.931657</td>\n",
       "      <td>0.966933</td>\n",
       "      <td>0.862335</td>\n",
       "      <td>0.905503</td>\n",
       "      <td>0.623171</td>\n",
       "      <td>0.956853</td>\n",
       "      <td>0.492667</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.819791</td>\n",
       "      <td>0.944562</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.873924</td>\n",
       "      <td>0.967031</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.982844</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.785701</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.869661</td>\n",
       "      <td>0.897373</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.841979</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.668925</td>\n",
       "      <td>0.748880</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.097587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1147</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.357713</td>\n",
       "      <td>0.367316</td>\n",
       "      <td>0.220788</td>\n",
       "      <td>0.106228</td>\n",
       "      <td>0.118888</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>0.902603</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.843826</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.963296</td>\n",
       "      <td>0.991371</td>\n",
       "      <td>0.918664</td>\n",
       "      <td>0.971693</td>\n",
       "      <td>0.988507</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.981334</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>0.996462</td>\n",
       "      <td>0.994342</td>\n",
       "      <td>0.988787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.671835</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.964374</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.852227</td>\n",
       "      <td>0.709510</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>0.133340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>807</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.416424</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.593264</td>\n",
       "      <td>0.806261</td>\n",
       "      <td>0.266716</td>\n",
       "      <td>0.069120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.580809</td>\n",
       "      <td>0.734558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1147</td>\n",
       "      <td>2147</td>\n",
       "      <td>0.520363</td>\n",
       "      <td>0.592209</td>\n",
       "      <td>0.417538</td>\n",
       "      <td>0.413222</td>\n",
       "      <td>0.320494</td>\n",
       "      <td>0.084118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.563564</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875643</td>\n",
       "      <td>0.974847</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.817317</td>\n",
       "      <td>0.827813</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.411920</td>\n",
       "      <td>0.406676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  Doc2Vec:Wikipedia,Size=300  Doc2Vec:PubMed,Size=100  Word2Vec:Wikipedia,Size=300,Mean  Word2Vec:Wikipedia,Size=300,Max  BERT: Base:Layers=4,Concatenated  BioBERT:PubMed,PMC,Layers=4,Concatenated  N-Grams:Full,Words,1-grams,2-grams  N-Grams:Full,Words,1-grams,2-grams,Binary  N-Grams:Full,Words,1-grams  N-Grams:Full,Words,1-grams,Binary  N-Grams:Full,Words,1-grams,2-grams,TFIDF  N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  N-Grams:Full,Words,1-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,2-grams  N-Grams:Simple,Words,1-grams,2-grams,Binary  N-Grams:Simple,Words,1-grams  N-Grams:Simple,Words,1-grams,Binary  N-Grams:Simple,Words,1-grams,2-grams,TFIDF  N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,TFIDF  N-Grams:Simple,Words,1-grams,Binary,TFIDF  N-Grams:Full,Nouns,1-grams  N-Grams:Full,Nouns,1-grams,Binary  N-Grams:Full,Nouns,1-grams,TFIDF  N-Grams:Full,Nouns,1-grams,Binary,TFIDF  \\\n",
       "1   1147  1344                    0.318656                 0.783955                          0.254395                         0.125013                          0.180123                                  0.045611                            0.916174                                   0.945455                    0.860314                           0.869565                                  0.951049                                         0.947206                          0.885984                                 0.825208                              0.939746                                     0.961538                      0.898071                             0.909091                                    0.958517                                           0.955076                            0.901777                                   0.854852                    0.760954                           0.900000                          0.831901                                 0.863162   \n",
       "2   1147  1625                    0.396891                 0.579636                          0.102970                         0.081193                          0.089893                                  0.024955                            0.688221                                   0.948052                    0.603559                           0.906250                                  0.859334                                         0.943495                          0.766539                                 0.871447                              0.495387                                     0.918803                      0.391184                             0.877778                                    0.726561                                           0.901916                            0.601700                                   0.856017                    0.908713                           0.925926                          0.918856                                 0.874575   \n",
       "3   1147  1802                    0.410111                 0.310422                          0.183713                         0.104128                          0.171168                                  0.043911                            0.853905                                   0.932203                    0.788815                           0.875000                                  0.900980                                         0.897332                          0.834516                                 0.781316                              0.912277                                     0.960784                      0.861057                             0.902439                                    0.939530                                           0.950194                            0.860583                                   0.832986                    0.894591                           0.916667                          0.871852                                 0.831785   \n",
       "4   1147  1246                    0.296903                 0.361018                          0.133517                         0.088694                          0.141595                                  0.045167                            0.784613                                   0.954023                    0.702634                           0.911765                                  0.880703                                         0.942025                          0.786124                                 0.874559                              0.827566                                     0.953488                      0.761716                             0.904762                                    0.911975                                           0.948572                            0.846235                                   0.886290                    0.790754                           0.903226                          0.871879                                 0.865594   \n",
       "5    780  1147                    0.454307                 0.544067                          0.593264                         0.806261                          0.249255                                  0.083882                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "6   1147  1765                    0.347951                 0.537185                          0.111751                         0.081102                          0.117849                                  0.038699                            0.862009                                   0.961538                    0.812342                           0.906977                                  0.924972                                         0.960921                          0.865344                                 0.877684                              0.792145                                     0.959770                      0.706158                             0.900000                                    0.917632                                           0.961583                            0.843510                                   0.879322                    0.919678                           0.944444                          0.946222                                 0.919291   \n",
       "7    624  1147                    0.315342                 0.380642                          0.133949                         0.088554                          0.115708                                  0.042031                            0.940015                                   0.990654                    0.904363                           0.977778                                  0.975409                                         0.994146                          0.943096                                 0.980544                              0.876983                                     0.974684                      0.801708                             0.939394                                    0.954288                                           0.986565                            0.893232                                   0.955711                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "8    466  1147                    0.372745                 0.562618                          0.152418                         0.079348                          0.144270                                  0.042165                            0.877468                                   0.963504                    0.826474                           0.934426                                  0.952113                                         0.973533                          0.909717                                 0.933976                              0.807824                                     0.955882                      0.721499                             0.918605                                    0.932896                                           0.968816                            0.875956                                   0.928069                    0.854905                           0.947368                          0.909442                                 0.938419   \n",
       "9   1147  1850                    0.512427                 0.294981                          0.686566                         0.917984                          0.285016                                  0.098114                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "10  1147  2500                    0.393056                 0.375218                          0.423950                         0.254019                          0.390414                                  0.097964                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "11   883  1147                    0.408397                 0.608083                          0.429039                         0.218154                          0.205298                                  0.064519                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              0.973909                                     0.984848                      0.954825                             0.965517                                    0.993808                                           0.992422                            0.984967                                   0.977448                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "12  1147  2075                    0.508925                 0.820118                          0.325950                         0.184269                          0.241163                                  0.063903                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "13  1147  1705                    0.339283                 0.619024                          0.205217                         0.141522                          0.134488                                  0.032986                            0.884337                                   0.953125                    0.866815                           0.931034                                  0.948515                                         0.967754                          0.916412                                 0.934309                              0.821616                                     0.934783                      0.760954                             0.902439                                    0.922926                                           0.951583                            0.871747                                   0.920932                    0.888197                           0.928571                          0.943064                                 0.935776   \n",
       "14   902  1147                    0.435477                 0.573403                          0.224005                         0.110161                          0.204125                                  0.062694                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              0.942617                                     0.988889                      0.902410                             0.975000                                    0.979291                                           0.993400                            0.948857                                   0.978509                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "15   454  1147                    0.380159                 0.125680                          0.103629                         0.081487                          0.114439                                  0.038235                            0.803949                                   0.954955                    0.732409                           0.891892                                  0.901806                                         0.952914                          0.829198                                 0.863940                              0.683489                                     0.944785                      0.581539                             0.895652                                    0.846196                                           0.933957                            0.752721                                   0.879087                    0.802646                           0.937500                          0.881487                                 0.925618   \n",
       "16    50  1147                    0.287311                 0.355538                          0.235820                         0.142752                          0.186015                                  0.052153                            0.958297                                   0.987342                    0.930920                           0.970588                                  0.982215                                         0.991542                          0.957400                                 0.970776                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "17   660  1147                    0.436745                 0.541221                          0.085788                         0.090799                          0.087911                                  0.028616                            0.850592                                   0.969582                    0.781072                           0.934579                                  0.931657                                         0.966933                          0.862335                                 0.905503                              0.623171                                     0.956853                      0.492667                             0.914474                                    0.819791                                           0.944562                            0.681239                                   0.873924                    0.967031                           0.978723                          0.982844                                 0.973944   \n",
       "18  1147  1971                    0.357713                 0.367316                          0.220788                         0.106228                          0.118888                                  0.030403                            0.902603                                   0.984848                    0.843826                           0.964286                                  0.963296                                         0.991371                          0.918664                                 0.971693                              0.988507                                     0.990099                      0.981334                             0.977273                                    0.997426                                           0.996462                            0.994342                                   0.988787                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "19   807  1147                    0.416424                 0.666386                          0.593264                         0.806261                          0.266716                                  0.069120                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "20  1147  2147                    0.520363                 0.592209                          0.417538                         0.413222                          0.320494                                  0.084118                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "\n",
       "    N-Grams:Full,Adjectives,1-grams  N-Grams:Full,Adjectives,1-grams,Binary  N-Grams:Full,Adjectives,1-grams,TFIDF  N-Grams:Full,Adjectives,1-grams,Binary,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams  N-Grams:Full,Words,Linares Pontes,1-grams,Binary  N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF  NOBLE Coder:Precise  NOBLE Coder:Partial  NOBLE Coder:Precise,TFIDF  NOBLE Coder:Partial,TFIDF       GO:       PO:  Word2Vec (Phenes):Wikipedia,Size=300,Mean  Word2Vec (Phenes):Wikipedia,Size=300,Max  \n",
       "1                          0.811392                                0.882353                               0.832013                                      0.817598                                   0.575666                                          0.814815                                         0.765539                                           0.767976                  0.690476             0.528302                   0.566112                   0.454574  0.866667  0.035714                                   0.127756                                  0.127756  \n",
       "2                          0.425402                                0.894737                               0.639406                                      0.860050                                   0.204231                                          0.868421                                         0.503658                                           0.824208                  0.830508             0.720588                   0.801673                   0.778646  0.923077  0.014815                                   0.077525                                  0.035615  \n",
       "3                          0.946162                                0.947368                               0.919562                                      0.873567                                   0.327407                                          0.843750                                         0.669670                                           0.801484                  0.666667             0.796875                   0.786568                   0.853335  0.923077  0.201439                                   0.198501                                  0.123670  \n",
       "4                          0.844222                                0.916667                               0.885868                                      0.873753                                   0.391990                                          0.869565                                         0.650102                                           0.841460                  0.827586             0.764444                   0.686311                   0.707151  0.937500  0.265734                                   0.165720                                  0.127851  \n",
       "5                          1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  0.714286             0.658824                   0.841851                   0.702436  1.000000  0.014815                                   0.580809                                  0.734558  \n",
       "6                          0.732397                                0.896552                               0.839397                                      0.867478                                   0.327220                                          0.880435                                         0.657274                                           0.857608                  0.769231             0.721311                   0.680644                   0.703234  0.866667  0.741259                                   0.117356                                  0.104615  \n",
       "7                          0.811392                                0.965517                               0.886071                                      0.968397                                   0.418110                                          0.962264                                         0.759421                                           0.975639                  0.873418             0.770950                   0.821110                   0.787983  0.888889  0.050000                                   0.176211                                  0.118129  \n",
       "8                          0.713921                                0.875000                               0.843729                                      0.884276                                   0.379826                                          0.898551                                         0.755266                                           0.887271                  0.837209             0.728723                   0.771921                   0.700281  0.850000  0.000000                                   0.127756                                  0.110231  \n",
       "9                          1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  1.000000             1.000000                   1.000000                   1.000000  0.933333  0.000000                                   0.706191                                  0.788820  \n",
       "10                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.691393                                          0.954545                                         0.929417                                           0.976124                  0.777778             0.866667                   0.880853                   0.881888  0.846154  0.022059                                   0.423548                                  0.249664  \n",
       "11                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.748024                                          0.960000                                         0.924830                                           0.982862                  0.625000             0.626667                   0.672499                   0.643330  0.894737  0.076389                                   0.394503                                  0.252882  \n",
       "12                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.475858                                          0.888889                                         0.789790                                           0.909557                  0.777778             0.868852                   0.880853                   0.890432  0.941176  0.155556                                   0.306003                                  0.278729  \n",
       "13                         0.748522                                0.900000                               0.866198                                      0.914052                                   0.448682                                          0.870968                                         0.755659                                           0.887348                  0.636364             0.800000                   0.673786                   0.830268  0.944444  0.770370                                   0.214572                                  0.169860  \n",
       "14                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.509489                                          0.969697                                         0.828323                                           0.988980                  0.653846             0.628866                   0.716691                   0.634404  0.933333  0.035971                                   0.211065                                  0.120674  \n",
       "15                         0.741404                                0.862745                               0.840375                                      0.827387                                   0.307233                                          0.863636                                         0.627167                                           0.830118                  0.837500             0.755869                   0.762962                   0.727495  0.900000  0.043165                                   0.000000                                  0.000000  \n",
       "16                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.849812                                          0.918919                                         0.908851                                           0.880243                  0.855072             0.718121                   0.837452                   0.719198  0.916667  0.118519                                   0.270086                                  0.181986  \n",
       "17                         0.785701                                0.926471                               0.869661                                      0.897373                                   0.232846                                          0.887850                                         0.534072                                           0.841979                  0.802632             0.783784                   0.668925                   0.748880  0.941176  0.021739                                   0.155945                                  0.097587  \n",
       "18                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.671835                                          0.944444                                         0.809278                                           0.964374                  0.772727             0.655172                   0.852227                   0.709510  0.916667  0.000000                                   0.225963                                  0.133340  \n",
       "19                         1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  0.714286             0.658824                   0.841851                   0.702436  0.846154  0.028777                                   0.580809                                  0.734558  \n",
       "20                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.563564                                          0.954545                                         0.875643                                           0.974847                  0.764706             0.800000                   0.817317                   0.827813  0.928571  0.007353                                   0.411920                                  0.406676  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging all the edgelists together.\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for method in methods}\n",
    "tags_dict = {method.name_with_hyperparameters:method.tag for method in methods}\n",
    "names = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "\n",
    "# Modify the edgelists for the methods that were using a phene split.\n",
    "for name,edgelist in edgelists.items():\n",
    "    # Converting phene IDs back to phenotype (gene) IDs where applicable.\n",
    "    if \"phene\" in tags_dict[name]:\n",
    "        edgelist[\"from\"] = edgelist[\"from\"].map(lambda x: phene_id_to_id[x])\n",
    "        edgelist[\"to\"] = edgelist[\"to\"].map(lambda x: phene_id_to_id[x])\n",
    "        edgelist = edgelist.groupby([\"from\",\"to\"], as_index=False).min()\n",
    "    # Making sure the edges are listed with the nodes sorted consistently.\n",
    "    cond = edgelist[\"from\"] > edgelist[\"to\"]\n",
    "    edgelist.loc[cond, ['from', 'to']] = edgelist.loc[cond, ['to', 'from']].values\n",
    "    edgelists[name] = edgelist\n",
    "\n",
    "# Do the merge step and remove self edges from the full dataframe.\n",
    "df = merge_edgelists(edgelists, default_value=1.000)\n",
    "df = remove_self_loops(df)\n",
    "df[\"from\"] = df[\"from\"].astype(\"int64\")\n",
    "df[\"to\"] = df[\"to\"].astype(\"int64\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "### Combining multiple distances measurements into summarizing distance values\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating indicating how likely is it that two genes share atleast one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Doc2Vec:PubMed,Size=100</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Max</th>\n",
       "      <th>BERT: Base:Layers=4,Concatenated</th>\n",
       "      <th>BioBERT:PubMed,PMC,Layers=4,Concatenated</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>NOBLE Coder:Precise</th>\n",
       "      <th>NOBLE Coder:Partial</th>\n",
       "      <th>NOBLE Coder:Precise,TFIDF</th>\n",
       "      <th>NOBLE Coder:Partial,TFIDF</th>\n",
       "      <th>GO:</th>\n",
       "      <th>PO:</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Max</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1147</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.318656</td>\n",
       "      <td>0.783955</td>\n",
       "      <td>0.254395</td>\n",
       "      <td>0.125013</td>\n",
       "      <td>0.180123</td>\n",
       "      <td>0.045611</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.860314</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.947206</td>\n",
       "      <td>0.885984</td>\n",
       "      <td>0.825208</td>\n",
       "      <td>0.939746</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.898071</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.958517</td>\n",
       "      <td>0.955076</td>\n",
       "      <td>0.901777</td>\n",
       "      <td>0.854852</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831901</td>\n",
       "      <td>0.863162</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.832013</td>\n",
       "      <td>0.817598</td>\n",
       "      <td>0.575666</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.765539</td>\n",
       "      <td>0.767976</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.566112</td>\n",
       "      <td>0.454574</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.192193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.396891</td>\n",
       "      <td>0.579636</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>0.081193</td>\n",
       "      <td>0.089893</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>0.688221</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.603559</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.859334</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>0.766539</td>\n",
       "      <td>0.871447</td>\n",
       "      <td>0.495387</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.391184</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.726561</td>\n",
       "      <td>0.901916</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.856017</td>\n",
       "      <td>0.908713</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.918856</td>\n",
       "      <td>0.874575</td>\n",
       "      <td>0.425402</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.639406</td>\n",
       "      <td>0.860050</td>\n",
       "      <td>0.204231</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.503658</td>\n",
       "      <td>0.824208</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.801673</td>\n",
       "      <td>0.778646</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.077525</td>\n",
       "      <td>0.035615</td>\n",
       "      <td>0.144827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1147</td>\n",
       "      <td>1802</td>\n",
       "      <td>0.410111</td>\n",
       "      <td>0.310422</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.104128</td>\n",
       "      <td>0.171168</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.853905</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.788815</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.897332</td>\n",
       "      <td>0.834516</td>\n",
       "      <td>0.781316</td>\n",
       "      <td>0.912277</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.939530</td>\n",
       "      <td>0.950194</td>\n",
       "      <td>0.860583</td>\n",
       "      <td>0.832986</td>\n",
       "      <td>0.894591</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.871852</td>\n",
       "      <td>0.831785</td>\n",
       "      <td>0.946162</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.873567</td>\n",
       "      <td>0.327407</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.786568</td>\n",
       "      <td>0.853335</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.201439</td>\n",
       "      <td>0.198501</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>0.218268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1147</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.296903</td>\n",
       "      <td>0.361018</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.088694</td>\n",
       "      <td>0.141595</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.784613</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.702634</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.880703</td>\n",
       "      <td>0.942025</td>\n",
       "      <td>0.786124</td>\n",
       "      <td>0.874559</td>\n",
       "      <td>0.827566</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.761716</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.911975</td>\n",
       "      <td>0.948572</td>\n",
       "      <td>0.846235</td>\n",
       "      <td>0.886290</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>0.865594</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.873753</td>\n",
       "      <td>0.391990</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.650102</td>\n",
       "      <td>0.841460</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.686311</td>\n",
       "      <td>0.707151</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.265734</td>\n",
       "      <td>0.165720</td>\n",
       "      <td>0.127851</td>\n",
       "      <td>0.184888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>780</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.454307</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.593264</td>\n",
       "      <td>0.806261</td>\n",
       "      <td>0.249255</td>\n",
       "      <td>0.083882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.580809</td>\n",
       "      <td>0.734558</td>\n",
       "      <td>0.728431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1147</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.347951</td>\n",
       "      <td>0.537185</td>\n",
       "      <td>0.111751</td>\n",
       "      <td>0.081102</td>\n",
       "      <td>0.117849</td>\n",
       "      <td>0.038699</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.812342</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.865344</td>\n",
       "      <td>0.877684</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.917632</td>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>0.879322</td>\n",
       "      <td>0.919678</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>0.919291</td>\n",
       "      <td>0.732397</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.839397</td>\n",
       "      <td>0.867478</td>\n",
       "      <td>0.327220</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.657274</td>\n",
       "      <td>0.857608</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.680644</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.117356</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.190398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>624</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.315342</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.088554</td>\n",
       "      <td>0.115708</td>\n",
       "      <td>0.042031</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.904363</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.994146</td>\n",
       "      <td>0.943096</td>\n",
       "      <td>0.980544</td>\n",
       "      <td>0.876983</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.801708</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.954288</td>\n",
       "      <td>0.986565</td>\n",
       "      <td>0.893232</td>\n",
       "      <td>0.955711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.886071</td>\n",
       "      <td>0.968397</td>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.759421</td>\n",
       "      <td>0.975639</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.821110</td>\n",
       "      <td>0.787983</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.118129</td>\n",
       "      <td>0.359138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>466</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.562618</td>\n",
       "      <td>0.152418</td>\n",
       "      <td>0.079348</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.877468</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.952113</td>\n",
       "      <td>0.973533</td>\n",
       "      <td>0.909717</td>\n",
       "      <td>0.933976</td>\n",
       "      <td>0.807824</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.721499</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.932896</td>\n",
       "      <td>0.968816</td>\n",
       "      <td>0.875956</td>\n",
       "      <td>0.928069</td>\n",
       "      <td>0.854905</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.909442</td>\n",
       "      <td>0.938419</td>\n",
       "      <td>0.713921</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.884276</td>\n",
       "      <td>0.379826</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.755266</td>\n",
       "      <td>0.887271</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.728723</td>\n",
       "      <td>0.771921</td>\n",
       "      <td>0.700281</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>0.211503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1147</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.294981</td>\n",
       "      <td>0.686566</td>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.285016</td>\n",
       "      <td>0.098114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1147</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.375218</td>\n",
       "      <td>0.423950</td>\n",
       "      <td>0.254019</td>\n",
       "      <td>0.390414</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691393</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.976124</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.423548</td>\n",
       "      <td>0.249664</td>\n",
       "      <td>0.669936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>883</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.408397</td>\n",
       "      <td>0.608083</td>\n",
       "      <td>0.429039</td>\n",
       "      <td>0.218154</td>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.064519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973909</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.954825</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.993808</td>\n",
       "      <td>0.992422</td>\n",
       "      <td>0.984967</td>\n",
       "      <td>0.977448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.748024</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.924830</td>\n",
       "      <td>0.982862</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.672499</td>\n",
       "      <td>0.643330</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.394503</td>\n",
       "      <td>0.252882</td>\n",
       "      <td>0.579471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1147</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.508925</td>\n",
       "      <td>0.820118</td>\n",
       "      <td>0.325950</td>\n",
       "      <td>0.184269</td>\n",
       "      <td>0.241163</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475858</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.909557</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.306003</td>\n",
       "      <td>0.278729</td>\n",
       "      <td>0.672783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1147</td>\n",
       "      <td>1705</td>\n",
       "      <td>0.339283</td>\n",
       "      <td>0.619024</td>\n",
       "      <td>0.205217</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.134488</td>\n",
       "      <td>0.032986</td>\n",
       "      <td>0.884337</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.866815</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.948515</td>\n",
       "      <td>0.967754</td>\n",
       "      <td>0.916412</td>\n",
       "      <td>0.934309</td>\n",
       "      <td>0.821616</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.922926</td>\n",
       "      <td>0.951583</td>\n",
       "      <td>0.871747</td>\n",
       "      <td>0.920932</td>\n",
       "      <td>0.888197</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.943064</td>\n",
       "      <td>0.935776</td>\n",
       "      <td>0.748522</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.866198</td>\n",
       "      <td>0.914052</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.755659</td>\n",
       "      <td>0.887348</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.673786</td>\n",
       "      <td>0.830268</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.214572</td>\n",
       "      <td>0.169860</td>\n",
       "      <td>0.254441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>902</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>0.573403</td>\n",
       "      <td>0.224005</td>\n",
       "      <td>0.110161</td>\n",
       "      <td>0.204125</td>\n",
       "      <td>0.062694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942617</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.902410</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.979291</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.948857</td>\n",
       "      <td>0.978509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.828323</td>\n",
       "      <td>0.988980</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>0.716691</td>\n",
       "      <td>0.634404</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>0.120674</td>\n",
       "      <td>0.531295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>454</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.380159</td>\n",
       "      <td>0.125680</td>\n",
       "      <td>0.103629</td>\n",
       "      <td>0.081487</td>\n",
       "      <td>0.114439</td>\n",
       "      <td>0.038235</td>\n",
       "      <td>0.803949</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.732409</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.901806</td>\n",
       "      <td>0.952914</td>\n",
       "      <td>0.829198</td>\n",
       "      <td>0.863940</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.944785</td>\n",
       "      <td>0.581539</td>\n",
       "      <td>0.895652</td>\n",
       "      <td>0.846196</td>\n",
       "      <td>0.933957</td>\n",
       "      <td>0.752721</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.802646</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.881487</td>\n",
       "      <td>0.925618</td>\n",
       "      <td>0.741404</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.840375</td>\n",
       "      <td>0.827387</td>\n",
       "      <td>0.307233</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.627167</td>\n",
       "      <td>0.830118</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.755869</td>\n",
       "      <td>0.762962</td>\n",
       "      <td>0.727495</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.043165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.287311</td>\n",
       "      <td>0.355538</td>\n",
       "      <td>0.235820</td>\n",
       "      <td>0.142752</td>\n",
       "      <td>0.186015</td>\n",
       "      <td>0.052153</td>\n",
       "      <td>0.958297</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.930920</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.982215</td>\n",
       "      <td>0.991542</td>\n",
       "      <td>0.957400</td>\n",
       "      <td>0.970776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849812</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.908851</td>\n",
       "      <td>0.880243</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.837452</td>\n",
       "      <td>0.719198</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.270086</td>\n",
       "      <td>0.181986</td>\n",
       "      <td>0.538461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>660</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.436745</td>\n",
       "      <td>0.541221</td>\n",
       "      <td>0.085788</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.087911</td>\n",
       "      <td>0.028616</td>\n",
       "      <td>0.850592</td>\n",
       "      <td>0.969582</td>\n",
       "      <td>0.781072</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.931657</td>\n",
       "      <td>0.966933</td>\n",
       "      <td>0.862335</td>\n",
       "      <td>0.905503</td>\n",
       "      <td>0.623171</td>\n",
       "      <td>0.956853</td>\n",
       "      <td>0.492667</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.819791</td>\n",
       "      <td>0.944562</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.873924</td>\n",
       "      <td>0.967031</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.982844</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.785701</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.869661</td>\n",
       "      <td>0.897373</td>\n",
       "      <td>0.232846</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.841979</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.668925</td>\n",
       "      <td>0.748880</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.097587</td>\n",
       "      <td>0.197589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1147</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.357713</td>\n",
       "      <td>0.367316</td>\n",
       "      <td>0.220788</td>\n",
       "      <td>0.106228</td>\n",
       "      <td>0.118888</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>0.902603</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.843826</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.963296</td>\n",
       "      <td>0.991371</td>\n",
       "      <td>0.918664</td>\n",
       "      <td>0.971693</td>\n",
       "      <td>0.988507</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.981334</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.997426</td>\n",
       "      <td>0.996462</td>\n",
       "      <td>0.994342</td>\n",
       "      <td>0.988787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.671835</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.964374</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.852227</td>\n",
       "      <td>0.709510</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>0.133340</td>\n",
       "      <td>0.441987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>807</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.416424</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>0.593264</td>\n",
       "      <td>0.806261</td>\n",
       "      <td>0.266716</td>\n",
       "      <td>0.069120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.580809</td>\n",
       "      <td>0.734558</td>\n",
       "      <td>0.710707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1147</td>\n",
       "      <td>2147</td>\n",
       "      <td>0.520363</td>\n",
       "      <td>0.592209</td>\n",
       "      <td>0.417538</td>\n",
       "      <td>0.413222</td>\n",
       "      <td>0.320494</td>\n",
       "      <td>0.084118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.563564</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.875643</td>\n",
       "      <td>0.974847</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.817317</td>\n",
       "      <td>0.827813</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.411920</td>\n",
       "      <td>0.406676</td>\n",
       "      <td>0.687671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  Doc2Vec:Wikipedia,Size=300  Doc2Vec:PubMed,Size=100  Word2Vec:Wikipedia,Size=300,Mean  Word2Vec:Wikipedia,Size=300,Max  BERT: Base:Layers=4,Concatenated  BioBERT:PubMed,PMC,Layers=4,Concatenated  N-Grams:Full,Words,1-grams,2-grams  N-Grams:Full,Words,1-grams,2-grams,Binary  N-Grams:Full,Words,1-grams  N-Grams:Full,Words,1-grams,Binary  N-Grams:Full,Words,1-grams,2-grams,TFIDF  N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  N-Grams:Full,Words,1-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,2-grams  N-Grams:Simple,Words,1-grams,2-grams,Binary  N-Grams:Simple,Words,1-grams  N-Grams:Simple,Words,1-grams,Binary  N-Grams:Simple,Words,1-grams,2-grams,TFIDF  N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,TFIDF  N-Grams:Simple,Words,1-grams,Binary,TFIDF  N-Grams:Full,Nouns,1-grams  N-Grams:Full,Nouns,1-grams,Binary  N-Grams:Full,Nouns,1-grams,TFIDF  N-Grams:Full,Nouns,1-grams,Binary,TFIDF  \\\n",
       "1   1147  1344                    0.318656                 0.783955                          0.254395                         0.125013                          0.180123                                  0.045611                            0.916174                                   0.945455                    0.860314                           0.869565                                  0.951049                                         0.947206                          0.885984                                 0.825208                              0.939746                                     0.961538                      0.898071                             0.909091                                    0.958517                                           0.955076                            0.901777                                   0.854852                    0.760954                           0.900000                          0.831901                                 0.863162   \n",
       "2   1147  1625                    0.396891                 0.579636                          0.102970                         0.081193                          0.089893                                  0.024955                            0.688221                                   0.948052                    0.603559                           0.906250                                  0.859334                                         0.943495                          0.766539                                 0.871447                              0.495387                                     0.918803                      0.391184                             0.877778                                    0.726561                                           0.901916                            0.601700                                   0.856017                    0.908713                           0.925926                          0.918856                                 0.874575   \n",
       "3   1147  1802                    0.410111                 0.310422                          0.183713                         0.104128                          0.171168                                  0.043911                            0.853905                                   0.932203                    0.788815                           0.875000                                  0.900980                                         0.897332                          0.834516                                 0.781316                              0.912277                                     0.960784                      0.861057                             0.902439                                    0.939530                                           0.950194                            0.860583                                   0.832986                    0.894591                           0.916667                          0.871852                                 0.831785   \n",
       "4   1147  1246                    0.296903                 0.361018                          0.133517                         0.088694                          0.141595                                  0.045167                            0.784613                                   0.954023                    0.702634                           0.911765                                  0.880703                                         0.942025                          0.786124                                 0.874559                              0.827566                                     0.953488                      0.761716                             0.904762                                    0.911975                                           0.948572                            0.846235                                   0.886290                    0.790754                           0.903226                          0.871879                                 0.865594   \n",
       "5    780  1147                    0.454307                 0.544067                          0.593264                         0.806261                          0.249255                                  0.083882                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "6   1147  1765                    0.347951                 0.537185                          0.111751                         0.081102                          0.117849                                  0.038699                            0.862009                                   0.961538                    0.812342                           0.906977                                  0.924972                                         0.960921                          0.865344                                 0.877684                              0.792145                                     0.959770                      0.706158                             0.900000                                    0.917632                                           0.961583                            0.843510                                   0.879322                    0.919678                           0.944444                          0.946222                                 0.919291   \n",
       "7    624  1147                    0.315342                 0.380642                          0.133949                         0.088554                          0.115708                                  0.042031                            0.940015                                   0.990654                    0.904363                           0.977778                                  0.975409                                         0.994146                          0.943096                                 0.980544                              0.876983                                     0.974684                      0.801708                             0.939394                                    0.954288                                           0.986565                            0.893232                                   0.955711                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "8    466  1147                    0.372745                 0.562618                          0.152418                         0.079348                          0.144270                                  0.042165                            0.877468                                   0.963504                    0.826474                           0.934426                                  0.952113                                         0.973533                          0.909717                                 0.933976                              0.807824                                     0.955882                      0.721499                             0.918605                                    0.932896                                           0.968816                            0.875956                                   0.928069                    0.854905                           0.947368                          0.909442                                 0.938419   \n",
       "9   1147  1850                    0.512427                 0.294981                          0.686566                         0.917984                          0.285016                                  0.098114                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "10  1147  2500                    0.393056                 0.375218                          0.423950                         0.254019                          0.390414                                  0.097964                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "11   883  1147                    0.408397                 0.608083                          0.429039                         0.218154                          0.205298                                  0.064519                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              0.973909                                     0.984848                      0.954825                             0.965517                                    0.993808                                           0.992422                            0.984967                                   0.977448                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "12  1147  2075                    0.508925                 0.820118                          0.325950                         0.184269                          0.241163                                  0.063903                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "13  1147  1705                    0.339283                 0.619024                          0.205217                         0.141522                          0.134488                                  0.032986                            0.884337                                   0.953125                    0.866815                           0.931034                                  0.948515                                         0.967754                          0.916412                                 0.934309                              0.821616                                     0.934783                      0.760954                             0.902439                                    0.922926                                           0.951583                            0.871747                                   0.920932                    0.888197                           0.928571                          0.943064                                 0.935776   \n",
       "14   902  1147                    0.435477                 0.573403                          0.224005                         0.110161                          0.204125                                  0.062694                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              0.942617                                     0.988889                      0.902410                             0.975000                                    0.979291                                           0.993400                            0.948857                                   0.978509                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "15   454  1147                    0.380159                 0.125680                          0.103629                         0.081487                          0.114439                                  0.038235                            0.803949                                   0.954955                    0.732409                           0.891892                                  0.901806                                         0.952914                          0.829198                                 0.863940                              0.683489                                     0.944785                      0.581539                             0.895652                                    0.846196                                           0.933957                            0.752721                                   0.879087                    0.802646                           0.937500                          0.881487                                 0.925618   \n",
       "16    50  1147                    0.287311                 0.355538                          0.235820                         0.142752                          0.186015                                  0.052153                            0.958297                                   0.987342                    0.930920                           0.970588                                  0.982215                                         0.991542                          0.957400                                 0.970776                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "17   660  1147                    0.436745                 0.541221                          0.085788                         0.090799                          0.087911                                  0.028616                            0.850592                                   0.969582                    0.781072                           0.934579                                  0.931657                                         0.966933                          0.862335                                 0.905503                              0.623171                                     0.956853                      0.492667                             0.914474                                    0.819791                                           0.944562                            0.681239                                   0.873924                    0.967031                           0.978723                          0.982844                                 0.973944   \n",
       "18  1147  1971                    0.357713                 0.367316                          0.220788                         0.106228                          0.118888                                  0.030403                            0.902603                                   0.984848                    0.843826                           0.964286                                  0.963296                                         0.991371                          0.918664                                 0.971693                              0.988507                                     0.990099                      0.981334                             0.977273                                    0.997426                                           0.996462                            0.994342                                   0.988787                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "19   807  1147                    0.416424                 0.666386                          0.593264                         0.806261                          0.266716                                  0.069120                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "20  1147  2147                    0.520363                 0.592209                          0.417538                         0.413222                          0.320494                                  0.084118                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "\n",
       "    N-Grams:Full,Adjectives,1-grams  N-Grams:Full,Adjectives,1-grams,Binary  N-Grams:Full,Adjectives,1-grams,TFIDF  N-Grams:Full,Adjectives,1-grams,Binary,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams  N-Grams:Full,Words,Linares Pontes,1-grams,Binary  N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF  NOBLE Coder:Precise  NOBLE Coder:Partial  NOBLE Coder:Precise,TFIDF  NOBLE Coder:Partial,TFIDF       GO:       PO:  Word2Vec (Phenes):Wikipedia,Size=300,Mean  Word2Vec (Phenes):Wikipedia,Size=300,Max      Mean  \n",
       "1                          0.811392                                0.882353                               0.832013                                      0.817598                                   0.575666                                          0.814815                                         0.765539                                           0.767976                  0.690476             0.528302                   0.566112                   0.454574  0.866667  0.035714                                   0.127756                                  0.127756  0.192193  \n",
       "2                          0.425402                                0.894737                               0.639406                                      0.860050                                   0.204231                                          0.868421                                         0.503658                                           0.824208                  0.830508             0.720588                   0.801673                   0.778646  0.923077  0.014815                                   0.077525                                  0.035615  0.144827  \n",
       "3                          0.946162                                0.947368                               0.919562                                      0.873567                                   0.327407                                          0.843750                                         0.669670                                           0.801484                  0.666667             0.796875                   0.786568                   0.853335  0.923077  0.201439                                   0.198501                                  0.123670  0.218268  \n",
       "4                          0.844222                                0.916667                               0.885868                                      0.873753                                   0.391990                                          0.869565                                         0.650102                                           0.841460                  0.827586             0.764444                   0.686311                   0.707151  0.937500  0.265734                                   0.165720                                  0.127851  0.184888  \n",
       "5                          1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  0.714286             0.658824                   0.841851                   0.702436  1.000000  0.014815                                   0.580809                                  0.734558  0.728431  \n",
       "6                          0.732397                                0.896552                               0.839397                                      0.867478                                   0.327220                                          0.880435                                         0.657274                                           0.857608                  0.769231             0.721311                   0.680644                   0.703234  0.866667  0.741259                                   0.117356                                  0.104615  0.190398  \n",
       "7                          0.811392                                0.965517                               0.886071                                      0.968397                                   0.418110                                          0.962264                                         0.759421                                           0.975639                  0.873418             0.770950                   0.821110                   0.787983  0.888889  0.050000                                   0.176211                                  0.118129  0.359138  \n",
       "8                          0.713921                                0.875000                               0.843729                                      0.884276                                   0.379826                                          0.898551                                         0.755266                                           0.887271                  0.837209             0.728723                   0.771921                   0.700281  0.850000  0.000000                                   0.127756                                  0.110231  0.211503  \n",
       "9                          1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  1.000000             1.000000                   1.000000                   1.000000  0.933333  0.000000                                   0.706191                                  0.788820  0.780118  \n",
       "10                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.691393                                          0.954545                                         0.929417                                           0.976124                  0.777778             0.866667                   0.880853                   0.881888  0.846154  0.022059                                   0.423548                                  0.249664  0.669936  \n",
       "11                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.748024                                          0.960000                                         0.924830                                           0.982862                  0.625000             0.626667                   0.672499                   0.643330  0.894737  0.076389                                   0.394503                                  0.252882  0.579471  \n",
       "12                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.475858                                          0.888889                                         0.789790                                           0.909557                  0.777778             0.868852                   0.880853                   0.890432  0.941176  0.155556                                   0.306003                                  0.278729  0.672783  \n",
       "13                         0.748522                                0.900000                               0.866198                                      0.914052                                   0.448682                                          0.870968                                         0.755659                                           0.887348                  0.636364             0.800000                   0.673786                   0.830268  0.944444  0.770370                                   0.214572                                  0.169860  0.254441  \n",
       "14                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.509489                                          0.969697                                         0.828323                                           0.988980                  0.653846             0.628866                   0.716691                   0.634404  0.933333  0.035971                                   0.211065                                  0.120674  0.531295  \n",
       "15                         0.741404                                0.862745                               0.840375                                      0.827387                                   0.307233                                          0.863636                                         0.627167                                           0.830118                  0.837500             0.755869                   0.762962                   0.727495  0.900000  0.043165                                   0.000000                                  0.000000  0.145142  \n",
       "16                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.849812                                          0.918919                                         0.908851                                           0.880243                  0.855072             0.718121                   0.837452                   0.719198  0.916667  0.118519                                   0.270086                                  0.181986  0.538461  \n",
       "17                         0.785701                                0.926471                               0.869661                                      0.897373                                   0.232846                                          0.887850                                         0.534072                                           0.841979                  0.802632             0.783784                   0.668925                   0.748880  0.941176  0.021739                                   0.155945                                  0.097587  0.197589  \n",
       "18                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.671835                                          0.944444                                         0.809278                                           0.964374                  0.772727             0.655172                   0.852227                   0.709510  0.916667  0.000000                                   0.225963                                  0.133340  0.441987  \n",
       "19                         1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  0.714286             0.658824                   0.841851                   0.702436  0.846154  0.028777                                   0.580809                                  0.734558  0.710707  \n",
       "20                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.563564                                          0.954545                                         0.875643                                           0.974847                  0.764706             0.800000                   0.817317                   0.827813  0.928571  0.007353                                   0.411920                                  0.406676  0.687671  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average distance percentile as a means of combining multiple scores.\n",
    "name = \"Mean\"\n",
    "df[name] = df[names].rank(pct=True).mean(axis=1)\n",
    "names.append(name)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec:Wikipedia,Size=300\n",
      "Doc2Vec:PubMed,Size=100\n",
      "Word2Vec:Wikipedia,Size=300,Mean\n",
      "Word2Vec:Wikipedia,Size=300,Max\n",
      "BERT: Base:Layers=4,Concatenated\n",
      "BioBERT:PubMed,PMC,Layers=4,Concatenated\n",
      "N-Grams:Full,Words,1-grams,2-grams\n",
      "N-Grams:Full,Words,1-grams,2-grams,Binary\n",
      "N-Grams:Full,Words,1-grams\n",
      "N-Grams:Full,Words,1-grams,Binary\n",
      "N-Grams:Full,Words,1-grams,2-grams,TFIDF\n",
      "N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF\n",
      "N-Grams:Full,Words,1-grams,TFIDF\n",
      "N-Grams:Full,Words,1-grams,Binary,TFIDF\n",
      "N-Grams:Simple,Words,1-grams,2-grams\n",
      "N-Grams:Simple,Words,1-grams,2-grams,Binary\n",
      "N-Grams:Simple,Words,1-grams\n",
      "N-Grams:Simple,Words,1-grams,Binary\n",
      "N-Grams:Simple,Words,1-grams,2-grams,TFIDF\n",
      "N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF\n",
      "N-Grams:Simple,Words,1-grams,TFIDF\n",
      "N-Grams:Simple,Words,1-grams,Binary,TFIDF\n",
      "N-Grams:Full,Nouns,1-grams\n",
      "N-Grams:Full,Nouns,1-grams,Binary\n",
      "N-Grams:Full,Nouns,1-grams,TFIDF\n",
      "N-Grams:Full,Nouns,1-grams,Binary,TFIDF\n",
      "N-Grams:Full,Adjectives,1-grams\n",
      "N-Grams:Full,Adjectives,1-grams,Binary\n",
      "N-Grams:Full,Adjectives,1-grams,TFIDF\n",
      "N-Grams:Full,Adjectives,1-grams,Binary,TFIDF\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,Binary\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF\n",
      "N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF\n",
      "NOBLE Coder:Precise\n",
      "NOBLE Coder:Partial\n",
      "NOBLE Coder:Precise,TFIDF\n",
      "NOBLE Coder:Partial,TFIDF\n",
      "GO:\n",
      "PO:\n",
      "Word2Vec (Phenes):Wikipedia,Size=300,Mean\n",
      "Word2Vec (Phenes):Wikipedia,Size=300,Max\n",
      "Mean\n"
     ]
    }
   ],
   "source": [
    "# Normalizing all of the array representations of the graphs so they can be combined. Then this version of the arrays\n",
    "# should be used by any other cells that need all of the arrays, rather than the arrays accessed from the graph\n",
    "# objects. This is necessary for this analysis because some of the graph objects refer to phene datasets not\n",
    "# phenotype datasets.\n",
    "name_to_array = {}\n",
    "ids = list(descriptions.keys())\n",
    "n = len(descriptions)\n",
    "id_to_array_index = {i:idx for idx,i in enumerate(ids)}\n",
    "array_index_to_id = {idx:i for i,idx in id_to_array_index.items()}\n",
    "for name in names:\n",
    "    print(name)\n",
    "    idx = list(df.columns).index(name)+1\n",
    "    arr = np.ones((n, n))\n",
    "    for row in df.itertuples():\n",
    "        arr[id_to_array_index[row[1]]][id_to_array_index[row[2]]] = row[idx]\n",
    "        arr[id_to_array_index[row[2]]][id_to_array_index[row[1]]] = row[idx]\n",
    "    np.fill_diagonal(arr, 0.000) \n",
    "    name_to_array[name] = arr    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cluster_analysis\"></a>\n",
    "# Part 5. Cluster Analysis\n",
    "The purpose of this section is to look at different ways that the embeddings obtained for the dataset of phenotype descriptions can be used to cluster or organize the genes to which those phenotypes are mapped into subgroups or representations. These approaches include generating topic models from the data, and doing agglomerative clustering to find clusters to which each gene belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic_modeling\"></a>\n",
    "### Approach 1: Topic modeling based on n-grams with a reduced vocabulary\n",
    "Topic modelling learns a set of word probability distributions from the dataset of text descriptions, which represent distinct topics which are present in the dataset. Each text description can then be represented as a discrete probability distribution over the learned topics based on the probability that a given piece of text belongs to each particular topics. This is a form of data reduction because a high dimensionsal bag-of-words can be represented as a vector of *k* probabilities where *k* is the number of topics. The main advantages of topic modelling over clustering is that topic modelling provides soft classifications that can be additionally interpreted, rather than hard classifications into a single cluster. Topic models are also explainable, because the word probability distributions for that topic can be used to determine which words are most representative of any given topic. One problem with topic modelling is that is uses the n-grams embeddings to semantic similarity between different words is not accounted for. To help alleviate this, this section uses implementations of some existing algorithms to compress the vocabulary as a preprocessing step based on word distance matrices generated using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>topic</th>\n",
       "      <th>GEM</th>\n",
       "      <th>EMG</th>\n",
       "      <th>MGD</th>\n",
       "      <th>EMB</th>\n",
       "      <th>MSD</th>\n",
       "      <th>SRL</th>\n",
       "      <th>GER</th>\n",
       "      <th>NLS</th>\n",
       "      <th>PIG</th>\n",
       "      <th>GRS</th>\n",
       "      <th>ROT</th>\n",
       "      <th>LEF</th>\n",
       "      <th>IST</th>\n",
       "      <th>ARC</th>\n",
       "      <th>MSL</th>\n",
       "      <th>FSM</th>\n",
       "      <th>OVP</th>\n",
       "      <th>SRF</th>\n",
       "      <th>SSC</th>\n",
       "      <th>FLT</th>\n",
       "      <th>SEN</th>\n",
       "      <th>STT</th>\n",
       "      <th>RTH</th>\n",
       "      <th>CUL</th>\n",
       "      <th>PRA</th>\n",
       "      <th>CPR</th>\n",
       "      <th>WAT</th>\n",
       "      <th>TMP</th>\n",
       "      <th>LIT</th>\n",
       "      <th>MEC</th>\n",
       "      <th>NUT</th>\n",
       "      <th>HRM</th>\n",
       "      <th>CHS</th>\n",
       "      <th>MCH</th>\n",
       "      <th>PTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.314955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110192</td>\n",
       "      <td>0.039894</td>\n",
       "      <td>0.312442</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080760</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.095523</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.047762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032103</td>\n",
       "      <td>0.166692</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.116377</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.060252</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.061155</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.027871</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.023170</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.020957</td>\n",
       "      <td>0.084513</td>\n",
       "      <td>0.029746</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0.031707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.091329</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.030381</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.089424</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.035563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>0.011453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078229</td>\n",
       "      <td>0.021496</td>\n",
       "      <td>0.042306</td>\n",
       "      <td>0.164657</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.007991</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.022576</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052154</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.008612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217796</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.143415</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030318</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032391</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.069869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117458</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.013664</td>\n",
       "      <td>0.023957</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021594</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016346</td>\n",
       "      <td>0.057083</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.138397</td>\n",
       "      <td>0.016198</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.025617</td>\n",
       "      <td>0.030521</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.042680</td>\n",
       "      <td>0.013697</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.062434</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.029619</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.106968</td>\n",
       "      <td>0.061051</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.010898</td>\n",
       "      <td>0.016859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.192813</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.033662</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.098902</td>\n",
       "      <td>0.145677</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.027706</td>\n",
       "      <td>0.038689</td>\n",
       "      <td>0.035979</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.027216</td>\n",
       "      <td>0.005795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058433</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094198</td>\n",
       "      <td>0.058947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135347</td>\n",
       "      <td>0.048514</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025369</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.031913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.011482</td>\n",
       "      <td>0.067768</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.035286</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.026439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.041303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.020570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.108829</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025990</td>\n",
       "      <td>0.032169</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>0.005583</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.001695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.035434</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.013758</td>\n",
       "      <td>0.026603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017399</td>\n",
       "      <td>0.018942</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024047</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.006795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0.029537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.107932</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.048407</td>\n",
       "      <td>0.031596</td>\n",
       "      <td>0.030989</td>\n",
       "      <td>0.172862</td>\n",
       "      <td>0.048755</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.046930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025544</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.106046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.035908</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.013702</td>\n",
       "      <td>0.020749</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>0.042945</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.130089</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.238673</td>\n",
       "      <td>0.307352</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065075</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017854</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.030773</td>\n",
       "      <td>0.032160</td>\n",
       "      <td>0.175971</td>\n",
       "      <td>0.116756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.037015</td>\n",
       "      <td>0.066454</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.009795</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.106644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019435</td>\n",
       "      <td>0.053322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042681</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.243281</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282244</td>\n",
       "      <td>0.214042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049163</td>\n",
       "      <td>0.027568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029699</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082996</td>\n",
       "      <td>0.067134</td>\n",
       "      <td>0.029090</td>\n",
       "      <td>0.034474</td>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.042679</td>\n",
       "      <td>0.075058</td>\n",
       "      <td>0.061483</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>0.029582</td>\n",
       "      <td>0.082587</td>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.063178</td>\n",
       "      <td>0.042403</td>\n",
       "      <td>0.024843</td>\n",
       "      <td>0.036082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.101057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052595</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.036524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092752</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>0.176421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003808</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.078873</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099670</td>\n",
       "      <td>0.236624</td>\n",
       "      <td>0.041998</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068255</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335265</td>\n",
       "      <td>0.335265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042725</td>\n",
       "      <td>0.138361</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>0.002904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.012297</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.041440</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047608</td>\n",
       "      <td>0.014458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.013427</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.033012</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.057350</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.035848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039967</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>0.015648</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.005844</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.040422</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.017655</td>\n",
       "      <td>0.020208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019033</td>\n",
       "      <td>0.161558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.010534</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019823</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042173</td>\n",
       "      <td>0.065884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061060</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.035189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148410</td>\n",
       "      <td>0.302397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108718</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.019972</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.057439</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>0.035677</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.053247</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.014884</td>\n",
       "      <td>0.140153</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.177925</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072370</td>\n",
       "      <td>0.008157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023905</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.017013</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>0.042653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.057301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.021863</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>0.165711</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032803</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.259679</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.037638</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>0.013330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>0.031561</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.042439</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>0.036519</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.091117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>0.286017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019546</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.004507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>0.019030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.312529</td>\n",
       "      <td>0.068864</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.042106</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026776</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.053270</td>\n",
       "      <td>0.048520</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.101676</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.007596</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.022436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008077</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019514</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>0.024950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063255</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113442</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.247804</td>\n",
       "      <td>0.017624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.011514</td>\n",
       "      <td>0.011214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.094686</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009461</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.056643</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053646</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>0.178268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043974</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order  topic       GEM       EMG       MGD       EMB       MSD       SRL       GER       NLS       PIG       GRS       ROT       LEF       IST       ARC       MSL       FSM       OVP       SRF       SSC       FLT       SEN       STT       RTH       CUL       PRA       CPR       WAT       TMP       LIT       MEC       NUT       HRM       CHS       MCH       PTH\n",
       "0       0     40  0.314955  0.000000  0.110192  0.039894  0.312442  0.101645  0.022139  0.000000  0.080760  0.000702  0.002490  0.000000  0.000000  0.000119  0.000000  0.000000  0.000000  0.001817  0.095523  0.001581  0.040798  0.047762  0.000000  0.000000  0.007480  0.000000  0.012452  0.000000  0.002638  0.000000  0.000000  0.005932  0.002581  0.000000  0.000000\n",
       "1       1      0  0.032103  0.166692  0.010642  0.116377  0.031674  0.010556  0.008126  0.000000  0.008842  0.000200  0.000195  0.000215  0.000000  0.000145  0.000084  0.000487  0.000000  0.000508  0.000000  0.000109  0.000484  0.000000  0.000000  0.000000  0.000074  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000328  0.000000  0.000196  0.000000\n",
       "2       2      5  0.060252  0.001652  0.061155  0.003730  0.027871  0.000039  0.007143  0.000000  0.003268  0.001948  0.002662  0.002958  0.000000  0.000673  0.000593  0.011807  0.023170  0.006280  0.000000  0.000399  0.002166  0.000174  0.000000  0.000000  0.000427  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.001294  0.000283  0.001101  0.000000\n",
       "3       3     39  0.037589  0.000459  0.020957  0.084513  0.029746  0.010624  0.013782  0.008281  0.012031  0.004106  0.016358  0.001674  0.011042  0.000000  0.000131  0.003739  0.000000  0.001174  0.000000  0.000000  0.000304  0.000000  0.000000  0.000000  0.000147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000306  0.000000\n",
       "4       4     28  0.031707  0.000000  0.023710  0.091329  0.020647  0.006874  0.012318  0.000000  0.009298  0.001981  0.003368  0.003689  0.000000  0.000000  0.000000  0.008305  0.000000  0.002427  0.000000  0.000000  0.000000  0.000000  0.016484  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000453  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "5       5     27  0.030381  0.017173  0.045543  0.089424  0.023694  0.055572  0.035563  0.000000  0.017363  0.007303  0.007483  0.010690  0.000000  0.000000  0.004906  0.018707  0.000000  0.007799  0.000000  0.004560  0.011453  0.000000  0.000000  0.000000  0.004293  0.000000  0.000000  0.000000  0.001967  0.000000  0.000000  0.000000  0.000000  0.011448  0.000000\n",
       "6       6     29  0.000000  0.078229  0.021496  0.042306  0.164657  0.001127  0.001216  0.000000  0.001257  0.013904  0.007991  0.008883  0.000000  0.000262  0.022576  0.019977  0.000000  0.016884  0.000000  0.000000  0.052154  0.007528  0.000000  0.000000  0.019557  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.052153  0.000000\n",
       "7       7      3  0.020115  0.000000  0.005188  0.008612  0.000000  0.217796  0.008069  0.002442  0.143415  0.004456  0.012449  0.003389  0.001546  0.001628  0.001799  0.005188  0.000000  0.002433  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.001655  0.000000  0.030318  0.007908  0.002543  0.049155  0.016388  0.000050  0.002259  0.000000  0.000000\n",
       "8       8     33  0.000000  0.032391  0.001686  0.069869  0.000000  0.117458  0.008247  0.008426  0.006055  0.007162  0.007110  0.004498  0.004708  0.005617  0.011656  0.013664  0.023957  0.010789  0.000000  0.003512  0.021593  0.000349  0.000000  0.000000  0.021611  0.000000  0.000000  0.000000  0.005042  0.018486  0.003847  0.000000  0.000000  0.021594  0.000000\n",
       "9       9     12  0.000000  0.016346  0.057083  0.016482  0.000000  0.000594  0.138397  0.016198  0.019472  0.025617  0.030521  0.029785  0.042680  0.013697  0.004670  0.062434  0.035387  0.029619  0.005488  0.106968  0.061051  0.003059  0.000000  0.000000  0.004087  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.018906  0.008338  0.010898  0.016859\n",
       "10     10     18  0.000000  0.000000  0.011181  0.002640  0.000000  0.000438  0.154250  0.192813  0.004515  0.033662  0.090700  0.098902  0.145677  0.173077  0.027706  0.038689  0.035979  0.015529  0.000000  0.011927  0.027216  0.005795  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.003954  0.000000  0.000000  0.011768  0.000000  0.000000  0.000000\n",
       "11     11     36  0.000000  0.000000  0.058433  0.009190  0.000000  0.094198  0.058947  0.000000  0.135347  0.048514  0.010528  0.013248  0.017356  0.013281  0.000000  0.020059  0.000000  0.005731  0.000000  0.008074  0.000000  0.029279  0.000000  0.000000  0.023898  0.000000  0.000000  0.000000  0.012938  0.053214  0.000000  0.025369  0.014497  0.020439  0.000000\n",
       "12     12     30  0.000000  0.000000  0.000000  0.000867  0.031913  0.000000  0.002795  0.000024  0.011482  0.067768  0.102059  0.024518  0.035286  0.020670  0.006817  0.026439  0.000000  0.016770  0.000000  0.044018  0.041303  0.000000  0.049660  0.000000  0.023982  0.020570  0.000000  0.000000  0.011152  0.000000  0.000000  0.033800  0.015633  0.000000  0.011930\n",
       "13     13     41  0.000000  0.000000  0.000000  0.005851  0.000000  0.009579  0.000076  0.000000  0.009148  0.015251  0.010324  0.108829  0.031732  0.025231  0.000969  0.023798  0.000000  0.013810  0.000000  0.025990  0.032169  0.016562  0.000000  0.000000  0.005291  0.009941  0.000000  0.000000  0.009730  0.000000  0.000000  0.026443  0.005583  0.006832  0.001695\n",
       "14     14      1  0.000000  0.006830  0.008357  0.000234  0.000000  0.000018  0.000524  0.013693  0.006136  0.015833  0.012911  0.035434  0.026480  0.009156  0.013758  0.026603  0.000000  0.015683  0.000000  0.017399  0.018942  0.008479  0.000000  0.000000  0.004183  0.001292  0.000000  0.024047  0.006871  0.000000  0.005309  0.010864  0.002002  0.009862  0.006795\n",
       "15     15     19  0.029537  0.000000  0.000000  0.006767  0.000000  0.000661  0.003280  0.107932  0.011164  0.048407  0.031596  0.030989  0.172862  0.048755  0.006444  0.046930  0.000000  0.020819  0.000000  0.025544  0.029441  0.000000  0.039824  0.000000  0.007021  0.021416  0.106046  0.000000  0.025877  0.000000  0.011594  0.035908  0.019512  0.000000  0.007420\n",
       "16     16     24  0.000000  0.000000  0.000618  0.003487  0.011090  0.000000  0.011160  0.002664  0.013702  0.020749  0.004387  0.042945  0.003552  0.130089  0.011703  0.000618  0.000000  0.001981  0.008412  0.108991  0.008422  0.004640  0.000000  0.000000  0.008123  0.004403  0.000000  0.000000  0.002086  0.000000  0.000000  0.012277  0.007028  0.003086  0.002800\n",
       "17     17     15  0.000000  0.007436  0.021424  0.001684  0.006522  0.003411  0.000000  0.000000  0.000029  0.035680  0.009836  0.008787  0.002663  0.004320  0.005828  0.238673  0.307352  0.083759  0.000000  0.065075  0.004957  0.020243  0.010016  0.000000  0.001859  0.000000  0.000000  0.000000  0.000145  0.000000  0.000000  0.000000  0.000000  0.005363  0.000000\n",
       "18     18     35  0.000000  0.017854  0.037512  0.000000  0.004979  0.000000  0.016315  0.000000  0.010068  0.031246  0.006847  0.007608  0.000000  0.020691  0.030773  0.032160  0.175971  0.116756  0.000000  0.004906  0.024985  0.000000  0.000000  0.000000  0.004464  0.000489  0.000311  0.000311  0.013544  0.000000  0.000000  0.012703  0.000298  0.011903  0.000000\n",
       "19     19      7  0.000000  0.000000  0.001419  0.000808  0.009750  0.001274  0.000000  0.000000  0.003010  0.004279  0.004190  0.003823  0.005049  0.022411  0.016459  0.037015  0.066454  0.106200  0.000000  0.002376  0.000000  0.028319  0.000000  0.000000  0.000000  0.030525  0.000000  0.000000  0.000770  0.000000  0.000000  0.000000  0.013082  0.000000  0.000000\n",
       "20     20      4  0.000000  0.004446  0.009795  0.002333  0.005464  0.006949  0.002949  0.000000  0.014874  0.003726  0.003635  0.002128  0.000000  0.000254  0.001488  0.006457  0.003399  0.003516  0.106644  0.000000  0.019435  0.053322  0.000000  0.000000  0.090256  0.000000  0.000000  0.000000  0.000000  0.011230  0.000000  0.001442  0.000000  0.002964  0.000000\n",
       "21     21      9  0.000000  0.000100  0.004584  0.002260  0.001135  0.001273  0.000000  0.042681  0.021832  0.013031  0.001644  0.012753  0.024132  0.000022  0.014066  0.008217  0.008214  0.009371  0.243281  0.002757  0.000000  0.282244  0.214042  0.000000  0.000000  0.000000  0.000000  0.049163  0.027568  0.000000  0.000000  0.029699  0.014927  0.000000  0.000000\n",
       "22     22     21  0.000000  0.000000  0.000000  0.001735  0.000000  0.000000  0.082996  0.067134  0.029090  0.034474  0.032485  0.042679  0.075058  0.061483  0.009551  0.029582  0.082587  0.048956  0.063178  0.042403  0.024843  0.036082  0.000000  0.494503  0.000000  0.000000  0.000000  0.001745  0.101057  0.000000  0.052595  0.025796  0.004094  0.036524  0.000000\n",
       "23     23     16  0.000000  0.000000  0.000239  0.004546  0.000000  0.000353  0.009319  0.003906  0.004956  0.002199  0.009822  0.001454  0.002386  0.004268  0.000541  0.000000  0.000000  0.001436  0.000000  0.009035  0.005437  0.003732  0.000000  0.000000  0.092752  0.004422  0.000000  0.001895  0.000541  0.000000  0.000000  0.004078  0.009891  0.003702  0.000000\n",
       "24     24     22  0.000000  0.003992  0.004132  0.000000  0.000693  0.002093  0.000000  0.001903  0.001395  0.001328  0.000670  0.000846  0.000000  0.000706  0.002833  0.000399  0.002675  0.001995  0.000000  0.005685  0.002661  0.000000  0.000000  0.000000  0.007175  0.176421  0.000000  0.003808  0.001088  0.006702  0.000000  0.023203  0.012408  0.002662  0.000000\n",
       "25     25     31  0.078873  0.002675  0.001101  0.001248  0.000000  0.007078  0.001549  0.005096  0.003056  0.001627  0.009615  0.002754  0.000828  0.003814  0.003407  0.013168  0.000000  0.005466  0.000000  0.002001  0.001783  0.001067  0.000000  0.000000  0.099670  0.236624  0.041998  0.006320  0.008412  0.000000  0.004355  0.000000  0.000539  0.001783  0.000000\n",
       "26     26     34  0.000000  0.000000  0.009063  0.003125  0.000000  0.000000  0.004082  0.004210  0.002090  0.003398  0.007400  0.004028  0.005613  0.000643  0.000712  0.009063  0.000000  0.002865  0.000000  0.068255  0.001286  0.000000  0.021781  0.000000  0.000000  0.000000  0.335265  0.335265  0.000000  0.000000  0.000000  0.042725  0.138361  0.056885  0.002904\n",
       "27     27     23  0.000000  0.007145  0.012297  0.004103  0.000000  0.001124  0.016092  0.027505  0.006220  0.041440  0.013148  0.017687  0.004935  0.002467  0.015644  0.015822  0.000000  0.013196  0.000000  0.000000  0.004764  0.000000  0.000000  0.000000  0.001786  0.002555  0.000000  0.047608  0.014458  0.000000  0.000000  0.000000  0.001095  0.004764  0.000000\n",
       "28     28     11  0.000000  0.011555  0.013427  0.001912  0.000000  0.000000  0.002991  0.002138  0.002923  0.007148  0.007814  0.033012  0.007850  0.003521  0.057350  0.020909  0.007207  0.035848  0.000000  0.039967  0.014580  0.000000  0.002580  0.000000  0.012922  0.000000  0.000000  0.000000  0.075551  0.000000  0.000000  0.008047  0.003401  0.007703  0.000962\n",
       "29     29      8  0.002356  0.010236  0.000000  0.000014  0.000000  0.014358  0.000000  0.010021  0.015648  0.006061  0.001316  0.005339  0.004004  0.005844  0.008361  0.002499  0.003535  0.007721  0.040422  0.005836  0.017655  0.020208  0.000000  0.000000  0.008268  0.009903  0.000000  0.019033  0.161558  0.000000  0.000000  0.018727  0.010534  0.006824  0.000000\n",
       "30     30     32  0.000000  0.019823  0.082353  0.004438  0.000000  0.042173  0.065884  0.000000  0.061060  0.030436  0.030240  0.012161  0.000000  0.029231  0.005988  0.007148  0.000000  0.009745  0.000000  0.008242  0.035189  0.000000  0.113538  0.000000  0.004956  0.000000  0.000000  0.000000  0.148410  0.302397  0.000000  0.036514  0.011447  0.013215  0.000000\n",
       "31     31     25  0.000000  0.108718  0.000238  0.019972  0.000318  0.036903  0.059446  0.008704  0.094405  0.057439  0.020111  0.002576  0.003878  0.023708  0.035677  0.000075  0.003424  0.053247  0.061176  0.014884  0.140153  0.030588  0.000000  0.000000  0.032919  0.000000  0.000000  0.000000  0.007200  0.177925  0.001381  0.029764  0.000000  0.072370  0.008157\n",
       "32     32     13  0.000000  0.023905  0.007762  0.000038  0.001979  0.012751  0.000000  0.003734  0.017013  0.068434  0.014221  0.012191  0.008160  0.019901  0.016984  0.014633  0.001503  0.036077  0.000000  0.018436  0.042653  0.000000  0.004586  0.000000  0.005976  0.000000  0.000000  0.007467  0.002736  0.086400  0.000032  0.020038  0.000000  0.015936  0.057301\n",
       "33     33     10  0.003159  0.005603  0.018032  0.000403  0.000000  0.032960  0.005391  0.006739  0.021863  0.050372  0.165711  0.011672  0.009396  0.010099  0.002013  0.019523  0.000000  0.006585  0.000000  0.032803  0.022135  0.017687  0.133926  0.000000  0.017787  0.000000  0.157933  0.000000  0.029005  0.259679  0.001334  0.010160  0.037638  0.003735  0.000000\n",
       "34     34     17  0.013330  0.000000  0.000000  0.001566  0.000000  0.001138  0.000916  0.001533  0.002205  0.002781  0.003654  0.000509  0.000517  0.000764  0.002846  0.000000  0.000000  0.002262  0.000000  0.000876  0.001443  0.000000  0.005081  0.000000  0.004701  0.003047  0.000000  0.000000  0.003400  0.000000  0.377559  0.001921  0.002107  0.000000  0.007814\n",
       "35     35     38  0.005372  0.027120  0.004458  0.000011  0.000000  0.019930  0.000033  0.006415  0.031561  0.024636  0.042439  0.010080  0.015738  0.036519  0.011414  0.016260  0.000000  0.029915  0.000000  0.033290  0.091117  0.000000  0.026948  0.000000  0.008621  0.000000  0.000000  0.012829  0.003805  0.000000  0.028164  0.286017  0.000000  0.019546  0.000000\n",
       "36     36     37  0.000000  0.000000  0.000000  0.004865  0.000000  0.000000  0.000914  0.001143  0.000943  0.003149  0.004263  0.004507  0.000000  0.006761  0.005752  0.004237  0.035366  0.005052  0.000000  0.008819  0.151468  0.000000  0.047325  0.000000  0.012038  0.037780  0.019030  0.000000  0.000000  0.000000  0.017620  0.312529  0.068864  0.017620  0.000000\n",
       "37     37     20  0.000000  0.000000  0.002765  0.000087  0.000000  0.000202  0.000000  0.000000  0.000516  0.001075  0.023283  0.001229  0.000000  0.000514  0.042106  0.002765  0.000000  0.000991  0.000000  0.026776  0.000941  0.000000  0.000000  0.000000  0.000279  0.009030  0.053270  0.048520  0.000973  0.000000  0.000744  0.021140  0.101676  0.000744  0.001177\n",
       "38     38      2  0.000547  0.003037  0.000000  0.000000  0.000000  0.000028  0.000000  0.007037  0.002388  0.003686  0.007596  0.003128  0.000000  0.002263  0.013032  0.000000  0.000000  0.003454  0.000000  0.001697  0.022436  0.000000  0.000000  0.000000  0.008077  0.062260  0.039107  0.015198  0.004745  0.000000  0.019514  0.013133  0.077067  0.021538  0.024950\n",
       "39     39     14  0.000000  0.060950  0.000000  0.000000  0.000000  0.000045  0.000000  0.001976  0.000348  0.008623  0.001298  0.000000  0.002634  0.001697  0.027514  0.001804  0.000000  0.009495  0.000000  0.001273  0.041905  0.001333  0.000000  0.000000  0.063255  0.007305  0.000000  0.000000  0.000000  0.000000  0.113442  0.001155  0.013231  0.247804  0.017624\n",
       "40     40      6  0.000000  0.009142  0.000000  0.000038  0.000000  0.000076  0.000000  0.025231  0.000000  0.008072  0.011514  0.011214  0.000000  0.000468  0.094686  0.000684  0.000000  0.009663  0.000000  0.009461  0.006095  0.000000  0.000000  0.000000  0.002286  0.005212  0.006181  0.056643  0.016322  0.000000  0.001728  0.000000  0.053646  0.017457  0.178268\n",
       "41     41     26  0.000000  0.000000  0.000150  0.000000  0.000000  0.000371  0.000039  0.000838  0.000248  0.000451  0.002015  0.000088  0.001053  0.000032  0.000000  0.000150  0.000000  0.000043  0.000000  0.002141  0.000000  0.003962  0.000000  0.000000  0.043974  0.000146  0.003707  0.003707  0.000052  0.002970  0.000000  0.009687  0.008925  0.000000  0.130999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of texts to create a topic model from, from one of the processed description dictionaries above. \n",
    "texts = [description for i,description in descriptions_linares_pontes.items()]\n",
    "\n",
    "# Creating and fitting the topic model, either NFM or LDA.\n",
    "number_of_topics = 42\n",
    "seed = 0\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", max_df=0.95, min_df=2, lowercase=False)\n",
    "features = vectorizer.fit_transform(texts)\n",
    "cls = NMF(n_components=number_of_topics, random_state=seed)\n",
    "cls.fit(features)\n",
    "\n",
    "# Function for retrieving the topic vectors for a list of text descriptions.\n",
    "def get_topic_embeddings(texts, model, vectorizer):\n",
    "    ngrams_vectors = vectorizer.transform(texts).toarray()\n",
    "    topic_vectors = model.transform(ngrams_vectors)\n",
    "    return(topic_vectors)\n",
    "    \n",
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_topic_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    texts = [descriptions_linares_pontes[i] for i in ids]\n",
    "    topic_vectors = get_topic_embeddings(texts, cls, vectorizer)\n",
    "    mean_topic_vector = np.mean(topic_vectors, axis=0)\n",
    "    group_to_topic_vector[group_id] = mean_topic_vector\n",
    "    \n",
    "tm_df = pd.DataFrame(group_to_topic_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "if NOTEBOOK_TAGS[\"subsets\"]:\n",
    "    filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "    lmtm_df = pd.read_csv(filename)    \n",
    "    columns_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "    tm_df = tm_df[columns_in_order]\n",
    "    \n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "tm_df[\"idxmax\"] = tm_df.idxmax(axis = 1)\n",
    "tm_df[\"idxmax\"] = tm_df[\"idxmax\"].apply(lambda x: tm_df.columns.get_loc(x))\n",
    "tm_df = tm_df.sort_values(by=\"idxmax\")\n",
    "tm_df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "tm_df = tm_df.reset_index(drop=False).rename({\"index\":\"topic\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "tm_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_topic_modeling.csv\"), index=False)\n",
    "tm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: globular clusters embryos zygotic embryonic fertilization sac embryogenesis embryo \n",
      "1: leaves shoots unopened leaf rosettes rosette \n",
      "2: insensitive sensitivity hypersensitive sensitive responsive salt \n",
      "3: pigment pigments lethality lethal \n",
      "4: suberin ttg arrangement composed composition \n",
      "5: female male gametophyte gametophytes meristem meristems \n",
      "6: susceptible resistance resistant necrotic lesions cortical nodules degeneration discoloration infection cyst \n",
      "7: cytokinesis polytene chromosomes meiosis chiasmata meiotic recombination formed formation \n",
      "8: red yellow darkness light \n",
      "9: trichome hairs mucilage trichomes hypocotyls hypocotyl stomatal bleached curly hair \n",
      "10: roots root lateral ventral longitudinal posterior \n",
      "11: hours weeks day days long short \n",
      "12: late early mid emerged period life gametophytic ethionine flowering \n",
      "13: elf dwarf giant completely fully almost partially \n",
      "14: nickel arsenic cobalt toxic cadmium higher levels concentrations level \n",
      "15: lobes carpels petals stigmatic pistil anther stamens whorls sepals gynoecium cells cell \n",
      "16: gibberellin biosynthesis enzymatic sterol pdh sterols oxidative abscisic jasmonate heme pathway sst low \n",
      "17: oxygen nitrogen pronounced \n",
      "18: inflorescence stem unbranched silique inflorescences pedicels growth expansion proliferation \n",
      "19: decreased increase increasing significantly increased ft peak tall width height \n",
      "20: stress rigidity nacl osmotic dehydration \n",
      "21: reduced reduction depleted fertility \n",
      "22: acclimation response responses \n",
      "23: chlorophyll photosynthesis npq photosynthetic xanthophylls slow slower slowed \n",
      "24: class standard type types wild \n",
      "25: pale darker black shade dark greening green \n",
      "26: pseudomonas pv syringae susceptible resistance resistant \n",
      "27: embryos zygotic embryonic fertilization sac embryogenesis embryo cotyledon cotyledons \n",
      "28:  embryos zygotic embryonic fertilization sac embryogenesis embryo \n",
      "29: embryos zygotic embryonic fertilization sac embryogenesis embryo stages stage dramatic \n",
      "30: threonine pfk methionine alanine accumulation accumulated stratification accumulates \n",
      "31: malate nad currents \n",
      "32: sucrose sugar anthocyanin mannitol starch malic mutant shapeshifter deetiolation mutants albino \n",
      "33: heterozygous heterozygotes penetrance homozygous recessive sst low \n",
      "34: freezing cold tolerance tolerant tolerate \n",
      "35: sterility sterile incomplete partial complete full \n",
      "36: mutant shapeshifter deetiolation mutants albino thylakoid chloroplasts chloroplast \n",
      "37: aba insensitive sensitivity hypersensitive sensitive responsive \n",
      "38: ispe ebf methyl urea ethylene \n",
      "39: change shift embryos zygotic embryonic fertilization sac embryogenesis embryo \n",
      "40: defective defect seeds seed \n",
      "41: cis ubc mutant shapeshifter deetiolation mutants albino \n"
     ]
    }
   ],
   "source": [
    "# Describing what the most representative tokens for each topic in the model are.\n",
    "num_top_words = 2\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    print(i,end=\": \")\n",
    "    for fid in topic_vec.argsort()[-1:-num_top_words-1:-1]:\n",
    "        word = feature_names[fid]\n",
    "        word = \" \".join(unreduce_lp[word])\n",
    "        print(word, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "### Approach 2: Agglomerative clustering and comparison to predefined groups\n",
    "This clustering approach uses agglomerative clustering to cluster the genes into a fixed number of clusters based off the distances between their embedding representations using all of the above methods. Clustering into a fixed number of clusters allows for clustering into a similar number of groups as a present in some existing grouping of the data, such as phenotype categories or biochemical pathways, and then determining if the clusters obtained are at all similar to the groupings that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate the numpy array where values are mean distance percentiles between all the methods.\n",
    "mean_pct_array = name_to_array[\"Mean\"]\n",
    "to_id = array_index_to_id\n",
    "\n",
    "# Do agglomerative clustering based on that distance matrix.\n",
    "number_of_clusters = 42\n",
    "ac = AgglomerativeClustering(n_clusters=number_of_clusters, linkage=\"complete\", affinity=\"precomputed\")\n",
    "clustering = ac.fit(mean_pct_array)\n",
    "id_to_cluster = {}\n",
    "cluster_to_ids = defaultdict(list)\n",
    "for idx,c in enumerate(clustering.labels_):\n",
    "    id_to_cluster[to_id[idx]] = c\n",
    "    cluster_to_ids[c].append(to_id[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>cluster</th>\n",
       "      <th>ARC</th>\n",
       "      <th>FLT</th>\n",
       "      <th>ROT</th>\n",
       "      <th>CHS</th>\n",
       "      <th>PIG</th>\n",
       "      <th>GRS</th>\n",
       "      <th>WAT</th>\n",
       "      <th>TMP</th>\n",
       "      <th>NLS</th>\n",
       "      <th>IST</th>\n",
       "      <th>EMB</th>\n",
       "      <th>STT</th>\n",
       "      <th>SRL</th>\n",
       "      <th>LEF</th>\n",
       "      <th>PTH</th>\n",
       "      <th>SRF</th>\n",
       "      <th>FSM</th>\n",
       "      <th>OVP</th>\n",
       "      <th>MSL</th>\n",
       "      <th>LIT</th>\n",
       "      <th>GEM</th>\n",
       "      <th>PRA</th>\n",
       "      <th>NUT</th>\n",
       "      <th>MCH</th>\n",
       "      <th>CUL</th>\n",
       "      <th>SEN</th>\n",
       "      <th>HRM</th>\n",
       "      <th>CPR</th>\n",
       "      <th>MGD</th>\n",
       "      <th>GER</th>\n",
       "      <th>MSD</th>\n",
       "      <th>MEC</th>\n",
       "      <th>EMG</th>\n",
       "      <th>RTH</th>\n",
       "      <th>SSC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order  cluster       ARC    FLT  ROT       CHS       PIG       GRS  WAT  TMP   NLS       IST     EMB  STT    SRL       LEF   PTH       SRF   FSM  OVP       MSL       LIT  GEM    PRA       NUT       MCH  CUL       SEN   HRM       CPR   MGD  GER       MSD  MEC  EMG  RTH  SSC\n",
       "0       0        7  0.500000  0.000  0.2  0.000000  0.000000  0.058824  0.0  0.0  0.50  0.333333  0.0000  0.0  0.000  0.222222  0.00  0.000000  0.00  0.0  0.142857  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.4  0.000000  0.0  0.0  0.0  0.0\n",
       "1       1        1  0.166667  0.375  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.111111  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.2  0.000000  0.0  0.0  0.0  0.0\n",
       "2       2       33  0.000000  0.000  0.0  0.142857  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.142857  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "3       3       21  0.000000  0.000  0.0  0.000000  0.083333  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.071429  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "4       4       24  0.000000  0.000  0.0  0.000000  0.000000  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "5       5       36  0.000000  0.125  0.0  0.142857  0.000000  0.000000  0.5  0.5  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "6       6       15  0.000000  0.125  0.2  0.142857  0.000000  0.000000  0.5  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "7       7       17  0.166667  0.250  0.1  0.000000  0.083333  0.176471  0.0  0.0  0.25  0.666667  0.0000  0.0  0.000  0.111111  0.00  0.142857  0.25  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.333333  0.25  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "8       8        3  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.5000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.333333  0.0  0.5  0.0  0.0\n",
       "9       9        8  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.1250  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "10     10       16  0.000000  0.000  0.0  0.000000  0.083333  0.000000  0.0  0.0  0.00  0.000000  0.0000  1.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  1.0\n",
       "11     11        2  0.000000  0.000  0.0  0.000000  0.166667  0.000000  0.0  0.0  0.00  0.000000  0.1250  0.0  0.375  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "12     12       29  0.000000  0.000  0.0  0.000000  0.250000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.375  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "13     13       27  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.111111  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "14     14        0  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.50  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "15     15       23  0.000000  0.000  0.0  0.142857  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.25  0.000000  0.00  0.0  0.142857  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "16     16       41  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.25  0.000000  0.00  0.0  0.142857  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "17     17        6  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.214286  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "18     18       26  0.000000  0.000  0.0  0.000000  0.000000  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.071429  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "19     19       12  0.000000  0.125  0.0  0.000000  0.000000  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.142857  0.50  0.5  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "20     20       38  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.071429  0.00  0.5  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "21     21       32  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.071429  0.00  0.0  0.142857  0.142857  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "22     22       14  0.000000  0.000  0.0  0.000000  0.000000  0.058824  0.0  0.5  0.25  0.000000  0.0000  0.0  0.000  0.111111  0.00  0.071429  0.00  0.0  0.142857  0.714286  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "23     23       19  0.000000  0.000  0.0  0.000000  0.083333  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  1.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.75  0.2  0.333333  0.0  0.0  0.0  0.0\n",
       "24     24       39  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.125  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "25     25       22  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.250  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "26     26       25  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.125  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "27     27        9  0.000000  0.000  0.1  0.000000  0.000000  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.125  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "28     28        4  0.000000  0.000  0.0  0.142857  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.125  0.333333  0.333333  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "29     29        5  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.666667  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "30     30       18  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.333333  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "31     31       28  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.142857  0.0  0.000  0.000000  0.000000  1.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "32     32       13  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.333333  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "33     33       35  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.25  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "34     34       10  0.166667  0.000  0.1  0.000000  0.000000  0.058824  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.222222  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.25  0.000000  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "35     35       40  0.000000  0.000  0.0  0.142857  0.166667  0.117647  0.0  0.0  0.00  0.000000  0.0625  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.25  0.000000  0.00  0.2  0.000000  0.0  0.0  0.0  0.0\n",
       "36     36       34  0.000000  0.000  0.0  0.142857  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.333333  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "37     37       37  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.333333  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "38     38       31  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.125  0.000000  0.000000  0.0  0.000000  0.00  0.333333  0.00  0.0  0.000000  0.0  0.0  0.0  0.0\n",
       "39     39       11  0.000000  0.000  0.2  0.000000  0.083333  0.117647  0.0  0.0  0.00  0.000000  0.0000  0.0  0.125  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  1.0  0.0  0.0  0.0\n",
       "40     40       20  0.000000  0.000  0.1  0.000000  0.000000  0.117647  0.0  0.0  0.00  0.000000  0.1875  0.0  0.125  0.111111  0.00  0.142857  0.25  0.0  0.142857  0.000000  0.0  0.125  0.000000  0.333333  0.0  0.333333  0.00  0.000000  0.25  0.0  0.333333  0.0  0.5  0.0  0.0\n",
       "41     41       30  0.000000  0.000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.00  0.000000  0.0000  0.0  0.000  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.000000  0.0  0.000  0.000000  0.000000  0.0  0.000000  0.00  0.000000  0.00  0.0  0.000000  0.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_cluster_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    \n",
    "    mean_cluster_vector = np.zeros(number_of_clusters)\n",
    "    for i in ids:\n",
    "        cluster = id_to_cluster[i]\n",
    "        mean_cluster_vector[cluster] = mean_cluster_vector[cluster]+1\n",
    "    mean_cluster_vector = mean_cluster_vector/mean_cluster_vector.sum(axis=0,keepdims=1)\n",
    "    group_to_cluster_vector[group_id] = mean_cluster_vector\n",
    "    \n",
    "ac_df = pd.DataFrame(group_to_cluster_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "if NOTEBOOK_TAGS[\"subsets\"]:\n",
    "    filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "    lmtm_df = pd.read_csv(filename)    \n",
    "    columns_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "    tm_df = tm_df[columns_in_order]\n",
    "\n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "ac_df[\"idxmax\"] = ac_df.idxmax(axis = 1)\n",
    "ac_df[\"idxmax\"] = ac_df[\"idxmax\"].apply(lambda x: ac_df.columns.get_loc(x))\n",
    "ac_df = ac_df.sort_values(by=\"idxmax\")\n",
    "ac_df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "ac_df = ac_df.reset_index(drop=False).rename({\"index\":\"cluster\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "ac_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_agglomerative_clustering.csv\"), index=False)\n",
    "ac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phenologs\"></a>\n",
    "### Approach 3: Looking for phenolog relationships between clusters and OMIM disease phenotypes\n",
    "This section produces a table of values that provides a score for the a particular pair of a cluster found for this dataset of plant genes and a disease phenotype. Currently the value indicates the fraction of the plant genes in that cluster that have orthologs associated with that disease phenotype. This should be replaced or supplemented with a p-value for evaluating the significance of this value given the distribution of genes and their mappings to all of the disease phenotypes. All the rows from the input dataframe containing the PantherDB and OMIM information where the ID from this dataset is not known or the mapping to a phenotype was unsuccessful are removed at this step, fix this if the metric for evaluating cluster to phenotype phenolog mappings need this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>gene_identifier</th>\n",
       "      <th>human_ortholog_gene_symbol</th>\n",
       "      <th>gene_mim_number</th>\n",
       "      <th>phenotype_mim_number</th>\n",
       "      <th>phenotype_mim_name</th>\n",
       "      <th>id</th>\n",
       "      <th>compressed_phenotype_mim_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>ARATH</td>\n",
       "      <td>AT1G02580</td>\n",
       "      <td>TPK1</td>\n",
       "      <td>606370.0</td>\n",
       "      <td>614458</td>\n",
       "      <td>Thiamine metabolism dysfunction syndrome 5 (ep...</td>\n",
       "      <td>613</td>\n",
       "      <td>Thiamine metabolism dysfunction syndrome 5 (ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>ARATH</td>\n",
       "      <td>AT1G02580</td>\n",
       "      <td>EZH2</td>\n",
       "      <td>601573.0</td>\n",
       "      <td>277590</td>\n",
       "      <td>Weaver syndrome</td>\n",
       "      <td>613</td>\n",
       "      <td>Weaver syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>ARATH</td>\n",
       "      <td>AT1G02580</td>\n",
       "      <td>PEX6</td>\n",
       "      <td>601498.0</td>\n",
       "      <td>616617</td>\n",
       "      <td>Heimler syndrome 2</td>\n",
       "      <td>613</td>\n",
       "      <td>Heimler syndrome 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>ARATH</td>\n",
       "      <td>AT1G02580</td>\n",
       "      <td>PEX6</td>\n",
       "      <td>601498.0</td>\n",
       "      <td>614862</td>\n",
       "      <td>Peroxisome biogenesis disorder 4A (Zellweger)</td>\n",
       "      <td>613</td>\n",
       "      <td>Peroxisome biogenesis disorder 4A (Zellweger)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>ARATH</td>\n",
       "      <td>AT1G02580</td>\n",
       "      <td>PEX6</td>\n",
       "      <td>601498.0</td>\n",
       "      <td>614863</td>\n",
       "      <td>Peroxisome biogenesis disorder 4B</td>\n",
       "      <td>613</td>\n",
       "      <td>Peroxisome biogenesis disorder 4B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    species gene_identifier human_ortholog_gene_symbol  gene_mim_number  phenotype_mim_number                                 phenotype_mim_name   id                      compressed_phenotype_mim_name\n",
       "286   ARATH       AT1G02580                       TPK1         606370.0                614458  Thiamine metabolism dysfunction syndrome 5 (ep...  613  Thiamine metabolism dysfunction syndrome 5 (ep...\n",
       "288   ARATH       AT1G02580                       EZH2         601573.0                277590                                    Weaver syndrome  613                                    Weaver syndrome\n",
       "289   ARATH       AT1G02580                       PEX6         601498.0                616617                                 Heimler syndrome 2  613                                 Heimler syndrome 2\n",
       "290   ARATH       AT1G02580                       PEX6         601498.0                614862      Peroxisome biogenesis disorder 4A (Zellweger)  613      Peroxisome biogenesis disorder 4A (Zellweger)\n",
       "291   ARATH       AT1G02580                       PEX6         601498.0                614863                  Peroxisome biogenesis disorder 4B  613                  Peroxisome biogenesis disorder 4B"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dataframe mapping plant genes --> human orthologs --> disease phenotypes.\n",
    "omim_df = pd.read_csv(panther_to_omim_filename)\n",
    "# Add a column that indicates which ID in the dataset those plant genes refer to, for mapping to phenotypes.\n",
    "name_to_id = dataset.get_name_to_id_dictionary()\n",
    "omim_df[\"id\"] = omim_df[\"gene_identifier\"].map(lambda x: name_to_id.get(x,None))\n",
    "omim_df = omim_df.dropna(subset=[\"id\",\"phenotype_mim_name\"], inplace=False)\n",
    "omim_df[\"phenotype_mim_name\"] = omim_df[\"phenotype_mim_name\"].astype(str)\n",
    "omim_df[\"compressed_phenotype_mim_name\"] = omim_df[\"phenotype_mim_name\"].map(lambda x: x.split(\",\")[0])\n",
    "omim_df[\"id\"] = omim_df[\"id\"].astype(\"int64\")\n",
    "omim_df[\"phenotype_mim_number\"] = omim_df[\"phenotype_mim_number\"].astype(\"int64\")\n",
    "# Generate mappings between the IDs in this dataset and disease phenotypes or orthologous genes.\n",
    "id_to_mim_phenotype_names = defaultdict(list)\n",
    "for i,p in zip(omim_df[\"id\"].values,omim_df[\"compressed_phenotype_mim_name\"].values):\n",
    "    id_to_mim_phenotype_names[i].append(p)\n",
    "id_to_human_gene_symbols = defaultdict(list)\n",
    "for i,s in zip(omim_df[\"id\"].values,omim_df[\"human_ortholog_gene_symbol\"].values):\n",
    "    id_to_human_gene_symbols[i].append(s)\n",
    "omim_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed_phenotype_mim_name\n",
      "3-methylglutaconic aciduria                                                                  1\n",
      "5-oxoprolinase deficiency                                                                    1\n",
      "?Aniridia 2                                                                                  1\n",
      "?Fanconi anemia                                                                              1\n",
      "?Glutathioninuria                                                                            1\n",
      "?Glycogen storage disease XV                                                                 1\n",
      "?Mitochondrial DNA depletion syndrome 14 (encephalocardiomyopathic type)                     3\n",
      "?Mungan syndrome                                                                             1\n",
      "?Orofacial cleft 10                                                                          1\n",
      "?Premature ovarian failure 13                                                                1\n",
      "?Spastic paraplegia 63                                                                       1\n",
      "?Tumoral calcinosis                                                                          2\n",
      "ACTH-independent macronodular adrenal hyperplasia                                            1\n",
      "Achromatopsia 4                                                                              1\n",
      "Acute myeloid leukemia                                                                       1\n",
      "Adenocarcinoma of lung                                                                       3\n",
      "Alopecia universalis                                                                         1\n",
      "Amyotrophic lateral sclerosis 21                                                             1\n",
      "Aromatase deficiency                                                                         2\n",
      "Aromatase excess syndrome                                                                    2\n",
      "Atrichia with papular lesions                                                                1\n",
      "Auriculocondylar syndrome 1                                                                  1\n",
      "Basal ganglia calcification                                                                  1\n",
      "Behr syndrome                                                                                3\n",
      "Bietti crystalline corneoretinal dystrophy                                                   2\n",
      "Blau syndrome                                                                                1\n",
      "Bloom syndrome                                                                               1\n",
      "Bone marrow failure syndrome 4                                                               1\n",
      "Capillary malformations                                                                      1\n",
      "Cardiofaciocutaneous syndrome                                                                2\n",
      "Cardiomyopathy                                                                               3\n",
      "Carnitine deficiency                                                                         1\n",
      "Cataract 44                                                                                  1\n",
      "Catel-Manzke syndrome                                                                        1\n",
      "Centronuclear myopathy 1                                                                     3\n",
      "Cerebellar ataxia                                                                            2\n",
      "Cerebroretinal microangiopathy with calcifications and cysts 2                               1\n",
      "Cerebrotendinous xanthomatosis                                                               2\n",
      "Charcot-Marie-Tooth disease                                                                  6\n",
      "Chediak-Higashi syndrome                                                                     2\n",
      "Cholestasis                                                                                  3\n",
      "Chromosome 5q14.3 deletion syndrome                                                          7\n",
      "Chronic granulomatous disease                                                                1\n",
      "Cleft palate                                                                                 4\n",
      "Coffin-Siris syndrome 1                                                                      1\n",
      "Coffin-Siris syndrome 2                                                                      1\n",
      "Coffin-Siris syndrome 4                                                                      1\n",
      "Coffin-Siris syndrome 8                                                                      1\n",
      "Cohen-Gibson syndrome                                                                        1\n",
      "Colorectal cancer                                                                            2\n",
      "Combined oxidative phosphorylation deficiency 20                                             1\n",
      "Combined oxidative phosphorylation deficiency 28                                             2\n",
      "Combined oxidative phosphorylation deficiency 39                                             1\n",
      "Cone-rod dystrophy 5                                                                         1\n",
      "Congenital anomalies of kidney and urinary tract syndrome with or without hearing loss       1\n",
      "Congenital disorder of glycosylation                                                         3\n",
      "Cornelia de Lange syndrome 1                                                                 1\n",
      "Cornelia de Lange syndrome 4                                                                 1\n",
      "Cortical dysplasia                                                                           1\n",
      "Craniosynostosis with radiohumeral fusions and other skeletal and craniofacial anomalies     2\n",
      "Cutis laxa                                                                                   3\n",
      "Deafness                                                                                     6\n",
      "Diabetes mellitus                                                                            2\n",
      "Dystonia 25                                                                                  1\n",
      "Ehlers-Danlos syndrome                                                                       1\n",
      "Epileptic encephalopathy                                                                     5\n",
      "FILS syndrome                                                                                1\n",
      "Fanconi anemia                                                                               1\n",
      "Fetal akinesia deformation sequence 4                                                        1\n",
      "Focal facial dermal dysplasia 4                                                              2\n",
      "Fructose intolerance                                                                         1\n",
      "GLOW syndrome                                                                                1\n",
      "Generalized epilepsy with febrile seizures plus                                              1\n",
      "Glutaric acidemia IIC                                                                        1\n",
      "Glycogen storage disease IV                                                                  1\n",
      "Glycogen storage disease XII                                                                 1\n",
      "Goiter                                                                                       1\n",
      "Gonadal dysgenesis                                                                           1\n",
      "Griscelli syndrome                                                                           1\n",
      "HMG-CoA lyase deficiency                                                                     1\n",
      "Heimler syndrome 2                                                                           2\n",
      "Hemolytic anemia due to hexokinase deficiency                                                1\n",
      "Hemophagocytic lymphohistiocytosis                                                           1\n",
      "Heyn-Sproul-Jackson syndrome                                                                 1\n",
      "Holoprosencephaly 4                                                                          4\n",
      "Hypercalcemia                                                                                2\n",
      "Hyperinsulinemic hypoglycemia                                                                1\n",
      "Hyperparathyroidism                                                                          1\n",
      "Hyperparathyroidism-jaw tumor syndrome                                                       1\n",
      "Hypocalcemia                                                                                 1\n",
      "Hypocalciuric hypercalcemia                                                                  1\n",
      "Hypotrichosis 14                                                                             1\n",
      "Hypotrichosis 4                                                                              1\n",
      "Hypouricemia                                                                                 1\n",
      "IMAGE-I syndrome                                                                             1\n",
      "Ichthyosis                                                                                   4\n",
      "Immunodeficiency                                                                             1\n",
      "Immunodeficiency 34                                                                          1\n",
      "Immunodeficiency-centromeric instability-facial anomalies syndrome 1                         1\n",
      "Intellectual developmental disorder with dysmorphic facies and ptosis                        1\n",
      "Johanson-Blizzard syndrome                                                                   1\n",
      "LEOPARD syndrome 2                                                                           2\n",
      "Lactase deficiency                                                                           2\n",
      "Lethal congenital contracture syndrome 5                                                     3\n",
      "Leukemia                                                                                     1\n",
      "Leukodystrophy                                                                               1\n",
      "Leukoencephalopathy                                                                          1\n",
      "MODY                                                                                         1\n",
      "McCune-Albright syndrome                                                                     1\n",
      "Megakaryoblastic leukemia                                                                    1\n",
      "Menke-Hennekam syndrome 1                                                                    1\n",
      "Menke-Hennekam syndrome 2                                                                    1\n",
      "Menkes disease                                                                               1\n",
      "Mental retardation                                                                          15\n",
      "Microcephaly                                                                                 1\n",
      "Microvillus inclusion disease                                                                1\n",
      "Mirror movements 2                                                                           1\n",
      "Mismatch repair cancer syndrome                                                              1\n",
      "Mitochondrial pyruvate carrier deficiency                                                    1\n",
      "Molybdenum cofactor deficiency C                                                             1\n",
      "Muir-Torre syndrome                                                                          1\n",
      "Multiple joint dislocations                                                                  2\n",
      "Muscular dystrophy-dystroglycanopathy (congenital with brain and eye anomalies               1\n",
      "Muscular dystrophy-dystroglycanopathy (congenital with brain and eye anomalies)              1\n",
      "Muscular dystrophy-dystroglycanopathy (congenital with mental retardation)                   1\n",
      "Muscular dystrophy-dystroglycanopathy (limb-girdle)                                          1\n",
      "Myelofibrosis                                                                                1\n",
      "Myopathy due to myoadenylate deaminase deficiency                                            1\n",
      "Neurodevelopmental disorder with absent language and variable seizures                       3\n",
      "Neurodevelopmental disorder with involuntary movements                                       1\n",
      "Neurodevelopmental disorder with visual defects and brain anomalies                          1\n",
      "Neuropathy                                                                                   4\n",
      "Nicolaides-Baraitser syndrome                                                                1\n",
      "Night blindness                                                                              2\n",
      "Nijmegen breakage syndrome-like disorder                                                     1\n",
      "Noonan syndrome 5                                                                            2\n",
      "Occipital horn syndrome                                                                      1\n",
      "Oocyte maturation defect 4                                                                   2\n",
      "Oocyte maturation defect 5                                                                   2\n",
      "Optic atrophy 1                                                                              3\n",
      "Optic atrophy plus syndrome                                                                  3\n",
      "Osseous heteroplasia                                                                         1\n",
      "Osteopetrosis                                                                                1\n",
      "Ovarian cancer                                                                               1\n",
      "Ovarian dysgenesis 5                                                                         1\n",
      "Parathyroid adenoma with cystic changes                                                      1\n",
      "Parathyroid carcinoma                                                                        1\n",
      "Parkinson disease                                                                            1\n",
      "Paroxysmal nonkinesigenic dyskinesia 1                                                       1\n",
      "Peroxisome biogenesis disorder 11A (Zellweger)                                               1\n",
      "Peroxisome biogenesis disorder 11B                                                           1\n",
      "Peroxisome biogenesis disorder 13A (Zellweger)                                               2\n",
      "Peroxisome biogenesis disorder 4A (Zellweger)                                                2\n",
      "Peroxisome biogenesis disorder 4B                                                            2\n",
      "Peroxisome biogenesis disorder 9B                                                            2\n",
      "Pituitary adenoma 3                                                                          1\n",
      "Pituitary adenoma 4                                                                          1\n",
      "Pleuropulmonary blastoma                                                                     1\n",
      "Polyglucosan body disease                                                                    1\n",
      "Polyglucosan body myopathy 2                                                                 1\n",
      "Pontocerebellar hypoplasia                                                                   1\n",
      "Pontocerebellar hypoplasia type 1A                                                           1\n",
      "Porokeratosis 7                                                                              1\n",
      "Protoporphyria                                                                               2\n",
      "Pseudohypoaldosteronism                                                                      3\n",
      "Pseudohypoparathyroidism Ia                                                                  1\n",
      "Pseudohypoparathyroidism Ib                                                                  1\n",
      "Pseudohypoparathyroidism Ic                                                                  1\n",
      "Pseudopseudohypoparathyroidism                                                               1\n",
      "Psychomotor retardation                                                                      1\n",
      "Pyridoxamine 5'-phosphate oxidase deficiency                                                 1\n",
      "Renal tubular acidosis                                                                       1\n",
      "Retinitis pigmentosa 79                                                                      1\n",
      "Rhabdomyosarcoma                                                                             1\n",
      "Rhizomelic chondrodysplasia punctata                                                         2\n",
      "Rubinstein-Taybi syndrome 1                                                                  1\n",
      "Rubinstein-Taybi syndrome 2                                                                  1\n",
      "Smith-McCort dysplasia 2                                                                     1\n",
      "Spastic paraplegia 13                                                                        1\n",
      "Spastic paraplegia 28                                                                        1\n",
      "Spastic paraplegia 54                                                                        1\n",
      "Spastic paraplegia 9A                                                                        1\n",
      "Spastic paraplegia 9B                                                                        1\n",
      "Spermatogenic failure 32                                                                     1\n",
      "Spermatogenic failure 36                                                                     1\n",
      "Spinal muscular atrophy                                                                      1\n",
      "Spondyloepimetaphyseal dysplasia                                                             1\n",
      "Spondyloepimetaphyseal dysplasia with joint laxity                                           1\n",
      "Spondylometaphyseal dysplasia                                                                1\n",
      "Striatonigral degeneration                                                                   1\n",
      "Sturge-Weber syndrome                                                                        1\n",
      "Succinic semialdehyde dehydrogenase deficiency                                               1\n",
      "Symmetric circumferential skin creases                                                       2\n",
      "Tatton-Brown-Rahman syndrome                                                                 1\n",
      "Thiamine metabolism dysfunction syndrome 5 (episodic encephalopathy type)                    2\n",
      "Thrombocythemia                                                                              1\n",
      "Thyroid dyshormonogenesis 6                                                                  1\n",
      "Trichothiodystrophy 2                                                                        1\n",
      "Tumor predisposition syndrome                                                                1\n",
      "Usher syndrome                                                                               1\n",
      "Vitamin D-dependent rickets                                                                  2\n",
      "Weaver syndrome                                                                              2\n",
      "Wilson disease                                                                               1\n",
      "Wrinkly skin syndrome                                                                        1\n",
      "XFE progeroid syndrome                                                                       1\n",
      "Xanthinuria                                                                                  1\n",
      "Xeroderma pigmentosum                                                                        3\n",
      "[AMP deaminase deficiency                                                                    1\n",
      "[Blood group                                                                                 2\n",
      "[Gamma-glutamyltransferase                                                                   1\n",
      "[Glyoxalase II deficiency]                                                                   1\n",
      "{Aerodigestive tract cancer                                                                  1\n",
      "{Alcohol dependence                                                                          2\n",
      "{Asthma                                                                                      1\n",
      "{Autism                                                                                      1\n",
      "{Breast cancer                                                                               1\n",
      "{Colorectal cancer                                                                           1\n",
      "{Coronary artery disease                                                                     7\n",
      "{Glaucoma                                                                                    3\n",
      "{Inflammatory bowel disease 1                                                                1\n",
      "{Parkinson disease                                                                           1\n",
      "{Parkinson disease 18}                                                                       1\n",
      "{Parkinson disease 8}                                                                        2\n",
      "{Psoriatic arthritis                                                                         1\n",
      "{Rhabdoid tumor predisposition syndrome 2}                                                   1\n",
      "{Rheumatoid arthritis                                                                        1\n",
      "{Yao syndrome}                                                                               1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How many genes in our dataset map to orthologs that map to the same OMIM phenotype?\n",
    "print(omim_df.groupby(\"compressed_phenotype_mim_name\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>15</th>\n",
       "      <th>40</th>\n",
       "      <th>36</th>\n",
       "      <th>17</th>\n",
       "      <th>3</th>\n",
       "      <th>16</th>\n",
       "      <th>20</th>\n",
       "      <th>12</th>\n",
       "      <th>27</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>29</th>\n",
       "      <th>4</th>\n",
       "      <th>14</th>\n",
       "      <th>19</th>\n",
       "      <th>26</th>\n",
       "      <th>28</th>\n",
       "      <th>33</th>\n",
       "      <th>11</th>\n",
       "      <th>10</th>\n",
       "      <th>34</th>\n",
       "      <th>25</th>\n",
       "      <th>7</th>\n",
       "      <th>41</th>\n",
       "      <th>35</th>\n",
       "      <th>5</th>\n",
       "      <th>30</th>\n",
       "      <th>32</th>\n",
       "      <th>9</th>\n",
       "      <th>23</th>\n",
       "      <th>37</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>39</th>\n",
       "      <th>31</th>\n",
       "      <th>24</th>\n",
       "      <th>13</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Thiamine metabolism dysfunction syndrome 5 (episodic encephalopathy type)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weaver syndrome</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heimler syndrome 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peroxisome biogenesis disorder 4A (Zellweger)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peroxisome biogenesis disorder 4B</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1   15  40  36  17  3   16  20  12  27  0   2   6   8   38  29  4   14  19  26  28  33  11  10  34  25  7   41  35  5   30  32  9   23  37  22  21  39  31  24  13  18\n",
       "Thiamine metabolism dysfunction syndrome 5 (epi...   0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "Weaver syndrome                                      0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "Heimler syndrome 2                                   0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "Peroxisome biogenesis disorder 4A (Zellweger)        0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "Peroxisome biogenesis disorder 4B                    0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenolog_x_dict = defaultdict(dict)\n",
    "phenolog_p_dict = defaultdict(dict)\n",
    "candidate_genes_dict = defaultdict(dict)\n",
    "phenotypes = pd.unique(omim_df[\"compressed_phenotype_mim_name\"].values)\n",
    "clusters = list(cluster_to_ids.keys())\n",
    "for cluster,phenotype in itertools.product(clusters,phenotypes):\n",
    "    \n",
    "    # What are the candidate genes predicted if this phenolog pairing is real?\n",
    "    ids = cluster_to_ids[cluster]\n",
    "    candidate_genes_dict[cluster][phenotype] = list(set(flatten([id_to_human_gene_symbols[i] for i in ids if phenotype not in id_to_mim_phenotype_names.get(i,[])])))\n",
    "\n",
    "    # What is the p-value for this phenolog pairing?\n",
    "    # The size of the population (genes in the dataset).\n",
    "    M = len(id_to_cluster.keys())\n",
    "    # The number of elements we draw without replacement (genes in the cluster).\n",
    "    N = len(cluster_to_ids[cluster])     \n",
    "    # The number of available successes in the population (genes that map to orthologs that map to this phenotype).\n",
    "    n = len([i for i in id_to_cluster.keys() if phenotype in id_to_mim_phenotype_names.get(i,[])])\n",
    "    # The number of successes drawn (genes in this cluster that map to orthologs that map to this phenotype).\n",
    "    x = list(set(flatten([id_to_mim_phenotype_names.get(i,[]) for i in ids]))).count(phenotype)\n",
    "    prob = 1-hypergeom.cdf(x-1, M, n, N) # Equivalent to prob = 1-sum([hypergeom.pmf(x_i, M, n, N) for x_i in range(0,x)])\n",
    "    phenolog_x_dict[cluster][phenotype] = x\n",
    "    phenolog_p_dict[cluster][phenotype] = prob\n",
    "    \n",
    "\n",
    "# Convert the dictionary to a table of values with cluster and phenotype as the rows and columns.\n",
    "phenolog_matrix = pd.DataFrame(phenolog_x_dict)        \n",
    "phenolog_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omim_phenotype_name</th>\n",
       "      <th>cluster</th>\n",
       "      <th>size</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "      <th>p_adjusted</th>\n",
       "      <th>candidate_gene_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8991</th>\n",
       "      <td>Cortical dysplasia</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>{Yao syndrome}</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>Blau syndrome</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>Bloom syndrome</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8990</th>\n",
       "      <td>Fetal akinesia deformation sequence 4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659</th>\n",
       "      <td>5-oxoprolinase deficiency</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>[Glyoxalase II deficiency]</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>Paroxysmal nonkinesigenic dyskinesia 1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>Glutaric acidemia IIC</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>{Psoriatic arthritis</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>Megakaryoblastic leukemia</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>Muir-Torre syndrome</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8487</th>\n",
       "      <td>Mismatch repair cancer syndrome</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>?Orofacial cleft 10</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>Spondylometaphyseal dysplasia</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>Smith-McCort dysplasia 2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8989</th>\n",
       "      <td>Cornelia de Lange syndrome 4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>?Mungan syndrome</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8353</th>\n",
       "      <td>Immunodeficiency 34</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>Chronic granulomatous disease</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>Thyroid dyshormonogenesis 6</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>Hypotrichosis 14</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5235</th>\n",
       "      <td>{Inflammatory bowel disease 1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8485</th>\n",
       "      <td>Polyglucosan body myopathy 2</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>{Asthma</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8484</th>\n",
       "      <td>?Glycogen storage disease XV</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>Cataract 44</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Microvillus inclusion disease</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>3-methylglutaconic aciduria</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TLK2|GPHN|RAD51|XDH|ATP7A|USP8|ATP7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>Mirror movements 2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DNMT3B|CLPB|DNMT3A|NIPBL|MEF2C|MEF2A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         omim_phenotype_name cluster  size  x   p_value  p_adjusted                candidate_gene_symbols\n",
       "8991                      Cortical dysplasia      13     1  1  0.010753         1.0                                      \n",
       "5237                          {Yao syndrome}      34     1  1  0.010753         1.0                                      \n",
       "5238                           Blau syndrome      34     1  1  0.010753         1.0                                      \n",
       "5239                          Bloom syndrome      34     1  1  0.010753         1.0                                      \n",
       "8990   Fetal akinesia deformation sequence 4      13     1  1  0.010753         1.0                                      \n",
       "7659               5-oxoprolinase deficiency      37     1  1  0.010753         1.0                                      \n",
       "5972              [Glyoxalase II deficiency]      41     1  1  0.010753         1.0                                      \n",
       "5971  Paroxysmal nonkinesigenic dyskinesia 1      41     1  1  0.010753         1.0                                      \n",
       "5970                   Glutaric acidemia IIC      41     1  1  0.010753         1.0                                      \n",
       "5236                    {Psoriatic arthritis      34     1  1  0.010753         1.0                                      \n",
       "5968               Megakaryoblastic leukemia      41     1  1  0.010753         1.0                                      \n",
       "8488                     Muir-Torre syndrome      31     1  1  0.010753         1.0                                      \n",
       "8487         Mismatch repair cancer syndrome      31     1  1  0.010753         1.0                                      \n",
       "6700                     ?Orofacial cleft 10      30     1  1  0.010753         1.0                                      \n",
       "5969           Spondylometaphyseal dysplasia      41     1  1  0.010753         1.0                                      \n",
       "1911                Smith-McCort dysplasia 2      27     1  1  0.010753         1.0                                      \n",
       "8989            Cornelia de Lange syndrome 4      13     1  1  0.010753         1.0                                      \n",
       "8988                        ?Mungan syndrome      13     1  1  0.010753         1.0                                      \n",
       "8353                     Immunodeficiency 34      39     1  1  0.010753         1.0                                      \n",
       "8352           Chronic granulomatous disease      39     1  1  0.010753         1.0                                      \n",
       "8351             Thyroid dyshormonogenesis 6      39     1  1  0.010753         1.0                                      \n",
       "8350                        Hypotrichosis 14      39     1  1  0.010753         1.0                                      \n",
       "5235           {Inflammatory bowel disease 1      34     1  1  0.010753         1.0                                      \n",
       "8485            Polyglucosan body myopathy 2      31     1  1  0.010753         1.0                                      \n",
       "1913                                 {Asthma      27     1  1  0.010753         1.0                                      \n",
       "8484            ?Glycogen storage disease XV      31     1  1  0.010753         1.0                                      \n",
       "8349                             Cataract 44      39     1  1  0.010753         1.0                                      \n",
       "2195           Microvillus inclusion disease       0     2  1  0.021505         1.0                                      \n",
       "2868             3-methylglutaconic aciduria       8     2  1  0.021505         1.0  TLK2|GPHN|RAD51|XDH|ATP7A|USP8|ATP7B\n",
       "2891                      Mirror movements 2       8     2  1  0.021505         1.0  DNMT3B|CLPB|DNMT3A|NIPBL|MEF2C|MEF2A"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a melted version of the phenolog matrix sorted by value and including predicted candidate genes.\n",
    "phenolog_matrix_reset = phenolog_matrix.reset_index(drop=False).rename({\"index\":\"omim_phenotype_name\"}, axis=\"columns\")\n",
    "phenolog_df = pd.melt(phenolog_matrix_reset, id_vars=[\"omim_phenotype_name\"], value_vars=phenolog_matrix.columns[1:], var_name=\"cluster\", value_name=\"x\")\n",
    "# What other information should be present in this melted phenologs matrix?\n",
    "phenolog_df[\"size\"] = phenolog_df[\"cluster\"].map(lambda x: len(cluster_to_ids[x]))\n",
    "phenolog_df[\"candidate_gene_symbols\"] = np.vectorize(lambda x,y: concatenate_with_bar_delim(*candidate_genes_dict[x][y]))(phenolog_df[\"cluster\"], phenolog_df[\"omim_phenotype_name\"])\n",
    "phenolog_df[\"p_value\"] = np.vectorize(lambda x,y: phenolog_p_dict[x][y])(phenolog_df[\"cluster\"], phenolog_df[\"omim_phenotype_name\"])\n",
    "phenolog_df[\"p_adjusted\"] = multipletests(phenolog_df[\"p_value\"].values, method='bonferroni')[1]\n",
    "#phenolog_df.sort_values(by=[\"x\"], inplace=True, ascending=False)\n",
    "phenolog_df.sort_values(by=[\"p_value\"], inplace=True, ascending=True)\n",
    "phenolog_df = phenolog_df[[\"omim_phenotype_name\", \"cluster\", \"size\", \"x\", \"p_value\", \"p_adjusted\", \"candidate_gene_symbols\"]]\n",
    "phenolog_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_phenologs.csv\"), index=False)\n",
    "phenolog_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: Agglomerative clustering and sillhouette scores for each NLP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Doc2Vec:PubMed,Size=100</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Max</th>\n",
       "      <th>BERT: Base:Layers=4,Concatenated</th>\n",
       "      <th>BioBERT:PubMed,PMC,Layers=4,Concatenated</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>NOBLE Coder:Precise</th>\n",
       "      <th>NOBLE Coder:Partial</th>\n",
       "      <th>NOBLE Coder:Precise,TFIDF</th>\n",
       "      <th>NOBLE Coder:Partial,TFIDF</th>\n",
       "      <th>GO:</th>\n",
       "      <th>PO:</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Max</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.136673</td>\n",
       "      <td>0.274092</td>\n",
       "      <td>0.301001</td>\n",
       "      <td>0.286630</td>\n",
       "      <td>0.109204</td>\n",
       "      <td>0.163610</td>\n",
       "      <td>0.121282</td>\n",
       "      <td>0.069286</td>\n",
       "      <td>0.173644</td>\n",
       "      <td>0.079993</td>\n",
       "      <td>0.086348</td>\n",
       "      <td>0.075841</td>\n",
       "      <td>0.127895</td>\n",
       "      <td>0.102930</td>\n",
       "      <td>0.165833</td>\n",
       "      <td>0.080925</td>\n",
       "      <td>0.187848</td>\n",
       "      <td>0.108788</td>\n",
       "      <td>0.101337</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>0.132853</td>\n",
       "      <td>0.114979</td>\n",
       "      <td>0.122034</td>\n",
       "      <td>0.192132</td>\n",
       "      <td>0.099080</td>\n",
       "      <td>0.050259</td>\n",
       "      <td>0.174908</td>\n",
       "      <td>0.102361</td>\n",
       "      <td>0.122716</td>\n",
       "      <td>0.084794</td>\n",
       "      <td>0.217753</td>\n",
       "      <td>0.086384</td>\n",
       "      <td>0.189687</td>\n",
       "      <td>0.116444</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>0.232276</td>\n",
       "      <td>0.275686</td>\n",
       "      <td>0.208827</td>\n",
       "      <td>0.091135</td>\n",
       "      <td>0.338963</td>\n",
       "      <td>0.342149</td>\n",
       "      <td>0.450186</td>\n",
       "      <td>0.232720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0.137677</td>\n",
       "      <td>0.290331</td>\n",
       "      <td>0.223098</td>\n",
       "      <td>0.322311</td>\n",
       "      <td>0.103053</td>\n",
       "      <td>0.165427</td>\n",
       "      <td>0.148398</td>\n",
       "      <td>0.078473</td>\n",
       "      <td>0.190788</td>\n",
       "      <td>0.101108</td>\n",
       "      <td>0.122023</td>\n",
       "      <td>0.082515</td>\n",
       "      <td>0.146649</td>\n",
       "      <td>0.113545</td>\n",
       "      <td>0.169024</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>0.160096</td>\n",
       "      <td>0.108526</td>\n",
       "      <td>0.121753</td>\n",
       "      <td>0.104547</td>\n",
       "      <td>0.153796</td>\n",
       "      <td>0.126619</td>\n",
       "      <td>0.144069</td>\n",
       "      <td>0.196093</td>\n",
       "      <td>0.104279</td>\n",
       "      <td>0.062627</td>\n",
       "      <td>0.188159</td>\n",
       "      <td>0.119143</td>\n",
       "      <td>0.141075</td>\n",
       "      <td>0.094153</td>\n",
       "      <td>0.283354</td>\n",
       "      <td>0.121842</td>\n",
       "      <td>0.198110</td>\n",
       "      <td>0.116825</td>\n",
       "      <td>0.365728</td>\n",
       "      <td>0.230984</td>\n",
       "      <td>0.315515</td>\n",
       "      <td>0.219901</td>\n",
       "      <td>0.095861</td>\n",
       "      <td>0.344820</td>\n",
       "      <td>0.452546</td>\n",
       "      <td>0.487303</td>\n",
       "      <td>0.303812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>0.141973</td>\n",
       "      <td>0.300382</td>\n",
       "      <td>0.237983</td>\n",
       "      <td>0.201867</td>\n",
       "      <td>0.123453</td>\n",
       "      <td>0.177594</td>\n",
       "      <td>0.154838</td>\n",
       "      <td>0.082809</td>\n",
       "      <td>0.202542</td>\n",
       "      <td>0.111583</td>\n",
       "      <td>0.128729</td>\n",
       "      <td>0.109886</td>\n",
       "      <td>0.156109</td>\n",
       "      <td>0.118718</td>\n",
       "      <td>0.203315</td>\n",
       "      <td>0.096696</td>\n",
       "      <td>0.184749</td>\n",
       "      <td>0.122888</td>\n",
       "      <td>0.136116</td>\n",
       "      <td>0.106995</td>\n",
       "      <td>0.152605</td>\n",
       "      <td>0.136543</td>\n",
       "      <td>0.168857</td>\n",
       "      <td>0.203918</td>\n",
       "      <td>0.136311</td>\n",
       "      <td>0.105635</td>\n",
       "      <td>0.204516</td>\n",
       "      <td>0.129363</td>\n",
       "      <td>0.148331</td>\n",
       "      <td>0.096893</td>\n",
       "      <td>0.271245</td>\n",
       "      <td>0.120020</td>\n",
       "      <td>0.207860</td>\n",
       "      <td>0.110110</td>\n",
       "      <td>0.367616</td>\n",
       "      <td>0.233721</td>\n",
       "      <td>0.327786</td>\n",
       "      <td>0.226869</td>\n",
       "      <td>0.094231</td>\n",
       "      <td>0.357175</td>\n",
       "      <td>0.452283</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.290822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0.136213</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.242074</td>\n",
       "      <td>0.207769</td>\n",
       "      <td>0.122701</td>\n",
       "      <td>0.170231</td>\n",
       "      <td>0.167670</td>\n",
       "      <td>0.082994</td>\n",
       "      <td>0.189973</td>\n",
       "      <td>0.113666</td>\n",
       "      <td>0.139463</td>\n",
       "      <td>0.113798</td>\n",
       "      <td>0.165610</td>\n",
       "      <td>0.140687</td>\n",
       "      <td>0.185639</td>\n",
       "      <td>0.114155</td>\n",
       "      <td>0.192676</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>0.129567</td>\n",
       "      <td>0.117368</td>\n",
       "      <td>0.165804</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.167397</td>\n",
       "      <td>0.210848</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>0.110090</td>\n",
       "      <td>0.219107</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>0.165543</td>\n",
       "      <td>0.113373</td>\n",
       "      <td>0.263570</td>\n",
       "      <td>0.112273</td>\n",
       "      <td>0.227990</td>\n",
       "      <td>0.114494</td>\n",
       "      <td>0.363554</td>\n",
       "      <td>0.257226</td>\n",
       "      <td>0.332633</td>\n",
       "      <td>0.232973</td>\n",
       "      <td>0.091711</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.553821</td>\n",
       "      <td>0.605603</td>\n",
       "      <td>0.285493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>0.146299</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.238879</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.119520</td>\n",
       "      <td>0.159883</td>\n",
       "      <td>0.169742</td>\n",
       "      <td>0.089193</td>\n",
       "      <td>0.202648</td>\n",
       "      <td>0.113337</td>\n",
       "      <td>0.138948</td>\n",
       "      <td>0.125951</td>\n",
       "      <td>0.166372</td>\n",
       "      <td>0.144004</td>\n",
       "      <td>0.176172</td>\n",
       "      <td>0.116784</td>\n",
       "      <td>0.217816</td>\n",
       "      <td>0.133439</td>\n",
       "      <td>0.152268</td>\n",
       "      <td>0.114918</td>\n",
       "      <td>0.171079</td>\n",
       "      <td>0.147711</td>\n",
       "      <td>0.167784</td>\n",
       "      <td>0.210635</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>0.110090</td>\n",
       "      <td>0.215240</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>0.175595</td>\n",
       "      <td>0.147277</td>\n",
       "      <td>0.256474</td>\n",
       "      <td>0.120820</td>\n",
       "      <td>0.218424</td>\n",
       "      <td>0.133778</td>\n",
       "      <td>0.359920</td>\n",
       "      <td>0.258032</td>\n",
       "      <td>0.323063</td>\n",
       "      <td>0.240119</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>0.466180</td>\n",
       "      <td>0.573651</td>\n",
       "      <td>0.637747</td>\n",
       "      <td>0.250547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>0.155994</td>\n",
       "      <td>0.260456</td>\n",
       "      <td>0.244043</td>\n",
       "      <td>0.193645</td>\n",
       "      <td>0.118097</td>\n",
       "      <td>0.148091</td>\n",
       "      <td>0.167817</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.179508</td>\n",
       "      <td>0.111542</td>\n",
       "      <td>0.139327</td>\n",
       "      <td>0.125992</td>\n",
       "      <td>0.161332</td>\n",
       "      <td>0.140299</td>\n",
       "      <td>0.183605</td>\n",
       "      <td>0.117536</td>\n",
       "      <td>0.215158</td>\n",
       "      <td>0.139967</td>\n",
       "      <td>0.154262</td>\n",
       "      <td>0.126498</td>\n",
       "      <td>0.183920</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.156192</td>\n",
       "      <td>0.217486</td>\n",
       "      <td>0.163250</td>\n",
       "      <td>0.118216</td>\n",
       "      <td>0.215822</td>\n",
       "      <td>0.143818</td>\n",
       "      <td>0.174521</td>\n",
       "      <td>0.160133</td>\n",
       "      <td>0.254368</td>\n",
       "      <td>0.117010</td>\n",
       "      <td>0.226511</td>\n",
       "      <td>0.144053</td>\n",
       "      <td>0.368838</td>\n",
       "      <td>0.253560</td>\n",
       "      <td>0.308238</td>\n",
       "      <td>0.239486</td>\n",
       "      <td>0.084918</td>\n",
       "      <td>0.476786</td>\n",
       "      <td>0.626957</td>\n",
       "      <td>0.667956</td>\n",
       "      <td>0.237406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44</td>\n",
       "      <td>0.168241</td>\n",
       "      <td>0.249411</td>\n",
       "      <td>0.237927</td>\n",
       "      <td>0.201699</td>\n",
       "      <td>0.121972</td>\n",
       "      <td>0.139732</td>\n",
       "      <td>0.179928</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>0.178305</td>\n",
       "      <td>0.126960</td>\n",
       "      <td>0.141774</td>\n",
       "      <td>0.129450</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.139634</td>\n",
       "      <td>0.190412</td>\n",
       "      <td>0.121957</td>\n",
       "      <td>0.216961</td>\n",
       "      <td>0.146717</td>\n",
       "      <td>0.153895</td>\n",
       "      <td>0.128867</td>\n",
       "      <td>0.203993</td>\n",
       "      <td>0.170769</td>\n",
       "      <td>0.174781</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.171153</td>\n",
       "      <td>0.136147</td>\n",
       "      <td>0.196040</td>\n",
       "      <td>0.158010</td>\n",
       "      <td>0.188891</td>\n",
       "      <td>0.171766</td>\n",
       "      <td>0.226952</td>\n",
       "      <td>0.118456</td>\n",
       "      <td>0.222196</td>\n",
       "      <td>0.141378</td>\n",
       "      <td>0.360956</td>\n",
       "      <td>0.284006</td>\n",
       "      <td>0.301541</td>\n",
       "      <td>0.261712</td>\n",
       "      <td>0.079641</td>\n",
       "      <td>0.479253</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.229432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>0.171470</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.220429</td>\n",
       "      <td>0.199625</td>\n",
       "      <td>0.128759</td>\n",
       "      <td>0.136821</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.111882</td>\n",
       "      <td>0.180820</td>\n",
       "      <td>0.124909</td>\n",
       "      <td>0.147961</td>\n",
       "      <td>0.128826</td>\n",
       "      <td>0.197751</td>\n",
       "      <td>0.140433</td>\n",
       "      <td>0.187293</td>\n",
       "      <td>0.122456</td>\n",
       "      <td>0.216072</td>\n",
       "      <td>0.146547</td>\n",
       "      <td>0.154399</td>\n",
       "      <td>0.127289</td>\n",
       "      <td>0.202114</td>\n",
       "      <td>0.167526</td>\n",
       "      <td>0.191327</td>\n",
       "      <td>0.228304</td>\n",
       "      <td>0.170204</td>\n",
       "      <td>0.134544</td>\n",
       "      <td>0.206589</td>\n",
       "      <td>0.163214</td>\n",
       "      <td>0.189865</td>\n",
       "      <td>0.172058</td>\n",
       "      <td>0.217522</td>\n",
       "      <td>0.136401</td>\n",
       "      <td>0.221484</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.356960</td>\n",
       "      <td>0.279892</td>\n",
       "      <td>0.298692</td>\n",
       "      <td>0.254909</td>\n",
       "      <td>0.082347</td>\n",
       "      <td>0.509278</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.218883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>0.169335</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.214433</td>\n",
       "      <td>0.198940</td>\n",
       "      <td>0.130090</td>\n",
       "      <td>0.140413</td>\n",
       "      <td>0.162748</td>\n",
       "      <td>0.110223</td>\n",
       "      <td>0.189776</td>\n",
       "      <td>0.122657</td>\n",
       "      <td>0.148668</td>\n",
       "      <td>0.129163</td>\n",
       "      <td>0.197631</td>\n",
       "      <td>0.137157</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.121048</td>\n",
       "      <td>0.211985</td>\n",
       "      <td>0.146413</td>\n",
       "      <td>0.159005</td>\n",
       "      <td>0.127598</td>\n",
       "      <td>0.197576</td>\n",
       "      <td>0.166331</td>\n",
       "      <td>0.194137</td>\n",
       "      <td>0.223544</td>\n",
       "      <td>0.175161</td>\n",
       "      <td>0.141182</td>\n",
       "      <td>0.207555</td>\n",
       "      <td>0.165653</td>\n",
       "      <td>0.213643</td>\n",
       "      <td>0.183819</td>\n",
       "      <td>0.213005</td>\n",
       "      <td>0.143676</td>\n",
       "      <td>0.213276</td>\n",
       "      <td>0.161633</td>\n",
       "      <td>0.362150</td>\n",
       "      <td>0.275668</td>\n",
       "      <td>0.290343</td>\n",
       "      <td>0.266430</td>\n",
       "      <td>0.084192</td>\n",
       "      <td>0.526507</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.216653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56</td>\n",
       "      <td>0.170641</td>\n",
       "      <td>0.195913</td>\n",
       "      <td>0.218547</td>\n",
       "      <td>0.190295</td>\n",
       "      <td>0.110533</td>\n",
       "      <td>0.134781</td>\n",
       "      <td>0.157302</td>\n",
       "      <td>0.108351</td>\n",
       "      <td>0.181724</td>\n",
       "      <td>0.134840</td>\n",
       "      <td>0.148093</td>\n",
       "      <td>0.138068</td>\n",
       "      <td>0.208460</td>\n",
       "      <td>0.133865</td>\n",
       "      <td>0.183516</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.197729</td>\n",
       "      <td>0.146245</td>\n",
       "      <td>0.155123</td>\n",
       "      <td>0.141034</td>\n",
       "      <td>0.194669</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.185280</td>\n",
       "      <td>0.224992</td>\n",
       "      <td>0.177076</td>\n",
       "      <td>0.145192</td>\n",
       "      <td>0.203947</td>\n",
       "      <td>0.163612</td>\n",
       "      <td>0.219946</td>\n",
       "      <td>0.180249</td>\n",
       "      <td>0.194046</td>\n",
       "      <td>0.148395</td>\n",
       "      <td>0.203253</td>\n",
       "      <td>0.157213</td>\n",
       "      <td>0.347104</td>\n",
       "      <td>0.276814</td>\n",
       "      <td>0.286832</td>\n",
       "      <td>0.267665</td>\n",
       "      <td>0.091113</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.205457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n  Doc2Vec:Wikipedia,Size=300  Doc2Vec:PubMed,Size=100  Word2Vec:Wikipedia,Size=300,Mean  Word2Vec:Wikipedia,Size=300,Max  BERT: Base:Layers=4,Concatenated  BioBERT:PubMed,PMC,Layers=4,Concatenated  N-Grams:Full,Words,1-grams,2-grams  N-Grams:Full,Words,1-grams,2-grams,Binary  N-Grams:Full,Words,1-grams  N-Grams:Full,Words,1-grams,Binary  N-Grams:Full,Words,1-grams,2-grams,TFIDF  N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  N-Grams:Full,Words,1-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,2-grams  N-Grams:Simple,Words,1-grams,2-grams,Binary  N-Grams:Simple,Words,1-grams  N-Grams:Simple,Words,1-grams,Binary  N-Grams:Simple,Words,1-grams,2-grams,TFIDF  N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,TFIDF  N-Grams:Simple,Words,1-grams,Binary,TFIDF  N-Grams:Full,Nouns,1-grams  N-Grams:Full,Nouns,1-grams,Binary  N-Grams:Full,Nouns,1-grams,TFIDF  N-Grams:Full,Nouns,1-grams,Binary,TFIDF  \\\n",
       "0  20                    0.136673                 0.274092                          0.301001                         0.286630                          0.109204                                  0.163610                            0.121282                                   0.069286                    0.173644                           0.079993                                  0.086348                                         0.075841                          0.127895                                 0.102930                              0.165833                                     0.080925                      0.187848                             0.108788                                    0.101337                                           0.080714                            0.132853                                   0.114979                    0.122034                           0.192132                          0.099080                                 0.050259   \n",
       "1  24                    0.137677                 0.290331                          0.223098                         0.322311                          0.103053                                  0.165427                            0.148398                                   0.078473                    0.190788                           0.101108                                  0.122023                                         0.082515                          0.146649                                 0.113545                              0.169024                                     0.089565                      0.160096                             0.108526                                    0.121753                                           0.104547                            0.153796                                   0.126619                    0.144069                           0.196093                          0.104279                                 0.062627   \n",
       "2  28                    0.141973                 0.300382                          0.237983                         0.201867                          0.123453                                  0.177594                            0.154838                                   0.082809                    0.202542                           0.111583                                  0.128729                                         0.109886                          0.156109                                 0.118718                              0.203315                                     0.096696                      0.184749                             0.122888                                    0.136116                                           0.106995                            0.152605                                   0.136543                    0.168857                           0.203918                          0.136311                                 0.105635   \n",
       "3  32                    0.136213                 0.278652                          0.242074                         0.207769                          0.122701                                  0.170231                            0.167670                                   0.082994                    0.189973                           0.113666                                  0.139463                                         0.113798                          0.165610                                 0.140687                              0.185639                                     0.114155                      0.192676                             0.133105                                    0.129567                                           0.117368                            0.165804                                   0.141384                    0.167397                           0.210848                          0.137491                                 0.110090   \n",
       "4  36                    0.146299                 0.252867                          0.238879                         0.211200                          0.119520                                  0.159883                            0.169742                                   0.089193                    0.202648                           0.113337                                  0.138948                                         0.125951                          0.166372                                 0.144004                              0.176172                                     0.116784                      0.217816                             0.133439                                    0.152268                                           0.114918                            0.171079                                   0.147711                    0.167784                           0.210635                          0.137491                                 0.110090   \n",
       "5  40                    0.155994                 0.260456                          0.244043                         0.193645                          0.118097                                  0.148091                            0.167817                                   0.100456                    0.179508                           0.111542                                  0.139327                                         0.125992                          0.161332                                 0.140299                              0.183605                                     0.117536                      0.215158                             0.139967                                    0.154262                                           0.126498                            0.183920                                   0.161473                    0.156192                           0.217486                          0.163250                                 0.118216   \n",
       "6  44                    0.168241                 0.249411                          0.237927                         0.201699                          0.121972                                  0.139732                            0.179928                                   0.112068                    0.178305                           0.126960                                  0.141774                                         0.129450                          0.192547                                 0.139634                              0.190412                                     0.121957                      0.216961                             0.146717                                    0.153895                                           0.128867                            0.203993                                   0.170769                    0.174781                           0.214400                          0.171153                                 0.136147   \n",
       "7  48                    0.171470                 0.243697                          0.220429                         0.199625                          0.128759                                  0.136821                            0.185029                                   0.111882                    0.180820                           0.124909                                  0.147961                                         0.128826                          0.197751                                 0.140433                              0.187293                                     0.122456                      0.216072                             0.146547                                    0.154399                                           0.127289                            0.202114                                   0.167526                    0.191327                           0.228304                          0.170204                                 0.134544   \n",
       "8  52                    0.169335                 0.229308                          0.214433                         0.198940                          0.130090                                  0.140413                            0.162748                                   0.110223                    0.189776                           0.122657                                  0.148668                                         0.129163                          0.197631                                 0.137157                              0.185300                                     0.121048                      0.211985                             0.146413                                    0.159005                                           0.127598                            0.197576                                   0.166331                    0.194137                           0.223544                          0.175161                                 0.141182   \n",
       "9  56                    0.170641                 0.195913                          0.218547                         0.190295                          0.110533                                  0.134781                            0.157302                                   0.108351                    0.181724                           0.134840                                  0.148093                                         0.138068                          0.208460                                 0.133865                              0.183516                                     0.119900                      0.197729                             0.146245                                    0.155123                                           0.141034                            0.194669                                   0.170707                    0.185280                           0.224992                          0.177076                                 0.145192   \n",
       "\n",
       "   N-Grams:Full,Adjectives,1-grams  N-Grams:Full,Adjectives,1-grams,Binary  N-Grams:Full,Adjectives,1-grams,TFIDF  N-Grams:Full,Adjectives,1-grams,Binary,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams  N-Grams:Full,Words,Linares Pontes,1-grams,Binary  N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF  NOBLE Coder:Precise  NOBLE Coder:Partial  NOBLE Coder:Precise,TFIDF  NOBLE Coder:Partial,TFIDF       GO:       PO:  Word2Vec (Phenes):Wikipedia,Size=300,Mean  Word2Vec (Phenes):Wikipedia,Size=300,Max      Mean  \n",
       "0                         0.174908                                0.102361                               0.122716                                      0.084794                                   0.217753                                          0.086384                                         0.189687                                           0.116444                  0.359050             0.232276                   0.275686                   0.208827  0.091135  0.338963                                   0.342149                                  0.450186  0.232720  \n",
       "1                         0.188159                                0.119143                               0.141075                                      0.094153                                   0.283354                                          0.121842                                         0.198110                                           0.116825                  0.365728             0.230984                   0.315515                   0.219901  0.095861  0.344820                                   0.452546                                  0.487303  0.303812  \n",
       "2                         0.204516                                0.129363                               0.148331                                      0.096893                                   0.271245                                          0.120020                                         0.207860                                           0.110110                  0.367616             0.233721                   0.327786                   0.226869  0.094231  0.357175                                   0.452283                                  0.549200  0.290822  \n",
       "3                         0.219107                                0.129876                               0.165543                                      0.113373                                   0.263570                                          0.112273                                         0.227990                                           0.114494                  0.363554             0.257226                   0.332633                   0.232973  0.091711  0.462900                                   0.553821                                  0.605603  0.285493  \n",
       "4                         0.215240                                0.139491                               0.175595                                      0.147277                                   0.256474                                          0.120820                                         0.218424                                           0.133778                  0.359920             0.258032                   0.323063                   0.240119  0.093541  0.466180                                   0.573651                                  0.637747  0.250547  \n",
       "5                         0.215822                                0.143818                               0.174521                                      0.160133                                   0.254368                                          0.117010                                         0.226511                                           0.144053                  0.368838             0.253560                   0.308238                   0.239486  0.084918  0.476786                                   0.626957                                  0.667956  0.237406  \n",
       "6                         0.196040                                0.158010                               0.188891                                      0.171766                                   0.226952                                          0.118456                                         0.222196                                           0.141378                  0.360956             0.284006                   0.301541                   0.261712  0.079641  0.479253                                   0.623400                                  0.634409  0.229432  \n",
       "7                         0.206589                                0.163214                               0.189865                                      0.172058                                   0.217522                                          0.136401                                         0.221484                                           0.168891                  0.356960             0.279892                   0.298692                   0.254909  0.082347  0.509278                                   0.537634                                  0.419355  0.218883  \n",
       "8                         0.207555                                0.165653                               0.213643                                      0.183819                                   0.213005                                          0.143676                                         0.213276                                           0.161633                  0.362150             0.275668                   0.290343                   0.266430  0.084192  0.526507                                   0.290323                                  0.419355  0.216653  \n",
       "9                         0.203947                                0.163612                               0.219946                                      0.180249                                   0.194046                                          0.148395                                         0.203253                                           0.157213                  0.347104             0.276814                   0.286832                   0.267665  0.091113  0.462366                                   0.290323                                  0.365591  0.205457  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "# Note that homogeneity scores don't fit for evaluating how close the clustering is to pathway membership, etc.\n",
    "# This is because genes can be assigned to more than one pathway, metric would have to be changed to account for this.\n",
    "# So all this section does is determines which values of n_clusters provide good clustering results for each matrix.\n",
    "n_clusters_silhouette_scores = defaultdict(dict)\n",
    "min_n_clusters = 20\n",
    "max_n_clusters = 80\n",
    "step_size = 4\n",
    "number_of_clusters = np.arange(min_n_clusters, max_n_clusters, step_size)\n",
    "for n in number_of_clusters:\n",
    "    for name in names:\n",
    "        distance_matrix = name_to_array[name]\n",
    "        #to_id = array_index_to_id\n",
    "        ac = AgglomerativeClustering(n_clusters=n, linkage=\"complete\", affinity=\"precomputed\")\n",
    "        clustering = ac.fit(distance_matrix)\n",
    "        sil_score = silhouette_score(distance_matrix, clustering.labels_, metric=\"precomputed\")\n",
    "        n_clusters_silhouette_scores[name][n] = sil_score\n",
    "sil_df = pd.DataFrame(n_clusters_silhouette_scores).reset_index(drop=False).rename({\"index\":\"n\"},axis=\"columns\")\n",
    "sil_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_silhouette_scores_by_n.csv\"), index=False)\n",
    "sil_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Supervised Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "### Option 1: Merging in the previously curated similarity values from the Oellrich, Walls et al. (2015) dataset\n",
    "This section reads in a file that contains the previously calculated distance values from the Oellrich, Walls et al. (2015) dataset, and merges it with the values which are obtained here for all of the applicable natural language processing or machine learning methods used, so that the graphs which are specified by these sets of distances values can be evaluated side by side in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Doc2Vec:PubMed,Size=100</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Max</th>\n",
       "      <th>BERT: Base:Layers=4,Concatenated</th>\n",
       "      <th>BioBERT:PubMed,PMC,Layers=4,Concatenated</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>NOBLE Coder:Precise</th>\n",
       "      <th>NOBLE Coder:Partial</th>\n",
       "      <th>NOBLE Coder:Precise,TFIDF</th>\n",
       "      <th>NOBLE Coder:Partial,TFIDF</th>\n",
       "      <th>GO:</th>\n",
       "      <th>PO:</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>EQs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1147</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.318656</td>\n",
       "      <td>0.783955</td>\n",
       "      <td>0.254395</td>\n",
       "      <td>0.125013</td>\n",
       "      <td>0.180123</td>\n",
       "      <td>0.045611</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.860314</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.947206</td>\n",
       "      <td>0.885984</td>\n",
       "      <td>0.825208</td>\n",
       "      <td>0.939746</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.898071</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.958517</td>\n",
       "      <td>0.955076</td>\n",
       "      <td>0.901777</td>\n",
       "      <td>0.854852</td>\n",
       "      <td>0.760954</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831901</td>\n",
       "      <td>0.863162</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.832013</td>\n",
       "      <td>0.817598</td>\n",
       "      <td>0.575666</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.765539</td>\n",
       "      <td>0.767976</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.566112</td>\n",
       "      <td>0.454574</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1147</td>\n",
       "      <td>1625</td>\n",
       "      <td>0.396891</td>\n",
       "      <td>0.579636</td>\n",
       "      <td>0.102970</td>\n",
       "      <td>0.081193</td>\n",
       "      <td>0.089893</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>0.688221</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.603559</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.859334</td>\n",
       "      <td>0.943495</td>\n",
       "      <td>0.766539</td>\n",
       "      <td>0.871447</td>\n",
       "      <td>0.495387</td>\n",
       "      <td>0.918803</td>\n",
       "      <td>0.391184</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.726561</td>\n",
       "      <td>0.901916</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.856017</td>\n",
       "      <td>0.908713</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.918856</td>\n",
       "      <td>0.874575</td>\n",
       "      <td>0.425402</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.639406</td>\n",
       "      <td>0.860050</td>\n",
       "      <td>0.204231</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.503658</td>\n",
       "      <td>0.824208</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.801673</td>\n",
       "      <td>0.778646</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.077525</td>\n",
       "      <td>0.035615</td>\n",
       "      <td>0.144827</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>1802</td>\n",
       "      <td>0.410111</td>\n",
       "      <td>0.310422</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.104128</td>\n",
       "      <td>0.171168</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.853905</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.788815</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.897332</td>\n",
       "      <td>0.834516</td>\n",
       "      <td>0.781316</td>\n",
       "      <td>0.912277</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.939530</td>\n",
       "      <td>0.950194</td>\n",
       "      <td>0.860583</td>\n",
       "      <td>0.832986</td>\n",
       "      <td>0.894591</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.871852</td>\n",
       "      <td>0.831785</td>\n",
       "      <td>0.946162</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.919562</td>\n",
       "      <td>0.873567</td>\n",
       "      <td>0.327407</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.786568</td>\n",
       "      <td>0.853335</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.201439</td>\n",
       "      <td>0.198501</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>0.218268</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1147</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.296903</td>\n",
       "      <td>0.361018</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.088694</td>\n",
       "      <td>0.141595</td>\n",
       "      <td>0.045167</td>\n",
       "      <td>0.784613</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.702634</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.880703</td>\n",
       "      <td>0.942025</td>\n",
       "      <td>0.786124</td>\n",
       "      <td>0.874559</td>\n",
       "      <td>0.827566</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.761716</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.911975</td>\n",
       "      <td>0.948572</td>\n",
       "      <td>0.846235</td>\n",
       "      <td>0.886290</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.871879</td>\n",
       "      <td>0.865594</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.885868</td>\n",
       "      <td>0.873753</td>\n",
       "      <td>0.391990</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.650102</td>\n",
       "      <td>0.841460</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.686311</td>\n",
       "      <td>0.707151</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.265734</td>\n",
       "      <td>0.165720</td>\n",
       "      <td>0.127851</td>\n",
       "      <td>0.184888</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>780</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.454307</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.593264</td>\n",
       "      <td>0.806261</td>\n",
       "      <td>0.249255</td>\n",
       "      <td>0.083882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.702436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.580809</td>\n",
       "      <td>0.734558</td>\n",
       "      <td>0.728431</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1147</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.347951</td>\n",
       "      <td>0.537185</td>\n",
       "      <td>0.111751</td>\n",
       "      <td>0.081102</td>\n",
       "      <td>0.117849</td>\n",
       "      <td>0.038699</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.812342</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.960921</td>\n",
       "      <td>0.865344</td>\n",
       "      <td>0.877684</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.917632</td>\n",
       "      <td>0.961583</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>0.879322</td>\n",
       "      <td>0.919678</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.946222</td>\n",
       "      <td>0.919291</td>\n",
       "      <td>0.732397</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.839397</td>\n",
       "      <td>0.867478</td>\n",
       "      <td>0.327220</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.657274</td>\n",
       "      <td>0.857608</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.680644</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.117356</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.190398</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>624</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.315342</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.088554</td>\n",
       "      <td>0.115708</td>\n",
       "      <td>0.042031</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.904363</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.994146</td>\n",
       "      <td>0.943096</td>\n",
       "      <td>0.980544</td>\n",
       "      <td>0.876983</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.801708</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.954288</td>\n",
       "      <td>0.986565</td>\n",
       "      <td>0.893232</td>\n",
       "      <td>0.955711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.886071</td>\n",
       "      <td>0.968397</td>\n",
       "      <td>0.418110</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.759421</td>\n",
       "      <td>0.975639</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.821110</td>\n",
       "      <td>0.787983</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.118129</td>\n",
       "      <td>0.359138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>466</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.562618</td>\n",
       "      <td>0.152418</td>\n",
       "      <td>0.079348</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.877468</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.826474</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.952113</td>\n",
       "      <td>0.973533</td>\n",
       "      <td>0.909717</td>\n",
       "      <td>0.933976</td>\n",
       "      <td>0.807824</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.721499</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.932896</td>\n",
       "      <td>0.968816</td>\n",
       "      <td>0.875956</td>\n",
       "      <td>0.928069</td>\n",
       "      <td>0.854905</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.909442</td>\n",
       "      <td>0.938419</td>\n",
       "      <td>0.713921</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.843729</td>\n",
       "      <td>0.884276</td>\n",
       "      <td>0.379826</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.755266</td>\n",
       "      <td>0.887271</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.728723</td>\n",
       "      <td>0.771921</td>\n",
       "      <td>0.700281</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.110231</td>\n",
       "      <td>0.211503</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1147</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.294981</td>\n",
       "      <td>0.686566</td>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.285016</td>\n",
       "      <td>0.098114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1147</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.375218</td>\n",
       "      <td>0.423950</td>\n",
       "      <td>0.254019</td>\n",
       "      <td>0.390414</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691393</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.976124</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.423548</td>\n",
       "      <td>0.249664</td>\n",
       "      <td>0.669936</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to  Doc2Vec:Wikipedia,Size=300  Doc2Vec:PubMed,Size=100  Word2Vec:Wikipedia,Size=300,Mean  Word2Vec:Wikipedia,Size=300,Max  BERT: Base:Layers=4,Concatenated  BioBERT:PubMed,PMC,Layers=4,Concatenated  N-Grams:Full,Words,1-grams,2-grams  N-Grams:Full,Words,1-grams,2-grams,Binary  N-Grams:Full,Words,1-grams  N-Grams:Full,Words,1-grams,Binary  N-Grams:Full,Words,1-grams,2-grams,TFIDF  N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  N-Grams:Full,Words,1-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,2-grams  N-Grams:Simple,Words,1-grams,2-grams,Binary  N-Grams:Simple,Words,1-grams  N-Grams:Simple,Words,1-grams,Binary  N-Grams:Simple,Words,1-grams,2-grams,TFIDF  N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,TFIDF  N-Grams:Simple,Words,1-grams,Binary,TFIDF  N-Grams:Full,Nouns,1-grams  N-Grams:Full,Nouns,1-grams,Binary  N-Grams:Full,Nouns,1-grams,TFIDF  N-Grams:Full,Nouns,1-grams,Binary,TFIDF  \\\n",
       "0  1147  1344                    0.318656                 0.783955                          0.254395                         0.125013                          0.180123                                  0.045611                            0.916174                                   0.945455                    0.860314                           0.869565                                  0.951049                                         0.947206                          0.885984                                 0.825208                              0.939746                                     0.961538                      0.898071                             0.909091                                    0.958517                                           0.955076                            0.901777                                   0.854852                    0.760954                           0.900000                          0.831901                                 0.863162   \n",
       "1  1147  1625                    0.396891                 0.579636                          0.102970                         0.081193                          0.089893                                  0.024955                            0.688221                                   0.948052                    0.603559                           0.906250                                  0.859334                                         0.943495                          0.766539                                 0.871447                              0.495387                                     0.918803                      0.391184                             0.877778                                    0.726561                                           0.901916                            0.601700                                   0.856017                    0.908713                           0.925926                          0.918856                                 0.874575   \n",
       "2  1147  1802                    0.410111                 0.310422                          0.183713                         0.104128                          0.171168                                  0.043911                            0.853905                                   0.932203                    0.788815                           0.875000                                  0.900980                                         0.897332                          0.834516                                 0.781316                              0.912277                                     0.960784                      0.861057                             0.902439                                    0.939530                                           0.950194                            0.860583                                   0.832986                    0.894591                           0.916667                          0.871852                                 0.831785   \n",
       "3  1147  1246                    0.296903                 0.361018                          0.133517                         0.088694                          0.141595                                  0.045167                            0.784613                                   0.954023                    0.702634                           0.911765                                  0.880703                                         0.942025                          0.786124                                 0.874559                              0.827566                                     0.953488                      0.761716                             0.904762                                    0.911975                                           0.948572                            0.846235                                   0.886290                    0.790754                           0.903226                          0.871879                                 0.865594   \n",
       "4   780  1147                    0.454307                 0.544067                          0.593264                         0.806261                          0.249255                                  0.083882                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "5  1147  1765                    0.347951                 0.537185                          0.111751                         0.081102                          0.117849                                  0.038699                            0.862009                                   0.961538                    0.812342                           0.906977                                  0.924972                                         0.960921                          0.865344                                 0.877684                              0.792145                                     0.959770                      0.706158                             0.900000                                    0.917632                                           0.961583                            0.843510                                   0.879322                    0.919678                           0.944444                          0.946222                                 0.919291   \n",
       "6   624  1147                    0.315342                 0.380642                          0.133949                         0.088554                          0.115708                                  0.042031                            0.940015                                   0.990654                    0.904363                           0.977778                                  0.975409                                         0.994146                          0.943096                                 0.980544                              0.876983                                     0.974684                      0.801708                             0.939394                                    0.954288                                           0.986565                            0.893232                                   0.955711                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "7   466  1147                    0.372745                 0.562618                          0.152418                         0.079348                          0.144270                                  0.042165                            0.877468                                   0.963504                    0.826474                           0.934426                                  0.952113                                         0.973533                          0.909717                                 0.933976                              0.807824                                     0.955882                      0.721499                             0.918605                                    0.932896                                           0.968816                            0.875956                                   0.928069                    0.854905                           0.947368                          0.909442                                 0.938419   \n",
       "8  1147  1850                    0.512427                 0.294981                          0.686566                         0.917984                          0.285016                                  0.098114                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "9  1147  2500                    0.393056                 0.375218                          0.423950                         0.254019                          0.390414                                  0.097964                            1.000000                                   1.000000                    1.000000                           1.000000                                  1.000000                                         1.000000                          1.000000                                 1.000000                              1.000000                                     1.000000                      1.000000                             1.000000                                    1.000000                                           1.000000                            1.000000                                   1.000000                    1.000000                           1.000000                          1.000000                                 1.000000   \n",
       "\n",
       "   N-Grams:Full,Adjectives,1-grams  N-Grams:Full,Adjectives,1-grams,Binary  N-Grams:Full,Adjectives,1-grams,TFIDF  N-Grams:Full,Adjectives,1-grams,Binary,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams  N-Grams:Full,Words,Linares Pontes,1-grams,Binary  N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF  NOBLE Coder:Precise  NOBLE Coder:Partial  NOBLE Coder:Precise,TFIDF  NOBLE Coder:Partial,TFIDF       GO:       PO:  Word2Vec (Phenes):Wikipedia,Size=300,Mean  Word2Vec (Phenes):Wikipedia,Size=300,Max      Mean  EQs  \n",
       "0                         0.811392                                0.882353                               0.832013                                      0.817598                                   0.575666                                          0.814815                                         0.765539                                           0.767976                  0.690476             0.528302                   0.566112                   0.454574  0.866667  0.035714                                   0.127756                                  0.127756  0.192193  1.0  \n",
       "1                         0.425402                                0.894737                               0.639406                                      0.860050                                   0.204231                                          0.868421                                         0.503658                                           0.824208                  0.830508             0.720588                   0.801673                   0.778646  0.923077  0.014815                                   0.077525                                  0.035615  0.144827  1.0  \n",
       "2                         0.946162                                0.947368                               0.919562                                      0.873567                                   0.327407                                          0.843750                                         0.669670                                           0.801484                  0.666667             0.796875                   0.786568                   0.853335  0.923077  0.201439                                   0.198501                                  0.123670  0.218268  1.0  \n",
       "3                         0.844222                                0.916667                               0.885868                                      0.873753                                   0.391990                                          0.869565                                         0.650102                                           0.841460                  0.827586             0.764444                   0.686311                   0.707151  0.937500  0.265734                                   0.165720                                  0.127851  0.184888  1.0  \n",
       "4                         1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  0.714286             0.658824                   0.841851                   0.702436  1.000000  0.014815                                   0.580809                                  0.734558  0.728431  1.0  \n",
       "5                         0.732397                                0.896552                               0.839397                                      0.867478                                   0.327220                                          0.880435                                         0.657274                                           0.857608                  0.769231             0.721311                   0.680644                   0.703234  0.866667  0.741259                                   0.117356                                  0.104615  0.190398  1.0  \n",
       "6                         0.811392                                0.965517                               0.886071                                      0.968397                                   0.418110                                          0.962264                                         0.759421                                           0.975639                  0.873418             0.770950                   0.821110                   0.787983  0.888889  0.050000                                   0.176211                                  0.118129  0.359138  1.0  \n",
       "7                         0.713921                                0.875000                               0.843729                                      0.884276                                   0.379826                                          0.898551                                         0.755266                                           0.887271                  0.837209             0.728723                   0.771921                   0.700281  0.850000  0.000000                                   0.127756                                  0.110231  0.211503  1.0  \n",
       "8                         1.000000                                1.000000                               1.000000                                      1.000000                                   1.000000                                          1.000000                                         1.000000                                           1.000000                  1.000000             1.000000                   1.000000                   1.000000  0.933333  0.000000                                   0.706191                                  0.788820  0.780118  1.0  \n",
       "9                         1.000000                                1.000000                               1.000000                                      1.000000                                   0.691393                                          0.954545                                         0.929417                                           0.976124                  0.777778             0.866667                   0.880853                   0.881888  0.846154  0.022059                                   0.423548                                  0.249664  0.669936  1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates the distance estimated using curated EQ statements.\n",
    "df = df.merge(right=pppn_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df.fillna(value=0.000,inplace=True)\n",
    "df.rename(columns={\"value\":\"EQs\"}, inplace=True)\n",
    "df[\"EQs\"] = 1-df[\"EQs\"]\n",
    "names.append(\"EQs\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Merging with information about shared biochemical pathways or groups.\n",
    "The relevant information for each edge includes questions like whether or not the two genes that edge connects share a group or biochemical pathway in common, or if those genes are from the same species. This information can then later be used as the target values for predictive models, or for filtering the graphs represented by these edge lists. Either the grouping information or the protein-protein interaction information should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3673, 1: 621})\n",
      "Counter({1: 4294})\n"
     ]
    }
   ],
   "source": [
    "# Column indicating whether or not the two genes share this features (e.g., pathway in common, same group).\n",
    "df[\"shared\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "# Column indicating whether the two genes are from the same species.\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)*1\n",
    "print(Counter(df[\"shared\"].values))\n",
    "print(Counter(df[\"same\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Merging with information about protein-protein interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Merging information from the protein-protein interaction database with this dataset.\n",
    "df = df.merge(right=string_data.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df.fillna(value=0,inplace=True)\n",
    "df[\"shared\"] = (df[\"combined_score\"] != 0.00)*1\n",
    "df.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ks\"></a>\n",
    "### Do the edges joining genes that share a group, pathway, or interaction come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation betwene the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "ppi_pos_dict = {name:(df[df[\"shared\"] > 0.00][name].values) for name in names}\n",
    "ppi_neg_dict = {name:(df[df[\"shared\"] == 0.00][name].values) for name in names}\n",
    "for name in names:\n",
    "    stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "    pos_mean = np.average(ppi_pos_dict[name])\n",
    "    neg_mean = np.average(ppi_neg_dict[name])\n",
    "    pos_n = len(ppi_pos_dict[name])\n",
    "    neg_n = len(ppi_neg_dict[name])\n",
    "    TABLE[name].update({\"mean_1\":pos_mean, \"mean_0\":neg_mean, \"n_1\":pos_n, \"n_0\":neg_n})\n",
    "    TABLE[name].update({\"ks\":stat, \"ks_pval\":p})\n",
    "    \n",
    "    \n",
    "# Show the kernel estimates for each distribution of weights for each method.\n",
    "#num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "#fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "#for name,ax in zip(names,axs.flatten()):\n",
    "#    ax.set_title(name)\n",
    "#    ax.set_xlabel(\"value\")\n",
    "#    ax.set_ylabel(\"density\")\n",
    "#    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "#    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "#fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "#fig.tight_layout()\n",
    "#fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_kernel_density.png\"),dpi=400)\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"within\"></a>\n",
    "### Looking at within-group or within-pathway distances in each graph\n",
    "The purpose of this section is to determine which methods generated graphs which tightly group genes which share common pathways or group membership with one another. In order to compare across different methods where the distance value distributions are different, the mean distance values for each group for each method are convereted to percentile scores. Lower percentile scores indicate that the average distance value between any two genes that belong to that group is lower than most of the distance values in the entire distribution for that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>n</th>\n",
       "      <th>mean_percentile</th>\n",
       "      <th>mean_rank</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Doc2Vec:PubMed,Size=100</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Max</th>\n",
       "      <th>BERT: Base:Layers=4,Concatenated</th>\n",
       "      <th>BioBERT:PubMed,PMC,Layers=4,Concatenated</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Simple,Words,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Nouns,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Adjectives,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>NOBLE Coder:Precise</th>\n",
       "      <th>NOBLE Coder:Partial</th>\n",
       "      <th>NOBLE Coder:Precise,TFIDF</th>\n",
       "      <th>NOBLE Coder:Partial,TFIDF</th>\n",
       "      <th>GO:</th>\n",
       "      <th>PO:</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Mean</th>\n",
       "      <th>Word2Vec (Phenes):Wikipedia,Size=300,Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>EQs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STT</td>\n",
       "      <td>Stomata, trichomes: Distribution, morphology</td>\n",
       "      <td>2</td>\n",
       "      <td>7.226315</td>\n",
       "      <td>4.795455</td>\n",
       "      <td>17.7690</td>\n",
       "      <td>15.7429</td>\n",
       "      <td>8.1975</td>\n",
       "      <td>1.8398</td>\n",
       "      <td>6.7070</td>\n",
       "      <td>0.6986</td>\n",
       "      <td>1.4206</td>\n",
       "      <td>3.3768</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>6.0550</td>\n",
       "      <td>0.5356</td>\n",
       "      <td>2.0727</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>4.5412</td>\n",
       "      <td>1.9329</td>\n",
       "      <td>2.4220</td>\n",
       "      <td>1.9795</td>\n",
       "      <td>3.9124</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>1.7233</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>2.4453</td>\n",
       "      <td>15.8361</td>\n",
       "      <td>17.6758</td>\n",
       "      <td>15.2771</td>\n",
       "      <td>16.7210</td>\n",
       "      <td>9.9441</td>\n",
       "      <td>5.5426</td>\n",
       "      <td>7.1262</td>\n",
       "      <td>4.4946</td>\n",
       "      <td>12.9250</td>\n",
       "      <td>4.7508</td>\n",
       "      <td>1.6302</td>\n",
       "      <td>3.7261</td>\n",
       "      <td>7.6968</td>\n",
       "      <td>7.0098</td>\n",
       "      <td>4.5645</td>\n",
       "      <td>5.9851</td>\n",
       "      <td>3.3069</td>\n",
       "      <td>78.1556</td>\n",
       "      <td>2.9110</td>\n",
       "      <td>2.8645</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>4.8905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMB</td>\n",
       "      <td>True embryo defective; No known gametophyte de...</td>\n",
       "      <td>16</td>\n",
       "      <td>10.974410</td>\n",
       "      <td>5.795455</td>\n",
       "      <td>4.2385</td>\n",
       "      <td>19.3759</td>\n",
       "      <td>29.4131</td>\n",
       "      <td>74.4294</td>\n",
       "      <td>38.6120</td>\n",
       "      <td>27.2939</td>\n",
       "      <td>1.9795</td>\n",
       "      <td>1.3507</td>\n",
       "      <td>2.3987</td>\n",
       "      <td>1.9795</td>\n",
       "      <td>1.5370</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>2.2357</td>\n",
       "      <td>1.9096</td>\n",
       "      <td>3.2837</td>\n",
       "      <td>1.3041</td>\n",
       "      <td>4.8905</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>1.9096</td>\n",
       "      <td>1.3274</td>\n",
       "      <td>2.7946</td>\n",
       "      <td>1.6069</td>\n",
       "      <td>23.7541</td>\n",
       "      <td>3.8891</td>\n",
       "      <td>20.8197</td>\n",
       "      <td>20.6800</td>\n",
       "      <td>4.0522</td>\n",
       "      <td>2.1658</td>\n",
       "      <td>3.3535</td>\n",
       "      <td>2.5151</td>\n",
       "      <td>18.9334</td>\n",
       "      <td>1.8398</td>\n",
       "      <td>6.5906</td>\n",
       "      <td>2.4453</td>\n",
       "      <td>19.3992</td>\n",
       "      <td>3.8659</td>\n",
       "      <td>14.4388</td>\n",
       "      <td>3.6097</td>\n",
       "      <td>26.1761</td>\n",
       "      <td>63.2976</td>\n",
       "      <td>2.9110</td>\n",
       "      <td>2.8645</td>\n",
       "      <td>10.7359</td>\n",
       "      <td>2.9110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MGD</td>\n",
       "      <td>Miscellaneous gametophyte defective</td>\n",
       "      <td>4</td>\n",
       "      <td>11.047285</td>\n",
       "      <td>5.954545</td>\n",
       "      <td>14.7182</td>\n",
       "      <td>37.0983</td>\n",
       "      <td>28.8775</td>\n",
       "      <td>30.2515</td>\n",
       "      <td>15.7196</td>\n",
       "      <td>14.6483</td>\n",
       "      <td>5.1234</td>\n",
       "      <td>2.8179</td>\n",
       "      <td>8.3605</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>2.7480</td>\n",
       "      <td>2.0028</td>\n",
       "      <td>4.9837</td>\n",
       "      <td>3.4932</td>\n",
       "      <td>9.7578</td>\n",
       "      <td>2.3754</td>\n",
       "      <td>15.2771</td>\n",
       "      <td>4.5412</td>\n",
       "      <td>3.2371</td>\n",
       "      <td>1.9329</td>\n",
       "      <td>5.8454</td>\n",
       "      <td>2.6782</td>\n",
       "      <td>3.3768</td>\n",
       "      <td>2.2357</td>\n",
       "      <td>2.7480</td>\n",
       "      <td>1.5836</td>\n",
       "      <td>5.6823</td>\n",
       "      <td>4.1453</td>\n",
       "      <td>3.4932</td>\n",
       "      <td>2.7014</td>\n",
       "      <td>35.9339</td>\n",
       "      <td>6.8002</td>\n",
       "      <td>10.8756</td>\n",
       "      <td>2.9343</td>\n",
       "      <td>4.5179</td>\n",
       "      <td>21.7047</td>\n",
       "      <td>5.2399</td>\n",
       "      <td>16.7210</td>\n",
       "      <td>33.1160</td>\n",
       "      <td>72.3801</td>\n",
       "      <td>7.0796</td>\n",
       "      <td>7.0796</td>\n",
       "      <td>8.2673</td>\n",
       "      <td>11.6674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLS</td>\n",
       "      <td>Non-lethal seedling: Cotyledon, hypocotyl</td>\n",
       "      <td>4</td>\n",
       "      <td>11.103995</td>\n",
       "      <td>7.784091</td>\n",
       "      <td>4.6810</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>8.6865</td>\n",
       "      <td>3.9590</td>\n",
       "      <td>12.2729</td>\n",
       "      <td>16.9306</td>\n",
       "      <td>8.6400</td>\n",
       "      <td>2.8179</td>\n",
       "      <td>9.9208</td>\n",
       "      <td>3.4700</td>\n",
       "      <td>4.5179</td>\n",
       "      <td>2.7946</td>\n",
       "      <td>6.6837</td>\n",
       "      <td>3.6563</td>\n",
       "      <td>15.7196</td>\n",
       "      <td>2.6083</td>\n",
       "      <td>19.6088</td>\n",
       "      <td>3.5864</td>\n",
       "      <td>6.4043</td>\n",
       "      <td>2.7480</td>\n",
       "      <td>10.5496</td>\n",
       "      <td>3.8891</td>\n",
       "      <td>10.3400</td>\n",
       "      <td>4.8440</td>\n",
       "      <td>7.0796</td>\n",
       "      <td>5.4960</td>\n",
       "      <td>13.4374</td>\n",
       "      <td>5.2632</td>\n",
       "      <td>9.4085</td>\n",
       "      <td>6.4741</td>\n",
       "      <td>41.6861</td>\n",
       "      <td>3.7727</td>\n",
       "      <td>20.9828</td>\n",
       "      <td>4.9604</td>\n",
       "      <td>7.8482</td>\n",
       "      <td>6.1015</td>\n",
       "      <td>7.1728</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>53.1439</td>\n",
       "      <td>74.3596</td>\n",
       "      <td>11.2483</td>\n",
       "      <td>15.6265</td>\n",
       "      <td>10.9222</td>\n",
       "      <td>10.3167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SRL</td>\n",
       "      <td>Seedling, rosette lethal; Severe seedling defe...</td>\n",
       "      <td>8</td>\n",
       "      <td>11.273562</td>\n",
       "      <td>6.215909</td>\n",
       "      <td>2.5617</td>\n",
       "      <td>51.9562</td>\n",
       "      <td>29.3200</td>\n",
       "      <td>27.0377</td>\n",
       "      <td>20.1444</td>\n",
       "      <td>17.1169</td>\n",
       "      <td>3.6796</td>\n",
       "      <td>2.3754</td>\n",
       "      <td>4.2850</td>\n",
       "      <td>2.4918</td>\n",
       "      <td>2.5151</td>\n",
       "      <td>2.3055</td>\n",
       "      <td>2.8645</td>\n",
       "      <td>2.3288</td>\n",
       "      <td>11.9469</td>\n",
       "      <td>2.6083</td>\n",
       "      <td>14.2292</td>\n",
       "      <td>3.6330</td>\n",
       "      <td>4.7974</td>\n",
       "      <td>2.6316</td>\n",
       "      <td>5.9385</td>\n",
       "      <td>3.2837</td>\n",
       "      <td>3.3069</td>\n",
       "      <td>2.2590</td>\n",
       "      <td>2.5384</td>\n",
       "      <td>1.3041</td>\n",
       "      <td>6.3111</td>\n",
       "      <td>3.7261</td>\n",
       "      <td>5.1467</td>\n",
       "      <td>3.6563</td>\n",
       "      <td>26.6884</td>\n",
       "      <td>2.6782</td>\n",
       "      <td>10.8989</td>\n",
       "      <td>3.1439</td>\n",
       "      <td>47.4383</td>\n",
       "      <td>7.9180</td>\n",
       "      <td>38.5654</td>\n",
       "      <td>7.0564</td>\n",
       "      <td>20.2841</td>\n",
       "      <td>29.5529</td>\n",
       "      <td>18.6539</td>\n",
       "      <td>25.7802</td>\n",
       "      <td>8.1509</td>\n",
       "      <td>5.9851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_id                                          full_name   n  mean_percentile  mean_rank  Doc2Vec:Wikipedia,Size=300  Doc2Vec:PubMed,Size=100  Word2Vec:Wikipedia,Size=300,Mean  Word2Vec:Wikipedia,Size=300,Max  BERT: Base:Layers=4,Concatenated  BioBERT:PubMed,PMC,Layers=4,Concatenated  N-Grams:Full,Words,1-grams,2-grams  N-Grams:Full,Words,1-grams,2-grams,Binary  N-Grams:Full,Words,1-grams  N-Grams:Full,Words,1-grams,Binary  N-Grams:Full,Words,1-grams,2-grams,TFIDF  N-Grams:Full,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  N-Grams:Full,Words,1-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,2-grams  N-Grams:Simple,Words,1-grams,2-grams,Binary  N-Grams:Simple,Words,1-grams  N-Grams:Simple,Words,1-grams,Binary  N-Grams:Simple,Words,1-grams,2-grams,TFIDF  N-Grams:Simple,Words,1-grams,2-grams,Binary,TFIDF  N-Grams:Simple,Words,1-grams,TFIDF  N-Grams:Simple,Words,1-grams,Binary,TFIDF  N-Grams:Full,Nouns,1-grams  N-Grams:Full,Nouns,1-grams,Binary  \\\n",
       "0      STT       Stomata, trichomes: Distribution, morphology   2         7.226315   4.795455                     17.7690                  15.7429                            8.1975                           1.8398                            6.7070                                    0.6986                              1.4206                                     3.3768                      1.5603                             6.0550                                    0.5356                                           2.0727                            0.5589                                   4.5412                                1.9329                                       2.4220                        1.9795                               3.9124                                      0.8617                                             1.7233                              0.6754                                     2.4453                     15.8361                            17.6758   \n",
       "1      EMB  True embryo defective; No known gametophyte de...  16        10.974410   5.795455                      4.2385                  19.3759                           29.4131                          74.4294                           38.6120                                   27.2939                              1.9795                                     1.3507                      2.3987                             1.9795                                    1.5370                                           1.4672                            2.2357                                   1.9096                                3.2837                                       1.3041                        4.8905                               1.4672                                      1.9096                                             1.3274                              2.7946                                     1.6069                     23.7541                             3.8891   \n",
       "2      MGD                Miscellaneous gametophyte defective   4        11.047285   5.954545                     14.7182                  37.0983                           28.8775                          30.2515                           15.7196                                   14.6483                              5.1234                                     2.8179                      8.3605                             4.4015                                    2.7480                                           2.0028                            4.9837                                   3.4932                                9.7578                                       2.3754                       15.2771                               4.5412                                      3.2371                                             1.9329                              5.8454                                     2.6782                      3.3768                             2.2357   \n",
       "3      NLS          Non-lethal seedling: Cotyledon, hypocotyl   4        11.103995   7.784091                      4.6810                   0.9082                            8.6865                           3.9590                           12.2729                                   16.9306                              8.6400                                     2.8179                      9.9208                             3.4700                                    4.5179                                           2.7946                            6.6837                                   3.6563                               15.7196                                       2.6083                       19.6088                               3.5864                                      6.4043                                             2.7480                             10.5496                                     3.8891                     10.3400                             4.8440   \n",
       "4      SRL  Seedling, rosette lethal; Severe seedling defe...   8        11.273562   6.215909                      2.5617                  51.9562                           29.3200                          27.0377                           20.1444                                   17.1169                              3.6796                                     2.3754                      4.2850                             2.4918                                    2.5151                                           2.3055                            2.8645                                   2.3288                               11.9469                                       2.6083                       14.2292                               3.6330                                      4.7974                                             2.6316                              5.9385                                     3.2837                      3.3069                             2.2590   \n",
       "\n",
       "   N-Grams:Full,Nouns,1-grams,TFIDF  N-Grams:Full,Nouns,1-grams,Binary,TFIDF  N-Grams:Full,Adjectives,1-grams  N-Grams:Full,Adjectives,1-grams,Binary  N-Grams:Full,Adjectives,1-grams,TFIDF  N-Grams:Full,Adjectives,1-grams,Binary,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams  N-Grams:Full,Words,Linares Pontes,1-grams,Binary  N-Grams:Full,Words,Linares Pontes,1-grams,TFIDF  N-Grams:Full,Words,Linares Pontes,1-grams,Binary,TFIDF  NOBLE Coder:Precise  NOBLE Coder:Partial  NOBLE Coder:Precise,TFIDF  NOBLE Coder:Partial,TFIDF      GO:      PO:  Word2Vec (Phenes):Wikipedia,Size=300,Mean  Word2Vec (Phenes):Wikipedia,Size=300,Max     Mean      EQs  \n",
       "0                           15.2771                                  16.7210                           9.9441                                  5.5426                                 7.1262                                        4.4946                                    12.9250                                            4.7508                                           1.6302                                             3.7261                    7.6968               7.0098                     4.5645                     5.9851   3.3069  78.1556                                     2.9110                                    2.8645   0.3260   4.8905  \n",
       "1                           20.8197                                  20.6800                           4.0522                                  2.1658                                 3.3535                                        2.5151                                    18.9334                                            1.8398                                           6.5906                                             2.4453                   19.3992               3.8659                    14.4388                     3.6097  26.1761  63.2976                                     2.9110                                    2.8645  10.7359   2.9110  \n",
       "2                            2.7480                                   1.5836                           5.6823                                  4.1453                                 3.4932                                        2.7014                                    35.9339                                            6.8002                                          10.8756                                             2.9343                    4.5179              21.7047                     5.2399                    16.7210  33.1160  72.3801                                     7.0796                                    7.0796   8.2673  11.6674  \n",
       "3                            7.0796                                   5.4960                          13.4374                                  5.2632                                 9.4085                                        6.4741                                    41.6861                                            3.7727                                          20.9828                                             4.9604                    7.8482               6.1015                     7.1728                     6.3577  53.1439  74.3596                                    11.2483                                   15.6265  10.9222  10.3167  \n",
       "4                            2.5384                                   1.3041                           6.3111                                  3.7261                                 5.1467                                        3.6563                                    26.6884                                            2.6782                                          10.8989                                             3.1439                   47.4383               7.9180                    38.5654                     7.0564  20.2841  29.5529                                    18.6539                                   25.7802   8.1509   5.9851  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the average within-pathway phenotype distance values for each method for each particular pathway.\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "within_percentiles_dict = defaultdict(lambda: defaultdict(list))\n",
    "all_weights_dict = {}\n",
    "for name in names:\n",
    "    all_weights_dict[name] = df[name].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        mean_weight = np.mean((graph.get_values(within_pairs, kind=name)))\n",
    "        within_weights_dict[name][group] = mean_weight\n",
    "        within_percentiles_dict[name][group] = stats.percentileofscore(df[name].values, mean_weight, kind=\"rank\")\n",
    "\n",
    "# Generating a dataframe of percentiles of the mean in-group distance scores.\n",
    "within_dist_data = pd.DataFrame(within_percentiles_dict)\n",
    "within_dist_data = within_dist_data.dropna(axis=0, inplace=False)\n",
    "within_dist_data = within_dist_data.round(4)\n",
    "\n",
    "# Adding relevant information to this dataframe and saving.\n",
    "within_dist_data[\"mean_rank\"] = within_dist_data.rank().mean(axis=1)\n",
    "within_dist_data[\"mean_percentile\"] = within_dist_data.mean(axis=1)\n",
    "within_dist_data.sort_values(by=\"mean_percentile\", inplace=True)\n",
    "within_dist_data.reset_index(inplace=True)\n",
    "within_dist_data[\"group_id\"] = within_dist_data[\"index\"]\n",
    "within_dist_data[\"full_name\"] = within_dist_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "within_dist_data[\"n\"] = within_dist_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "within_dist_data = within_dist_data[flatten([\"group_id\",\"full_name\",\"n\",\"mean_percentile\",\"mean_rank\",names])]\n",
    "within_dist_data.to_csv(os.path.join(OUTPUT_DIR,\"part_6_within_distances.csv\"), index=False)\n",
    "within_dist_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auc\"></a>\n",
    "### Predicting whether two genes belong to the same group, pathway, or share an interaction\n",
    "The purpose of this section is to see if whether or not two genes share atleast one common pathway can be predicted from the distance scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true_dict = {name:df[\"shared\"] for name in names}\n",
    "y_prob_dict = {name:(1 - df[name].values) for name in names}\n",
    "num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for name,ax in zip(names, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true, y_prob = y_true_dict[name], y_prob_dict[name]\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[name].update({\"auc\":area, \"baseline\":baseline, \"ratio\":auc_to_baseline_auc_ratio})\n",
    "    \n",
    "    # Find the maximum Fß score for different values of ß.  \n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f1_max = np.nanmax(f_beta(precision,recall,beta=1))\n",
    "    f5_max = np.nanmax(f_beta(precision,recall,beta=0.5))\n",
    "    f2_max = np.nanmax(f_beta(precision,recall,beta=2))\n",
    "    TABLE[name].update({\"f1_max\":f1_max, \"f5_max\":f5_max, \"f2_max\":f2_max})\n",
    "    \n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_prcurve_shared.png\"),dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"y\"></a>\n",
    "### Are genes in the same group or pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "# graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "# i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "# in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "# connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "df = make_undirected(df)\n",
    "\n",
    "# What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "# each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "# recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "k = 10      # The threshold of interest for gene ranks.\n",
    "n = 100     # Number of Monte Carlo simulation iterations to complete.\n",
    "df[list(names)] = df.groupby(\"from\")[list(names)].rank()\n",
    "ys = df[df[\"shared\"]==1][list(names)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "ymax = sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "# Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "# edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "ysims = [sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "for name in names:\n",
    "    pvalue = len([ysim for ysim in ysims if ysim>=ys[name]])/float(n)\n",
    "    TABLE[name].update({\"y\":ys[name], \"y_max\":ymax, \"y_ratio\":ys[name]/ymax, \"y_pval\":pvalue})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean\"></a>\n",
    "### Predicting biochemical pathway or group membership based on mean vectors\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes asociated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "# vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "# names = list(vector_dicts.keys())\n",
    "# group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "# valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "# valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "# pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# for name in names:\n",
    "#     for group in valid_group_ids:\n",
    "#         ids = group_id_to_ids[group]\n",
    "#         for identifier in valid_ids:\n",
    "#             # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "#             vectors = np.array([vector_dicts[name][some_id] for some_id in ids if not some_id==identifier])\n",
    "#             mean_vector = vectors.mean(axis=0)\n",
    "#             this_vector = vector_dicts[name][identifier]\n",
    "#             pred_dict[name][identifier][group] = 1-metric_dict[name](mean_vector, this_vector)\n",
    "#             true_dict[name][identifier][group] = (identifier in group_id_to_ids[group])*1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "# fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "# for name,ax in zip(names, axs.flatten()):\n",
    "#     \n",
    "#     # Obtaining the values and metrics.\n",
    "#     y_true = pd.DataFrame(true_dict[name]).as_matrix().flatten()\n",
    "#     y_prob = pd.DataFrame(pred_dict[name]).as_matrix().flatten()\n",
    "#     n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "#     baseline = Counter(y_true)[1]/len(y_true) \n",
    "#     area = auc(recall, precision)\n",
    "#     auc_to_baseline_auc_ratio = area/baseline\n",
    "#     TABLE[name].update({\"mean_auc\":area, \"mean_baseline\":baseline, \"mean_ratio\":auc_to_baseline_auc_ratio})\n",
    "# \n",
    "#     # Producing the precision recall curve.\n",
    "#     step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "#     ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "#     ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "#     ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "#     ax.set_xlabel('Recall')\n",
    "#     ax.set_ylabel('Precision')\n",
    "#     ax.set_ylim([0.0, 1.05])\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name[:10], baseline))\n",
    "#     \n",
    "# fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_prcurve_mean_classifier.png\"),dpi=400)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway membership based on mean similarity values\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representationt of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership with KNN classifier\n",
    "This section looks at how well the group(s) or biochemical pathway(s) that a particular gene belongs to can be predicted based on a KNN classifier generated using every other gene. For this section, only the groups or pathways which contain more than one gene, and the genes mapped to those groups or pathways, are of interest. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but the KNN classifier could never predict this because when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be members of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"output\"></a>\n",
    "### Summarizing the results for this notebook\n",
    "Write a large table of results to an output file. Columns are generally metrics and rows are generally methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Group</th>\n",
       "      <th>Order</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Data</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_0</th>\n",
       "      <th>ks</th>\n",
       "      <th>ks_pval</th>\n",
       "      <th>auc</th>\n",
       "      <th>baseline</th>\n",
       "      <th>ratio</th>\n",
       "      <th>f1_max</th>\n",
       "      <th>f5_max</th>\n",
       "      <th>f2_max</th>\n",
       "      <th>y</th>\n",
       "      <th>y_max</th>\n",
       "      <th>y_ratio</th>\n",
       "      <th>y_pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.380030</td>\n",
       "      <td>0.445694</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.272873</td>\n",
       "      <td>8.837957e-35</td>\n",
       "      <td>0.349697</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.418033</td>\n",
       "      <td>0.357749</td>\n",
       "      <td>0.391395</td>\n",
       "      <td>0.468805</td>\n",
       "      <td>345.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>PubMed,Size=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.512237</td>\n",
       "      <td>0.574730</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>8.530905e-15</td>\n",
       "      <td>0.222001</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.535059</td>\n",
       "      <td>0.293568</td>\n",
       "      <td>0.264874</td>\n",
       "      <td>0.459123</td>\n",
       "      <td>281.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.374667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Wikipedia,Size=300,Mean</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.265781</td>\n",
       "      <td>0.389467</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.299054</td>\n",
       "      <td>1.091091e-41</td>\n",
       "      <td>0.305045</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.109277</td>\n",
       "      <td>0.349084</td>\n",
       "      <td>0.313606</td>\n",
       "      <td>0.511163</td>\n",
       "      <td>339.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Wikipedia,Size=300,Max</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.256259</td>\n",
       "      <td>0.343812</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.256763</td>\n",
       "      <td>7.638628e-31</td>\n",
       "      <td>0.318463</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.202062</td>\n",
       "      <td>0.337703</td>\n",
       "      <td>0.340681</td>\n",
       "      <td>0.466257</td>\n",
       "      <td>331.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.441333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>Base</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.146446</td>\n",
       "      <td>0.190122</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.282021</td>\n",
       "      <td>4.021492e-37</td>\n",
       "      <td>0.244477</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.690474</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.290990</td>\n",
       "      <td>0.495183</td>\n",
       "      <td>330.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BioBERT</td>\n",
       "      <td>PubMed,PMC,Layers=4,Concatenated</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.045344</td>\n",
       "      <td>0.061124</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.305850</td>\n",
       "      <td>1.383699e-43</td>\n",
       "      <td>0.265881</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.838477</td>\n",
       "      <td>0.356899</td>\n",
       "      <td>0.312770</td>\n",
       "      <td>0.508438</td>\n",
       "      <td>339.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,2-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.811737</td>\n",
       "      <td>0.951619</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.476416</td>\n",
       "      <td>3.794331e-105</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.274352</td>\n",
       "      <td>0.463592</td>\n",
       "      <td>0.505021</td>\n",
       "      <td>0.599907</td>\n",
       "      <td>423.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,2-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.922847</td>\n",
       "      <td>0.985018</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.454370</td>\n",
       "      <td>1.114130e-95</td>\n",
       "      <td>0.426381</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.948279</td>\n",
       "      <td>0.446589</td>\n",
       "      <td>0.419929</td>\n",
       "      <td>0.586255</td>\n",
       "      <td>399.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>8</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.752836</td>\n",
       "      <td>0.926640</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.467495</td>\n",
       "      <td>2.913736e-101</td>\n",
       "      <td>0.447631</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.095215</td>\n",
       "      <td>0.453251</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.597184</td>\n",
       "      <td>420.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>9</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.882465</td>\n",
       "      <td>0.969034</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.441013</td>\n",
       "      <td>3.673221e-90</td>\n",
       "      <td>0.414329</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.864940</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.412579</td>\n",
       "      <td>0.583950</td>\n",
       "      <td>391.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.521333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,2-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>10</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.886823</td>\n",
       "      <td>0.978371</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.503041</td>\n",
       "      <td>3.525752e-117</td>\n",
       "      <td>0.514553</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.557955</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>0.533903</td>\n",
       "      <td>0.614055</td>\n",
       "      <td>451.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.601333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,2-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>11</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.914571</td>\n",
       "      <td>0.985504</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.530110</td>\n",
       "      <td>4.402048e-130</td>\n",
       "      <td>0.507965</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.512399</td>\n",
       "      <td>0.495425</td>\n",
       "      <td>0.507026</td>\n",
       "      <td>0.624364</td>\n",
       "      <td>446.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.594667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>12</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.828462</td>\n",
       "      <td>0.958181</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.493313</td>\n",
       "      <td>1.045364e-112</td>\n",
       "      <td>0.482651</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.337362</td>\n",
       "      <td>0.483990</td>\n",
       "      <td>0.488141</td>\n",
       "      <td>0.612153</td>\n",
       "      <td>444.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>13</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.857152</td>\n",
       "      <td>0.964009</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.521883</td>\n",
       "      <td>4.335114e-126</td>\n",
       "      <td>0.472886</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.269841</td>\n",
       "      <td>0.487324</td>\n",
       "      <td>0.483611</td>\n",
       "      <td>0.624095</td>\n",
       "      <td>428.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,2-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>14</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.811911</td>\n",
       "      <td>0.932316</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.412801</td>\n",
       "      <td>4.772043e-79</td>\n",
       "      <td>0.378666</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.618346</td>\n",
       "      <td>0.404798</td>\n",
       "      <td>0.392482</td>\n",
       "      <td>0.568106</td>\n",
       "      <td>381.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,2-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>15</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.928008</td>\n",
       "      <td>0.982944</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.386879</td>\n",
       "      <td>1.748674e-69</td>\n",
       "      <td>0.381156</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.635565</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.555331</td>\n",
       "      <td>355.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>16</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.752348</td>\n",
       "      <td>0.898027</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.389649</td>\n",
       "      <td>1.780166e-70</td>\n",
       "      <td>0.356217</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.463117</td>\n",
       "      <td>0.385727</td>\n",
       "      <td>0.374370</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>376.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.501333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>17</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.891585</td>\n",
       "      <td>0.964929</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.380638</td>\n",
       "      <td>2.838229e-67</td>\n",
       "      <td>0.364722</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.521924</td>\n",
       "      <td>0.398175</td>\n",
       "      <td>0.382963</td>\n",
       "      <td>0.551123</td>\n",
       "      <td>350.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,2-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>18</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.893341</td>\n",
       "      <td>0.973791</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.490975</td>\n",
       "      <td>1.204693e-111</td>\n",
       "      <td>0.433491</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.997438</td>\n",
       "      <td>0.451073</td>\n",
       "      <td>0.433596</td>\n",
       "      <td>0.601726</td>\n",
       "      <td>413.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.550667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,2-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>19</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.918762</td>\n",
       "      <td>0.984439</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.503771</td>\n",
       "      <td>1.614204e-117</td>\n",
       "      <td>0.473747</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.275794</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.483391</td>\n",
       "      <td>0.608782</td>\n",
       "      <td>428.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>20</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.840142</td>\n",
       "      <td>0.950219</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.460489</td>\n",
       "      <td>2.911328e-98</td>\n",
       "      <td>0.406808</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.812933</td>\n",
       "      <td>0.438085</td>\n",
       "      <td>0.399594</td>\n",
       "      <td>0.588031</td>\n",
       "      <td>396.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Simple,Words,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>21</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.867173</td>\n",
       "      <td>0.963103</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.484514</td>\n",
       "      <td>9.745512e-109</td>\n",
       "      <td>0.445013</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.077112</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>0.461627</td>\n",
       "      <td>0.605433</td>\n",
       "      <td>409.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Nouns,1-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>22</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.889942</td>\n",
       "      <td>0.964300</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.287472</td>\n",
       "      <td>1.486042e-38</td>\n",
       "      <td>0.300266</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.076235</td>\n",
       "      <td>0.364069</td>\n",
       "      <td>0.333972</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>246.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Nouns,1-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>23</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.919515</td>\n",
       "      <td>0.973106</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.299368</td>\n",
       "      <td>8.931988e-42</td>\n",
       "      <td>0.289974</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.005073</td>\n",
       "      <td>0.365237</td>\n",
       "      <td>0.330174</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>285.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Nouns,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>24</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.916952</td>\n",
       "      <td>0.976414</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.290009</td>\n",
       "      <td>3.134394e-39</td>\n",
       "      <td>0.309979</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.143400</td>\n",
       "      <td>0.368758</td>\n",
       "      <td>0.352255</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>261.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Nouns,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>25</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.922877</td>\n",
       "      <td>0.978349</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.283405</td>\n",
       "      <td>1.751090e-37</td>\n",
       "      <td>0.318101</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.199559</td>\n",
       "      <td>0.369910</td>\n",
       "      <td>0.353349</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>262.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Adjectives,1-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>26</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.816039</td>\n",
       "      <td>0.945054</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.432609</td>\n",
       "      <td>8.960595e-87</td>\n",
       "      <td>0.362877</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.509172</td>\n",
       "      <td>0.422505</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.566899</td>\n",
       "      <td>313.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.417333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Adjectives,1-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>27</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.907685</td>\n",
       "      <td>0.976370</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.420879</td>\n",
       "      <td>3.728879e-82</td>\n",
       "      <td>0.373423</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.582092</td>\n",
       "      <td>0.418274</td>\n",
       "      <td>0.392196</td>\n",
       "      <td>0.562396</td>\n",
       "      <td>349.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.465333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Adjectives,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>28</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.869882</td>\n",
       "      <td>0.966305</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.435361</td>\n",
       "      <td>7.080944e-88</td>\n",
       "      <td>0.382598</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.645531</td>\n",
       "      <td>0.429104</td>\n",
       "      <td>0.413574</td>\n",
       "      <td>0.565269</td>\n",
       "      <td>343.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.457333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Adjectives,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>29</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.882681</td>\n",
       "      <td>0.970946</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.451192</td>\n",
       "      <td>2.369196e-94</td>\n",
       "      <td>0.405948</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.806989</td>\n",
       "      <td>0.435013</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>0.575451</td>\n",
       "      <td>376.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.501333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,Linares Pontes,1-grams</td>\n",
       "      <td>NLP</td>\n",
       "      <td>30</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.619705</td>\n",
       "      <td>0.758801</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.255032</td>\n",
       "      <td>1.958583e-30</td>\n",
       "      <td>0.250134</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.729588</td>\n",
       "      <td>0.314756</td>\n",
       "      <td>0.283804</td>\n",
       "      <td>0.524358</td>\n",
       "      <td>335.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,Linares Pontes,1-grams,Binary</td>\n",
       "      <td>NLP</td>\n",
       "      <td>31</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.859414</td>\n",
       "      <td>0.945534</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.370943</td>\n",
       "      <td>6.531689e-64</td>\n",
       "      <td>0.379256</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.622424</td>\n",
       "      <td>0.400862</td>\n",
       "      <td>0.406091</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>362.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.482667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,Linares Pontes,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>32</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.755785</td>\n",
       "      <td>0.890758</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.367706</td>\n",
       "      <td>8.282196e-63</td>\n",
       "      <td>0.340325</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.353228</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.365268</td>\n",
       "      <td>0.548730</td>\n",
       "      <td>389.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.518667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>33</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.819586</td>\n",
       "      <td>0.939092</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.484504</td>\n",
       "      <td>9.847205e-109</td>\n",
       "      <td>0.444572</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.074062</td>\n",
       "      <td>0.463496</td>\n",
       "      <td>0.458095</td>\n",
       "      <td>0.600097</td>\n",
       "      <td>416.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NOBLE Coder</td>\n",
       "      <td>Precise</td>\n",
       "      <td>NLP</td>\n",
       "      <td>34</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.671470</td>\n",
       "      <td>0.765169</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.210104</td>\n",
       "      <td>8.583385e-21</td>\n",
       "      <td>0.259220</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.792419</td>\n",
       "      <td>0.311730</td>\n",
       "      <td>0.340398</td>\n",
       "      <td>0.460272</td>\n",
       "      <td>286.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.381333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NOBLE Coder</td>\n",
       "      <td>Partial</td>\n",
       "      <td>NLP</td>\n",
       "      <td>35</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.606160</td>\n",
       "      <td>0.729879</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.263450</td>\n",
       "      <td>1.897005e-32</td>\n",
       "      <td>0.316856</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.190948</td>\n",
       "      <td>0.351967</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.481627</td>\n",
       "      <td>322.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.429333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NOBLE Coder</td>\n",
       "      <td>Precise,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>36</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.665174</td>\n",
       "      <td>0.789114</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.303217</td>\n",
       "      <td>7.603115e-43</td>\n",
       "      <td>0.297838</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.059445</td>\n",
       "      <td>0.355485</td>\n",
       "      <td>0.338824</td>\n",
       "      <td>0.487421</td>\n",
       "      <td>299.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.398667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NOBLE Coder</td>\n",
       "      <td>Partial,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>37</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.627984</td>\n",
       "      <td>0.766697</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.343156</td>\n",
       "      <td>9.334927e-55</td>\n",
       "      <td>0.356049</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.461953</td>\n",
       "      <td>0.378919</td>\n",
       "      <td>0.401767</td>\n",
       "      <td>0.517433</td>\n",
       "      <td>361.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.481333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GO</td>\n",
       "      <td></td>\n",
       "      <td>NLP</td>\n",
       "      <td>38</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.887257</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.097419</td>\n",
       "      <td>7.656154e-05</td>\n",
       "      <td>0.154913</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.071168</td>\n",
       "      <td>0.252696</td>\n",
       "      <td>0.174468</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>163.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.217333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>PO</td>\n",
       "      <td></td>\n",
       "      <td>NLP</td>\n",
       "      <td>39</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.241543</td>\n",
       "      <td>0.275235</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.093090</td>\n",
       "      <td>1.850049e-04</td>\n",
       "      <td>0.185127</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>1.280090</td>\n",
       "      <td>0.257854</td>\n",
       "      <td>0.202958</td>\n",
       "      <td>0.460104</td>\n",
       "      <td>149.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.198667</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Word2Vec (Phenes)</td>\n",
       "      <td>Wikipedia,Size=300,Mean</td>\n",
       "      <td>NLP</td>\n",
       "      <td>40</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.206370</td>\n",
       "      <td>0.399867</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.416722</td>\n",
       "      <td>1.506985e-80</td>\n",
       "      <td>0.484609</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.350901</td>\n",
       "      <td>0.475620</td>\n",
       "      <td>0.558069</td>\n",
       "      <td>0.552696</td>\n",
       "      <td>500.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Word2Vec (Phenes)</td>\n",
       "      <td>Wikipedia,Size=300,Max</td>\n",
       "      <td>NLP</td>\n",
       "      <td>41</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.178102</td>\n",
       "      <td>0.360070</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.377738</td>\n",
       "      <td>2.934327e-66</td>\n",
       "      <td>0.473885</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.276748</td>\n",
       "      <td>0.459608</td>\n",
       "      <td>0.543071</td>\n",
       "      <td>0.526969</td>\n",
       "      <td>493.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.657333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Mean</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>42</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.306116</td>\n",
       "      <td>0.531696</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.462760</td>\n",
       "      <td>3.138363e-99</td>\n",
       "      <td>0.429494</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>2.969799</td>\n",
       "      <td>0.459590</td>\n",
       "      <td>0.441104</td>\n",
       "      <td>0.589084</td>\n",
       "      <td>432.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>EQs</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>43</td>\n",
       "      <td>Biochemical Pathways</td>\n",
       "      <td>Filtered</td>\n",
       "      <td>0.756664</td>\n",
       "      <td>0.973527</td>\n",
       "      <td>621.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>0.474669</td>\n",
       "      <td>2.217125e-104</td>\n",
       "      <td>0.507649</td>\n",
       "      <td>0.14462</td>\n",
       "      <td>3.510214</td>\n",
       "      <td>0.497743</td>\n",
       "      <td>0.492611</td>\n",
       "      <td>0.568248</td>\n",
       "      <td>519.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Method                                 Hyperparameters Group  Order                 Topic      Data    mean_1    mean_0    n_1     n_0        ks        ks_pval       auc  baseline     ratio    f1_max    f5_max    f2_max      y  y_max   y_ratio  y_pval\n",
       "0             Doc2Vec                              Wikipedia,Size=300   NLP      0  Biochemical Pathways  Filtered  0.380030  0.445694  621.0  3673.0  0.272873   8.837957e-35  0.349697   0.14462  2.418033  0.357749  0.391395  0.468805  345.0  750.0  0.460000    0.00\n",
       "1             Doc2Vec                                 PubMed,Size=100   NLP      1  Biochemical Pathways  Filtered  0.512237  0.574730  621.0  3673.0  0.176480   8.530905e-15  0.222001   0.14462  1.535059  0.293568  0.264874  0.459123  281.0  750.0  0.374667    0.00\n",
       "2            Word2Vec                         Wikipedia,Size=300,Mean   NLP      2  Biochemical Pathways  Filtered  0.265781  0.389467  621.0  3673.0  0.299054   1.091091e-41  0.305045   0.14462  2.109277  0.349084  0.313606  0.511163  339.0  750.0  0.452000    0.00\n",
       "3            Word2Vec                          Wikipedia,Size=300,Max   NLP      3  Biochemical Pathways  Filtered  0.256259  0.343812  621.0  3673.0  0.256763   7.638628e-31  0.318463   0.14462  2.202062  0.337703  0.340681  0.466257  331.0  750.0  0.441333    0.00\n",
       "4                BERT                                            Base   NLP      4  Biochemical Pathways  Filtered  0.146446  0.190122  621.0  3673.0  0.282021   4.021492e-37  0.244477   0.14462  1.690474  0.341709  0.290990  0.495183  330.0  750.0  0.440000    0.00\n",
       "5             BioBERT                PubMed,PMC,Layers=4,Concatenated   NLP      5  Biochemical Pathways  Filtered  0.045344  0.061124  621.0  3673.0  0.305850   1.383699e-43  0.265881   0.14462  1.838477  0.356899  0.312770  0.508438  339.0  750.0  0.452000    0.00\n",
       "6             N-Grams                      Full,Words,1-grams,2-grams   NLP      6  Biochemical Pathways  Filtered  0.811737  0.951619  621.0  3673.0  0.476416  3.794331e-105  0.473538   0.14462  3.274352  0.463592  0.505021  0.599907  423.0  750.0  0.564000    0.00\n",
       "7             N-Grams               Full,Words,1-grams,2-grams,Binary   NLP      7  Biochemical Pathways  Filtered  0.922847  0.985018  621.0  3673.0  0.454370   1.114130e-95  0.426381   0.14462  2.948279  0.446589  0.419929  0.586255  399.0  750.0  0.532000    0.00\n",
       "8             N-Grams                              Full,Words,1-grams   NLP      8  Biochemical Pathways  Filtered  0.752836  0.926640  621.0  3673.0  0.467495  2.913736e-101  0.447631   0.14462  3.095215  0.453251  0.465116  0.597184  420.0  750.0  0.560000    0.00\n",
       "9             N-Grams                       Full,Words,1-grams,Binary   NLP      9  Biochemical Pathways  Filtered  0.882465  0.969034  621.0  3673.0  0.441013   3.673221e-90  0.414329   0.14462  2.864940  0.436364  0.412579  0.583950  391.0  750.0  0.521333    0.00\n",
       "10            N-Grams                Full,Words,1-grams,2-grams,TFIDF   NLP     10  Biochemical Pathways  Filtered  0.886823  0.978371  621.0  3673.0  0.503041  3.525752e-117  0.514553   0.14462  3.557955  0.497354  0.533903  0.614055  451.0  750.0  0.601333    0.00\n",
       "11            N-Grams         Full,Words,1-grams,2-grams,Binary,TFIDF   NLP     11  Biochemical Pathways  Filtered  0.914571  0.985504  621.0  3673.0  0.530110  4.402048e-130  0.507965   0.14462  3.512399  0.495425  0.507026  0.624364  446.0  750.0  0.594667    0.00\n",
       "12            N-Grams                        Full,Words,1-grams,TFIDF   NLP     12  Biochemical Pathways  Filtered  0.828462  0.958181  621.0  3673.0  0.493313  1.045364e-112  0.482651   0.14462  3.337362  0.483990  0.488141  0.612153  444.0  750.0  0.592000    0.00\n",
       "13            N-Grams                 Full,Words,1-grams,Binary,TFIDF   NLP     13  Biochemical Pathways  Filtered  0.857152  0.964009  621.0  3673.0  0.521883  4.335114e-126  0.472886   0.14462  3.269841  0.487324  0.483611  0.624095  428.0  750.0  0.570667    0.00\n",
       "14            N-Grams                    Simple,Words,1-grams,2-grams   NLP     14  Biochemical Pathways  Filtered  0.811911  0.932316  621.0  3673.0  0.412801   4.772043e-79  0.378666   0.14462  2.618346  0.404798  0.392482  0.568106  381.0  750.0  0.508000    0.00\n",
       "15            N-Grams             Simple,Words,1-grams,2-grams,Binary   NLP     15  Biochemical Pathways  Filtered  0.928008  0.982944  621.0  3673.0  0.386879   1.748674e-69  0.381156   0.14462  2.635565  0.407821  0.373134  0.555331  355.0  750.0  0.473333    0.00\n",
       "16            N-Grams                            Simple,Words,1-grams   NLP     16  Biochemical Pathways  Filtered  0.752348  0.898027  621.0  3673.0  0.389649   1.780166e-70  0.356217   0.14462  2.463117  0.385727  0.374370  0.558333  376.0  750.0  0.501333    0.00\n",
       "17            N-Grams                     Simple,Words,1-grams,Binary   NLP     17  Biochemical Pathways  Filtered  0.891585  0.964929  621.0  3673.0  0.380638   2.838229e-67  0.364722   0.14462  2.521924  0.398175  0.382963  0.551123  350.0  750.0  0.466667    0.00\n",
       "18            N-Grams              Simple,Words,1-grams,2-grams,TFIDF   NLP     18  Biochemical Pathways  Filtered  0.893341  0.973791  621.0  3673.0  0.490975  1.204693e-111  0.433491   0.14462  2.997438  0.451073  0.433596  0.601726  413.0  750.0  0.550667    0.00\n",
       "19            N-Grams       Simple,Words,1-grams,2-grams,Binary,TFIDF   NLP     19  Biochemical Pathways  Filtered  0.918762  0.984439  621.0  3673.0  0.503771  1.614204e-117  0.473747   0.14462  3.275794  0.487834  0.483391  0.608782  428.0  750.0  0.570667    0.00\n",
       "20            N-Grams                      Simple,Words,1-grams,TFIDF   NLP     20  Biochemical Pathways  Filtered  0.840142  0.950219  621.0  3673.0  0.460489   2.911328e-98  0.406808   0.14462  2.812933  0.438085  0.399594  0.588031  396.0  750.0  0.528000    0.00\n",
       "21            N-Grams               Simple,Words,1-grams,Binary,TFIDF   NLP     21  Biochemical Pathways  Filtered  0.867173  0.963103  621.0  3673.0  0.484514  9.745512e-109  0.445013   0.14462  3.077112  0.463609  0.461627  0.605433  409.0  750.0  0.545333    0.00\n",
       "22            N-Grams                              Full,Nouns,1-grams   NLP     22  Biochemical Pathways  Filtered  0.889942  0.964300  621.0  3673.0  0.287472   1.486042e-38  0.300266   0.14462  2.076235  0.364069  0.333972  0.458100  246.0  750.0  0.328000    0.00\n",
       "23            N-Grams                       Full,Nouns,1-grams,Binary   NLP     23  Biochemical Pathways  Filtered  0.919515  0.973106  621.0  3673.0  0.299368   8.931988e-42  0.289974   0.14462  2.005073  0.365237  0.330174  0.458100  285.0  750.0  0.380000    0.00\n",
       "24            N-Grams                        Full,Nouns,1-grams,TFIDF   NLP     24  Biochemical Pathways  Filtered  0.916952  0.976414  621.0  3673.0  0.290009   3.134394e-39  0.309979   0.14462  2.143400  0.368758  0.352255  0.458100  261.0  750.0  0.348000    0.00\n",
       "25            N-Grams                 Full,Nouns,1-grams,Binary,TFIDF   NLP     25  Biochemical Pathways  Filtered  0.922877  0.978349  621.0  3673.0  0.283405   1.751090e-37  0.318101   0.14462  2.199559  0.369910  0.353349  0.458100  262.0  750.0  0.349333    0.00\n",
       "26            N-Grams                         Full,Adjectives,1-grams   NLP     26  Biochemical Pathways  Filtered  0.816039  0.945054  621.0  3673.0  0.432609   8.960595e-87  0.362877   0.14462  2.509172  0.422505  0.385773  0.566899  313.0  750.0  0.417333    0.00\n",
       "27            N-Grams                  Full,Adjectives,1-grams,Binary   NLP     27  Biochemical Pathways  Filtered  0.907685  0.976370  621.0  3673.0  0.420879   3.728879e-82  0.373423   0.14462  2.582092  0.418274  0.392196  0.562396  349.0  750.0  0.465333    0.00\n",
       "28            N-Grams                   Full,Adjectives,1-grams,TFIDF   NLP     28  Biochemical Pathways  Filtered  0.869882  0.966305  621.0  3673.0  0.435361   7.080944e-88  0.382598   0.14462  2.645531  0.429104  0.413574  0.565269  343.0  750.0  0.457333    0.00\n",
       "29            N-Grams            Full,Adjectives,1-grams,Binary,TFIDF   NLP     29  Biochemical Pathways  Filtered  0.882681  0.970946  621.0  3673.0  0.451192   2.369196e-94  0.405948   0.14462  2.806989  0.435013  0.426559  0.575451  376.0  750.0  0.501333    0.00\n",
       "30            N-Grams               Full,Words,Linares Pontes,1-grams   NLP     30  Biochemical Pathways  Filtered  0.619705  0.758801  621.0  3673.0  0.255032   1.958583e-30  0.250134   0.14462  1.729588  0.314756  0.283804  0.524358  335.0  750.0  0.446667    0.00\n",
       "31            N-Grams        Full,Words,Linares Pontes,1-grams,Binary   NLP     31  Biochemical Pathways  Filtered  0.859414  0.945534  621.0  3673.0  0.370943   6.531689e-64  0.379256   0.14462  2.622424  0.400862  0.406091  0.529501  362.0  750.0  0.482667    0.00\n",
       "32            N-Grams         Full,Words,Linares Pontes,1-grams,TFIDF   NLP     32  Biochemical Pathways  Filtered  0.755785  0.890758  621.0  3673.0  0.367706   8.282196e-63  0.340325   0.14462  2.353228  0.397436  0.365268  0.548730  389.0  750.0  0.518667    0.00\n",
       "33            N-Grams  Full,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP     33  Biochemical Pathways  Filtered  0.819586  0.939092  621.0  3673.0  0.484504  9.847205e-109  0.444572   0.14462  3.074062  0.463496  0.458095  0.600097  416.0  750.0  0.554667    0.00\n",
       "34        NOBLE Coder                                         Precise   NLP     34  Biochemical Pathways  Filtered  0.671470  0.765169  621.0  3673.0  0.210104   8.583385e-21  0.259220   0.14462  1.792419  0.311730  0.340398  0.460272  286.0  750.0  0.381333    0.00\n",
       "35        NOBLE Coder                                         Partial   NLP     35  Biochemical Pathways  Filtered  0.606160  0.729879  621.0  3673.0  0.263450   1.897005e-32  0.316856   0.14462  2.190948  0.351967  0.364706  0.481627  322.0  750.0  0.429333    0.00\n",
       "36        NOBLE Coder                                   Precise,TFIDF   NLP     36  Biochemical Pathways  Filtered  0.665174  0.789114  621.0  3673.0  0.303217   7.603115e-43  0.297838   0.14462  2.059445  0.355485  0.338824  0.487421  299.0  750.0  0.398667    0.00\n",
       "37        NOBLE Coder                                   Partial,TFIDF   NLP     37  Biochemical Pathways  Filtered  0.627984  0.766697  621.0  3673.0  0.343156   9.334927e-55  0.356049   0.14462  2.461953  0.378919  0.401767  0.517433  361.0  750.0  0.481333    0.00\n",
       "38                 GO                                                   NLP     38  Biochemical Pathways  Filtered  0.887257  0.890258  621.0  3673.0  0.097419   7.656154e-05  0.154913   0.14462  1.071168  0.252696  0.174468  0.458100  163.0  750.0  0.217333    0.00\n",
       "39                 PO                                                   NLP     39  Biochemical Pathways  Filtered  0.241543  0.275235  621.0  3673.0  0.093090   1.850049e-04  0.185127   0.14462  1.280090  0.257854  0.202958  0.460104  149.0  750.0  0.198667    0.03\n",
       "40  Word2Vec (Phenes)                         Wikipedia,Size=300,Mean   NLP     40  Biochemical Pathways  Filtered  0.206370  0.399867  621.0  3673.0  0.416722   1.506985e-80  0.484609   0.14462  3.350901  0.475620  0.558069  0.552696  500.0  750.0  0.666667    0.00\n",
       "41  Word2Vec (Phenes)                          Wikipedia,Size=300,Max   NLP     41  Biochemical Pathways  Filtered  0.178102  0.360070  621.0  3673.0  0.377738   2.934327e-66  0.473885   0.14462  3.276748  0.459608  0.543071  0.526969  493.0  750.0  0.657333    0.00\n",
       "42               Mean                                            None   NLP     42  Biochemical Pathways  Filtered  0.306116  0.531696  621.0  3673.0  0.462760   3.138363e-99  0.429494   0.14462  2.969799  0.459590  0.441104  0.589084  432.0  750.0  0.576000    0.00\n",
       "43                EQs                                            None   NLP     43  Biochemical Pathways  Filtered  0.756664  0.973527  621.0  3673.0  0.474669  2.217125e-104  0.507649   0.14462  3.510214  0.497743  0.492611  0.568248  519.0  750.0  0.692000    0.00"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "columns = flatten([\"Hyperparameters\",\"Group\",\"Order\",\"Topic\",\"Data\",results.columns])\n",
    "results[\"Hyperparameters\"] = \"\"\n",
    "results[\"Group\"] = \"NLP\"\n",
    "results[\"Order\"] = np.arange(results.shape[0])\n",
    "results[\"Topic\"] = TOPIC\n",
    "results[\"Data\"] = DATA\n",
    "results = results[columns]\n",
    "results.reset_index(inplace=True)\n",
    "results = results.rename({\"index\":\"Method\"}, axis=\"columns\")\n",
    "hyperparam_sep = \":\"\n",
    "results[\"Hyperparameters\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[1] if hyperparam_sep in x else \"None\")\n",
    "results[\"Method\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[0])\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"part_6_full_table.csv\"), index=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
