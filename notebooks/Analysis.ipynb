{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "\n",
    "- [Links of Interest](#links)\n",
    "\n",
    "- [Loading Data](#paths)\n",
    "    - [Setting input and output paths](#paths)\n",
    "    - [Reading in a dataset of text descriptions](#read_this_data_)\n",
    "    - [Reading in a dataset of groups or categories](#read_other_data)\n",
    "    - [Relating the datasets to one another](#relating)\n",
    "    - [Filtering the datasets](#filtering)\n",
    "    \n",
    "- [Language Models](#load_models)\n",
    "    - [Loading models](#load_models)\n",
    "    - [BERT and BioBERT]()\n",
    "    - [Word2Vec and Doc2Vec]()\n",
    "    \n",
    "- [NLP Choices](#part8)\n",
    "    - [Creating vocabularies](#vocab)\n",
    "    - [Preprocessing descriptions](#preprocessing)\n",
    "    - [POS Tagging](#pos_tagging)\n",
    "    - [Annotating descriptions](#annotation)\n",
    "    \n",
    "- [Building a Distance Matrix](#matrix)\n",
    "    - [Defining a list of methods to use](#methods)\n",
    "    - [Running each method](#run)\n",
    "    - [Adding additional information](#merging)\n",
    "    - [Combining methods with ensemble models](#ensemble)\n",
    "- [Analysis]()\n",
    "    - [Comparing distributions of distance values between methods](#ks)\n",
    "    - [Comparing the within-group distance values across gene groups and methods](#within)\n",
    "    - [Comparing the AUC for predicting shared pathways, gene groups, or interactions between methods](#auc)\n",
    "    - [Comparing querying for similar genes using distance matrices for each method](#y)\n",
    "    - [Comparing the AUC for predicting the specific pathway or group of a gene](#mean)\n",
    "    - [Generating a table of resulting metrics for each method](#output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction: Text Mining Analysis of Phenotype Descriptions in Plants\n",
    "The purpose of this notebook is to evaluate what can be learned from a natural language processing approach to analyzing free-text descriptions of phenotype descriptions of plants. The approach is to generate pairwise distances matrices between a set of plant phenotype descriptions across different species, sourced from academic papers and online model organism databases. These pairwise distance matrices can be constructed using any vectorization method that can be applied to natural language. In this notebook, we specifically evaluate the use of n-gram and bag-of-words techniques, word and document embedding using Word2Vec and Doc2Vec, context-dependent word-embeddings using BERT and BioBERT, and ontology term annotations with automated annotation tools such as NOBLE Coder.\n",
    "\n",
    "Loading, manipulation, and filtering of the dataset of phenotype descriptions associated with genes across different plant species is largely handled through a Python package created for this purpose called OATS (Ontology Annotation and Text Similarity) which is available [here](https://github.com/irbraun/oats). Preprocessing of the descriptions, mapping the dataset to additional resources such as protein-protein interaction databases and biochemical pathway databases are handled in this notebook using that package as well. In the evaluation of each of these natural language processing approaches to analyzing this dataset of descriptions, we compare performance against a dataset generated through manual annotation of a similar dataset in Oellrich Walls et al. (2015) and against manual annotations with experimentally determined terms from the Gene Ontology (PO) and the Plant Ontology (PO).\n",
    "\n",
    "<a id=\"links\"></a>\n",
    "### Relevant Links of Interest:\n",
    "- Paper describing comparison of NLP and ontology annotation approaches to curation: [Braun, Lawrence-Dill (2019)](https://doi.org/10.3389/fpls.2019.01629)\n",
    "- Paper describing results of manual phenotype description curation: [Oellrich, Walls et al. (2015](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0053-y)\n",
    "- Plant databases with phenotype description text data available: [TAIR](https://www.arabidopsis.org/), [SGN](https://solgenomics.net/), [MaizeGDB](https://www.maizegdb.org/)\n",
    "- Python package for working with phenotype descriptions: [OATS](https://github.com/irbraun/oats)\n",
    "- Python package used for general NLP functions: [NLTK](https://www.nltk.org/)\n",
    "- Python package used for working with biological ontologies: [Pronto](https://pronto.readthedocs.io/en/latest/)\n",
    "- Python package for loading pretrained BERT models: [PyTorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/)\n",
    "- For BERT Models pretrained on PubMed and PMC: [BioBERT Paper](https://arxiv.org/abs/1901.08746), [BioBERT Models](https://github.com/naver/biobert-pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.datasets.edges import Edges\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.graphs import pairwise as pw\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.graphs.weighting import train_logistic_regression_model, apply_logistic_regression_model\n",
    "from oats.graphs.weighting import train_random_forest_model, apply_random_forest_model\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, build_vocabulary_from_tokens\n",
    "from oats.nlp.vocabulary import collapse_vocabulary_by_distance\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "from oats.nlp.preprocess import concatenate_with_bar_delim\n",
    "\n",
    "from _utils import Method\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths\"></a>\n",
    "### Setting up the output table and input and output filepaths\n",
    "This section defines some constants which are used for creating a uniquely named directory to contain all the outputs from running this instance of this notebook. The naming scheme is based on the time that the notebook is run. The other constants are used for specifying information in the output table about what the topic was for this notebook when it was run, such as looking at KEGG biochemical pathways or STRING protein-protein interaction data some other type of gene function grouping or hierarchy. These values are arbitrary and are just for keeping better notes about what the output of the notebook corresponds to. All the input and output file paths for loading datasets or models are also contained within this cell, so that if anything is moved the directories and file names should only have to be changed at this point and nowhere else further into the notebook. If additional files are added to the notebook cells they should be put here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summarizing output dictionary has the shape TABLE[method][metric] --> value.\n",
    "TOPIC = \"Biochemical Pathways\"\n",
    "DATA = \"Filtered\"\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = \"../data/pickles/text_plus_annotations_dataset.pickle\"        # The full dataset pickle.\n",
    "groupings_filename = \"../data/pickles/lloyd_subsets.pickle\"                      # The groupings pickle.\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"       # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_small.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/pubmed_dbow/doc2vec_2.bin\"                  # File holding saved Doc2Vec model.\n",
    "doc2vec_wikipedia_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                 # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                       # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                       # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                            # Jar for NOBLE Coder tool.\n",
    "biobert_pmc_path = \"../gensim/biobert_v1.0_pmc/pytorch_model\"                    # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_path = \"../gensim/biobert_v1.0_pubmed/pytorch_model\"              # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_pmc_path = \"../gensim/biobert_v1.0_pubmed_pmc/pytorch_model\"      # Path for PyTorch BioBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_this_data\"></a>\n",
    "### Reading in the dataset of genes and their associated phenotype descriptions and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 30169\n",
      "Number of unique IDs:            30169\n",
      "Number of unique descriptions:   4566\n",
      "Number of unique gene name sets: 30169\n",
      "Number of species represented:   6\n",
      "Number of rows in the dataframe: 6030\n",
      "Number of unique IDs:            6030\n",
      "Number of unique descriptions:   3611\n",
      "Number of unique gene name sets: 6030\n",
      "Number of species represented:   1\n",
      "Number of rows in the dataframe: 6030\n",
      "Number of unique IDs:            6030\n",
      "Number of unique descriptions:   3611\n",
      "Number of unique gene name sets: 6030\n",
      "Number of species represented:   1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>description</th>\n",
       "      <th>term_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ath</td>\n",
       "      <td>At3g49600|UBP26|AT3G49600|SUP32|ATUBP26|ubiqui...</td>\n",
       "      <td>50% defective seeds. Low penetrance of endospe...</td>\n",
       "      <td>GO:0005730|GO:0048316|PO:0000013|PO:0000037|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G25620|YUC6|AtYUC6|YUCCA6|T14C9.160|T14C9_160</td>\n",
       "      <td>epinastic cotyledons, long hypocotyls, curled ...</td>\n",
       "      <td>GO:0009851|PO:0009031|PO:0000293|PO:0007611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G25220|KNAT3|KNOTTED1-like homeobox gene 3|...</td>\n",
       "      <td>No visible root phenotype. Normal embryo sac d...</td>\n",
       "      <td>GO:0005829|GO:0071345|GO:0009416|GO:0005515|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24860|FPF1|ATFPF1|FLOWERING PROMOTING FACT...</td>\n",
       "      <td>late flowering.</td>\n",
       "      <td>GO:0009739|GO:0010228|GO:0003674|GO:0009911|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24740|SHBY|SHRUBBY|T4C12.10</td>\n",
       "      <td>no visible developmental phenotypes were obser...</td>\n",
       "      <td>GO:0010082|GO:0010015|GO:1990064|PO:0000282|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24380|YSL2|ATYSL2|YELLOW STRIPE like 2|K16...</td>\n",
       "      <td>Morphology and development of mutant plants gr...</td>\n",
       "      <td>GO:0005886|GO:0010039|GO:0010043|PO:0005059|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24360|IRE1-1|ATIRE1-1|AtIRE1b|IRE1B|ARABID...</td>\n",
       "      <td>bZIP60 mRNA splicing is detectable in ire1b-1 ...</td>\n",
       "      <td>GO:0034263|GO:0005783|GO:0009816|GO:0000394|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24330|ATXR6|SDG34|ARABIDOPSIS TRITHORAX-RE...</td>\n",
       "      <td>reduced H3K27me1 in vivo; partial heterochroma...</td>\n",
       "      <td>GO:0006275|GO:0070734|GO:0005634|GO:0009901|GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24300|SS1|ATSS1|STARCH SYNTHASE 1|MOP9.12|...</td>\n",
       "      <td>Germination percentages, rates of growth,flowe...</td>\n",
       "      <td>GO:0009011|GO:0009570|GO:0009507|GO:0010021|PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ath</td>\n",
       "      <td>AT5G24290|MEB2|MEMBRANE OF ER BODY 2|MOP9.11|M...</td>\n",
       "      <td>The plants do not accumulate MEB2 protein. The...</td>\n",
       "      <td>GO:0005515|GO:0010168|GO:0005384|GO:0005381|PO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id species                                         gene_names                                        description                                           term_ids\n",
       "0   0     ath  At3g49600|UBP26|AT3G49600|SUP32|ATUBP26|ubiqui...  50% defective seeds. Low penetrance of endospe...  GO:0005730|GO:0048316|PO:0000013|PO:0000037|PO...\n",
       "1   1     ath   AT5G25620|YUC6|AtYUC6|YUCCA6|T14C9.160|T14C9_160  epinastic cotyledons, long hypocotyls, curled ...        GO:0009851|PO:0009031|PO:0000293|PO:0007611\n",
       "2   2     ath  AT5G25220|KNAT3|KNOTTED1-like homeobox gene 3|...  No visible root phenotype. Normal embryo sac d...  GO:0005829|GO:0071345|GO:0009416|GO:0005515|GO...\n",
       "3   3     ath  AT5G24860|FPF1|ATFPF1|FLOWERING PROMOTING FACT...                                    late flowering.  GO:0009739|GO:0010228|GO:0003674|GO:0009911|PO...\n",
       "4   4     ath                    AT5G24740|SHBY|SHRUBBY|T4C12.10  no visible developmental phenotypes were obser...  GO:0010082|GO:0010015|GO:1990064|PO:0000282|PO...\n",
       "5   5     ath  AT5G24380|YSL2|ATYSL2|YELLOW STRIPE like 2|K16...  Morphology and development of mutant plants gr...  GO:0005886|GO:0010039|GO:0010043|PO:0005059|PO...\n",
       "6   6     ath  AT5G24360|IRE1-1|ATIRE1-1|AtIRE1b|IRE1B|ARABID...  bZIP60 mRNA splicing is detectable in ire1b-1 ...  GO:0034263|GO:0005783|GO:0009816|GO:0000394|GO...\n",
       "7   7     ath  AT5G24330|ATXR6|SDG34|ARABIDOPSIS TRITHORAX-RE...  reduced H3K27me1 in vivo; partial heterochroma...  GO:0006275|GO:0070734|GO:0005634|GO:0009901|GO...\n",
       "8   8     ath  AT5G24300|SS1|ATSS1|STARCH SYNTHASE 1|MOP9.12|...  Germination percentages, rates of growth,flowe...  GO:0009011|GO:0009570|GO:0009507|GO:0010021|PO...\n",
       "9   9     ath  AT5G24290|MEB2|MEMBRANE OF ER BODY 2|MOP9.11|M...  The plants do not accumulate MEB2 protein. The...  GO:0005515|GO:0010168|GO:0005384|GO:0005381|PO..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_pickle(dataset_filename)\n",
    "dataset.describe()\n",
    "dataset.filter_by_species(\"ath\")\n",
    "dataset.filter_has_description()\n",
    "#dataset.filter_has_annotation()\n",
    "dataset.describe()\n",
    "#dataset.filter_has_annotation(\"GO\")\n",
    "#dataset.filter_has_annotation(\"PO\")\n",
    "dataset.describe()\n",
    "dataset.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_other_data\"></a>\n",
    "### Reading in the dataset of groupings, pathways, or some other kind of categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups present for each species\n",
      "  ath: 42\n",
      "Number of genes names mapped to any group for each species\n",
      "  ath: 7620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_id</th>\n",
       "      <th>gene_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>FSM</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB|FSM|OVP|SRF</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>CDR|LIT</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|WAT</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP|SRF</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>CHS</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>NLS|GRS|IST</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>LEF|FSM</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species                         group_id                                         gene_names\n",
       "0     ath                              FSM                         at1g01030|nga3|top1|ngatha\n",
       "1     ath                  EMB|FSM|OVP|SRF    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2     ath                          CDR|LIT             at1g01060|lhy|late elongated hypocotyl\n",
       "3     ath                          IST|WAT   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4     ath                          OVP|SRF                 at1g01280|cyp703a2|cytochrome p450\n",
       "5     ath                              EMB        at1g01370|cenh3|centromere-specific histone\n",
       "6     ath                              CHS  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7     ath                      NLS|GRS|IST  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8     ath                          LEF|FSM                          at1g01510|an|angustifolia\n",
       "9     ath  SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP                              at1g01550|bps1|bypass"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = load_from_pickle(groupings_filename)\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "groups.describe()\n",
    "groups.to_csv(os.path.join(OUTPUT_DIR,\"groupings.csv\"))\n",
    "groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relating\"></a>\n",
    "### Relating the dataset of genes to the dataset of groupings or categories\n",
    "This section generates tables that indicate how the genes present in the dataset were mapped to the defined pathways or groups. This includes a summary table that indicates how many genes by species were succcessfully mapped to atleast one pathway or group, as well as a more detailed table describing how many genes from each species were mapped to each particular pathway or group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "summary = defaultdict(dict)\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "for species in dataset.get_species():\n",
    "    summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "    summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table.loc[\"total\"]= table.sum()\n",
    "table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"species\"}, axis=\"columns\")\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_summary.csv\"), index=False)\n",
    "\n",
    "# Generate a table describing how many genes from each species map to which particular group.\n",
    "summary = defaultdict(dict)\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "    summary[group_id][\"total\"] = len([x for x in ids])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table = table.sort_values(by=\"total\", ascending=False)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "table.loc[\"total\"] = table.sum()\n",
    "table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_by_group.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>\n",
    "### Option 1: Filtering the dataset based on presence in the curated Oellrich, Walls et al. (2015) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on whether or not the genes were in the curated dataset.\n",
    "# This is similar to filtering based on protein interaction data because the dataset is a list of edge values.\n",
    "pppn_edgelist_path = \"../data/supplemental_files_oellrich_walls/13007_2015_53_MOESM9_ESM.txt\"\n",
    "pppn_edgelist = Edges(dataset.get_name_to_id_dictionary(), pppn_edgelist_path)\n",
    "dataset.filter_with_ids(pppn_edgelist.ids)\n",
    "dataset.filter_random_k(100)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Filtering the dataset based on protein-protein interactions\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mentioned atleast one time in the STRING database for a given species. If a gene is not mentioned at all in STRING, there is no information available for whether or not it interacts with any other proteins in the dataset so choose to not include it in the analysis. Only genes that have atleast one true positive are included because these are the only ones for which the missing information (negatives) is meaningful. This should be run instead of the subsequent cell, or the other way around, based on whether or not protein-protein interactions is the prediction goal for the current analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset based on whether or not the genes were successfully mapped to an interaction.\n",
    "# Reduce size of the dataset by removing genes not mentioned in the STRING.\n",
    "naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis thaliana\n",
    "    \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\", # maize\n",
    "    \"../data/group_related_files/string/4530.protein.links.detailed.v11.0.txt\", # tomato \n",
    "    \"../data/group_related_files/string/4081.protein.links.detailed.v11.0.txt\", # medicago\n",
    "    \"../data/group_related_files/string/3880.protein.links.detailed.v11.0.txt\", # rice \n",
    "    \"../data/group_related_files/string/3847.protein.links.detailed.v11.0.txt\", # soybean\n",
    "]\n",
    "genes = dataset.get_gene_dictionary()\n",
    "string_data = String(genes, naming_file, *interaction_files)\n",
    "dataset.filter_with_ids(string_data.ids)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Filtering the dataset based on membership in pathways or phenotype category\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mapped to atleast one pathway in whatever the source of pathway membership we are using is (KEGG, Plant Metabolic Network, etc). This is because for these genes, it will be impossible to correctly predict their pathway membership, and we have no evidence that they belong or do not belong in certain pathways so they can not be identified as being true or false negatives in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 2557\n",
      "Number of unique IDs:            2557\n",
      "Number of unique descriptions:   2167\n",
      "Number of unique gene name sets: 2557\n",
      "Number of species represented:   1\n"
     ]
    }
   ],
   "source": [
    "# Filter based on succcessful mappings to groups or pathways.\n",
    "dataset.filter_with_ids(group_mapped_ids)\n",
    "dataset.describe()\n",
    "# Get the mappings in each direction again now that the dataset has been subset.\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making an LDA figure from that to see if the topics match up, for Lloyd subsets.\n",
    "\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "tokens = list(set([w for w in flatten(word_tokenize(d) for d in descriptions_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "\n",
    "# Get the objects needed to collapse the vocabulary and make a new vocabulary.\n",
    "# Make sure that the tokens list is in the same order as the indices representing each word in the distance matrix.\n",
    "# This is only trivial here because the IDs used are ordered integers 0 to n, but this might not always be the case.\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.row_index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "threshold = 0.1\n",
    "vocab, transform, untransform =  collapse_vocabulary_by_distance(tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>topic</th>\n",
       "      <th>GAM</th>\n",
       "      <th>GEM</th>\n",
       "      <th>EMG</th>\n",
       "      <th>MGD</th>\n",
       "      <th>EMB</th>\n",
       "      <th>MSD</th>\n",
       "      <th>NHM</th>\n",
       "      <th>SRL</th>\n",
       "      <th>GER</th>\n",
       "      <th>NLS</th>\n",
       "      <th>PIG</th>\n",
       "      <th>GRS</th>\n",
       "      <th>ROT</th>\n",
       "      <th>LEF</th>\n",
       "      <th>IST</th>\n",
       "      <th>ARC</th>\n",
       "      <th>MSL</th>\n",
       "      <th>FSM</th>\n",
       "      <th>OVP</th>\n",
       "      <th>SRF</th>\n",
       "      <th>SSC</th>\n",
       "      <th>FLT</th>\n",
       "      <th>SEN</th>\n",
       "      <th>CDR</th>\n",
       "      <th>MTM</th>\n",
       "      <th>STT</th>\n",
       "      <th>RTH</th>\n",
       "      <th>TCM</th>\n",
       "      <th>CUL</th>\n",
       "      <th>PRA</th>\n",
       "      <th>CPR</th>\n",
       "      <th>WAT</th>\n",
       "      <th>TMP</th>\n",
       "      <th>LIT</th>\n",
       "      <th>MEC</th>\n",
       "      <th>MPH</th>\n",
       "      <th>NUT</th>\n",
       "      <th>HRM</th>\n",
       "      <th>CHS</th>\n",
       "      <th>MCH</th>\n",
       "      <th>PTH</th>\n",
       "      <th>OBI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.206465</td>\n",
       "      <td>0.192433</td>\n",
       "      <td>0.142681</td>\n",
       "      <td>0.121135</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.040561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>0.021510</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.058361</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>0.028977</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.005461</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.017424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.018243</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.057424</td>\n",
       "      <td>0.009802</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.039785</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.087187</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.013663</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.012197</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.022140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.262169</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>0.006278</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.003737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013515</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.044882</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.062232</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.005364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.059957</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.005614</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.008992</td>\n",
       "      <td>0.016123</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.019695</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.017404</td>\n",
       "      <td>0.011016</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.003115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.035449</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.010495</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.011622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>0.003152</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.012752</td>\n",
       "      <td>0.049442</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.020016</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>0.021773</td>\n",
       "      <td>0.021793</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.017223</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>0.014026</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.005071</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.006676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.008485</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.017018</td>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.011388</td>\n",
       "      <td>0.028067</td>\n",
       "      <td>0.014992</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.012612</td>\n",
       "      <td>0.011474</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.011829</td>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.012263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.007083</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.013262</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.004068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.022143</td>\n",
       "      <td>0.012838</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.033695</td>\n",
       "      <td>0.028497</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>0.026726</td>\n",
       "      <td>0.069657</td>\n",
       "      <td>0.031989</td>\n",
       "      <td>0.037196</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.024371</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>0.033331</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.053941</td>\n",
       "      <td>0.022435</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.029591</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.009465</td>\n",
       "      <td>0.009905</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.019853</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.015295</td>\n",
       "      <td>0.031656</td>\n",
       "      <td>0.019664</td>\n",
       "      <td>0.037580</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.017119</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.021561</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.019421</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>0.018113</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.047682</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.019492</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.011612</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.023303</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.041410</td>\n",
       "      <td>0.019111</td>\n",
       "      <td>0.020054</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>0.016257</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.011277</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.008020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.045083</td>\n",
       "      <td>0.018417</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.010017</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>0.023338</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.045204</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.071050</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018309</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.017342</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.028371</td>\n",
       "      <td>0.028561</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.029938</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.033641</td>\n",
       "      <td>0.041581</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.042626</td>\n",
       "      <td>0.043575</td>\n",
       "      <td>0.047231</td>\n",
       "      <td>0.036044</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.031577</td>\n",
       "      <td>0.031599</td>\n",
       "      <td>0.043350</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.038709</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>0.020905</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.024233</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>0.025404</td>\n",
       "      <td>0.027360</td>\n",
       "      <td>0.020511</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.010175</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.031948</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.011153</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.006386</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.003392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.009097</td>\n",
       "      <td>0.021376</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.005194</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>0.092877</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.011007</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.008536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.011513</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.009321</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>0.072724</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.016981</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.013349</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.016916</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.018676</td>\n",
       "      <td>0.012843</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.016756</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>0.011622</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.008668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>0.013315</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>0.006932</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.130706</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.006210</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.008371</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.070529</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.057581</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.015455</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.016346</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.034775</td>\n",
       "      <td>0.019280</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.011905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.016857</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.013741</td>\n",
       "      <td>0.014057</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.021517</td>\n",
       "      <td>0.020184</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.012948</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.013113</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.015745</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.022916</td>\n",
       "      <td>0.013076</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.066736</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.011952</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.017332</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.007397</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.069085</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.005514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.057531</td>\n",
       "      <td>0.041537</td>\n",
       "      <td>0.010550</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>0.033036</td>\n",
       "      <td>0.038028</td>\n",
       "      <td>0.050707</td>\n",
       "      <td>0.051120</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.015760</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.018931</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.017253</td>\n",
       "      <td>0.059550</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.047082</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>0.017856</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.039206</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.014899</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.083011</td>\n",
       "      <td>0.023414</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.045671</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.019291</td>\n",
       "      <td>0.053616</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.009026</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.007440</td>\n",
       "      <td>0.006966</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.006282</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>0.015168</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>0.013046</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.020388</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.011562</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.028095</td>\n",
       "      <td>0.012441</td>\n",
       "      <td>0.000867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.014223</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.007844</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.010990</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>0.012578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.002767</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.026264</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.008717</td>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.028026</td>\n",
       "      <td>0.025574</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.046758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order  topic       GAM       GEM       EMG       MGD       EMB       MSD       NHM       SRL       GER       NLS       PIG       GRS       ROT       LEF       IST       ARC       MSL       FSM       OVP       SRF       SSC       FLT       SEN       CDR       MTM       STT       RTH       TCM       CUL       PRA       CPR       WAT       TMP       LIT       MEC       MPH       NUT       HRM       CHS       MCH       PTH       OBI\n",
       "0       0      2  0.206465  0.192433  0.142681  0.121135  0.006981  0.040561  0.000000  0.003730  0.007837  0.003482  0.001725  0.006544  0.007578  0.003693  0.001681  0.001899  0.003997  0.003619  0.003305  0.010721  0.000317  0.001600  0.001777  0.000247  0.002788  0.001106  0.006391  0.003825  0.001988  0.000941  0.002064  0.000519  0.003258  0.001169  0.000623  0.002805  0.001063  0.000982  0.001453  0.000086  0.000451  0.000000\n",
       "1       1      0  0.004698  0.011868  0.021510  0.001570  0.058361  0.011149  0.000244  0.003034  0.000579  0.000541  0.003409  0.002428  0.001793  0.001888  0.000427  0.000178  0.005884  0.000774  0.001366  0.000784  0.000269  0.002231  0.000617  0.000106  0.000306  0.000111  0.000978  0.000765  0.004561  0.001064  0.000289  0.000132  0.001890  0.000220  0.000000  0.000074  0.000613  0.000517  0.000431  0.000370  0.000121  0.000007\n",
       "2       2     21  0.008398  0.023307  0.028977  0.003173  0.038768  0.022700  0.000516  0.008301  0.001796  0.001207  0.007185  0.002461  0.001848  0.002379  0.003289  0.001270  0.003363  0.001301  0.000645  0.001692  0.000715  0.001363  0.001578  0.000286  0.003746  0.000908  0.002614  0.001799  0.003097  0.000763  0.000709  0.000347  0.001227  0.000603  0.000000  0.006052  0.000624  0.001341  0.000566  0.000690  0.000399  0.000043\n",
       "3       3      9  0.001523  0.002149  0.019167  0.003671  0.035158  0.010354  0.003123  0.010431  0.006736  0.003205  0.005461  0.003117  0.002675  0.002888  0.003402  0.003523  0.003116  0.004961  0.002847  0.004114  0.001461  0.002066  0.002724  0.000291  0.001344  0.001765  0.001156  0.003401  0.001688  0.002910  0.000788  0.003205  0.004606  0.002541  0.000570  0.000855  0.003215  0.003368  0.001990  0.003492  0.002019  0.017424\n",
       "4       4      8  0.003798  0.011468  0.018243  0.001391  0.057424  0.009802  0.000236  0.002956  0.000257  0.000801  0.002208  0.000958  0.001153  0.001055  0.002476  0.000172  0.000456  0.001224  0.000293  0.001013  0.000264  0.001256  0.000552  0.000112  0.000288  0.000148  0.000996  0.003264  0.008392  0.000741  0.000305  0.000153  0.001566  0.000247  0.000011  0.000084  0.001408  0.001087  0.001017  0.000282  0.001253  0.000000\n",
       "5       5      6  0.011152  0.028868  0.039785  0.004846  0.087187  0.021400  0.000623  0.013663  0.003992  0.009933  0.009072  0.005224  0.006453  0.006993  0.005066  0.004211  0.003949  0.005423  0.001540  0.004602  0.006319  0.003832  0.004517  0.003934  0.000576  0.001303  0.006422  0.012197  0.005723  0.000821  0.000737  0.000409  0.002370  0.004514  0.006109  0.000258  0.001688  0.002375  0.001847  0.001538  0.001177  0.022140\n",
       "6       6     33  0.009088  0.007088  0.008954  0.008810  0.004574  0.016767  0.262169  0.010049  0.004607  0.005871  0.008684  0.008692  0.005513  0.006923  0.005826  0.010912  0.012315  0.006254  0.005450  0.006750  0.005030  0.005617  0.008846  0.006278  0.006783  0.004649  0.003456  0.006584  0.011526  0.002679  0.005109  0.008421  0.008755  0.006405  0.005164  0.004237  0.004350  0.004317  0.003814  0.004692  0.004293  0.003737\n",
       "7       7      5  0.013515  0.034613  0.044882  0.001020  0.002280  0.000256  0.000012  0.062232  0.000375  0.000853  0.052411  0.000483  0.000511  0.000630  0.000140  0.000431  0.000580  0.000757  0.000005  0.000649  0.013769  0.000322  0.005104  0.000016  0.000060  0.000849  0.001823  0.001531  0.000807  0.000539  0.001037  0.000035  0.001213  0.000596  0.002834  0.000003  0.000323  0.000201  0.000369  0.000468  0.000460  0.005364\n",
       "8       8     18  0.001775  0.005562  0.005065  0.003878  0.006226  0.002865  0.005627  0.061448  0.009681  0.005633  0.024385  0.003478  0.003367  0.002834  0.001322  0.001868  0.002180  0.001689  0.000077  0.001238  0.000512  0.001773  0.003148  0.000726  0.001953  0.001177  0.000957  0.004714  0.004345  0.001234  0.001176  0.002048  0.004479  0.003657  0.008630  0.001450  0.005328  0.004031  0.001545  0.000566  0.001530  0.000297\n",
       "9       9     26  0.003133  0.003484  0.001818  0.004276  0.002164  0.005713  0.000883  0.007208  0.059957  0.008508  0.005384  0.006608  0.006950  0.006083  0.005713  0.006012  0.004132  0.004155  0.006218  0.005454  0.010419  0.005614  0.013990  0.001030  0.004904  0.003196  0.000423  0.003834  0.003998  0.002381  0.002192  0.008992  0.016123  0.003653  0.003108  0.019695  0.003648  0.017404  0.011016  0.003634  0.001976  0.003115\n",
       "10     10      4  0.000278  0.000099  0.002896  0.000621  0.004366  0.001099  0.006750  0.013249  0.006915  0.004418  0.035449  0.012720  0.002654  0.006623  0.006665  0.003618  0.010992  0.003854  0.000755  0.003930  0.006489  0.004731  0.004018  0.003743  0.000000  0.003523  0.000804  0.002351  0.005076  0.001579  0.001408  0.002400  0.002742  0.004466  0.010495  0.000538  0.003211  0.001592  0.002893  0.002174  0.002477  0.011622\n",
       "11     11     25  0.003152  0.001498  0.000881  0.009007  0.002566  0.001379  0.016154  0.003554  0.012907  0.011124  0.012752  0.049442  0.013040  0.020016  0.017491  0.021773  0.021793  0.014114  0.001446  0.017223  0.003937  0.014591  0.014026  0.000169  0.000000  0.005172  0.004278  0.013555  0.012202  0.000934  0.001025  0.003255  0.005071  0.007352  0.004908  0.011316  0.002256  0.005470  0.003305  0.001580  0.007533  0.006676\n",
       "12     12     31  0.002159  0.003635  0.002647  0.012014  0.002171  0.003811  0.008485  0.008264  0.017018  0.013618  0.011388  0.028067  0.014992  0.013888  0.008701  0.012612  0.011474  0.008342  0.001070  0.009738  0.009330  0.012467  0.006250  0.004680  0.000000  0.002798  0.005222  0.011181  0.008998  0.003832  0.003479  0.017226  0.008795  0.006733  0.007653  0.021922  0.011829  0.010253  0.009750  0.009822  0.006238  0.012263\n",
       "13     13      7  0.001746  0.001886  0.001113  0.007920  0.001930  0.000182  0.000029  0.005839  0.003712  0.011517  0.004687  0.010239  0.043792  0.009725  0.009991  0.010140  0.004990  0.005216  0.002229  0.007083  0.002192  0.008743  0.005752  0.001659  0.003415  0.005330  0.006655  0.010502  0.002049  0.005555  0.005249  0.008667  0.003271  0.009199  0.016124  0.013262  0.007826  0.011909  0.005255  0.012350  0.001646  0.004068\n",
       "14     14     36  0.002712  0.002226  0.006262  0.007138  0.005969  0.002592  0.022143  0.012838  0.013551  0.033695  0.028497  0.037299  0.026726  0.069657  0.031989  0.037196  0.034380  0.034761  0.006035  0.024371  0.022318  0.033331  0.030889  0.003893  0.053941  0.022435  0.009199  0.029591  0.007362  0.009652  0.009465  0.009905  0.011512  0.018841  0.019853  0.020345  0.012927  0.008055  0.005435  0.006817  0.012268  0.000000\n",
       "15     15     29  0.004683  0.002114  0.001455  0.009231  0.003212  0.003286  0.000000  0.004520  0.012667  0.032095  0.006603  0.015295  0.031656  0.019664  0.037580  0.020062  0.015328  0.015883  0.002291  0.015270  0.010744  0.017119  0.007176  0.011717  0.018400  0.008008  0.021561  0.020881  0.005333  0.002287  0.000378  0.004002  0.002673  0.028490  0.004971  0.011854  0.004880  0.006478  0.003544  0.000527  0.001089  0.000000\n",
       "16     16     28  0.001231  0.000074  0.001414  0.005560  0.001485  0.000372  0.010451  0.002962  0.011535  0.019421  0.007148  0.016632  0.018113  0.020733  0.029933  0.047682  0.010208  0.019492  0.010019  0.013095  0.011612  0.022941  0.023303  0.003809  0.013401  0.041410  0.019111  0.020054  0.007733  0.005030  0.006916  0.016257  0.008759  0.011277  0.007209  0.008479  0.006618  0.015359  0.011225  0.004247  0.009848  0.008020\n",
       "17     17     20  0.001978  0.000986  0.002225  0.003546  0.002576  0.002225  0.002211  0.003942  0.005364  0.012400  0.003802  0.008472  0.006275  0.015802  0.029690  0.029764  0.012104  0.045083  0.018417  0.012401  0.005937  0.010017  0.010201  0.000046  0.006877  0.007958  0.002308  0.017047  0.005117  0.000626  0.000547  0.002341  0.002001  0.003277  0.003404  0.000850  0.001987  0.002756  0.001301  0.000643  0.000594  0.001660\n",
       "18     18     23  0.023338  0.037512  0.013596  0.045204  0.001688  0.026864  0.000340  0.001008  0.001362  0.002789  0.001245  0.002498  0.002027  0.003012  0.005626  0.002974  0.001548  0.005013  0.071050  0.017967  0.004725  0.002034  0.007594  0.000000  0.000496  0.001192  0.012263  0.003112  0.001735  0.000952  0.003889  0.000414  0.003639  0.000326  0.004937  0.002125  0.002550  0.001547  0.002115  0.000395  0.000617  0.000900\n",
       "19     19      1  0.018309  0.012387  0.023640  0.022068  0.017342  0.042932  0.003978  0.028371  0.028561  0.041992  0.029938  0.032586  0.036241  0.033641  0.041581  0.043823  0.042626  0.043575  0.047231  0.036044  0.026004  0.031577  0.031599  0.043350  0.038697  0.035949  0.032012  0.038709  0.029080  0.028341  0.030153  0.020905  0.025999  0.034371  0.030706  0.029891  0.024233  0.027892  0.025404  0.027360  0.020511  0.014856\n",
       "20     20     13  0.003021  0.004101  0.002082  0.017111  0.002068  0.004938  0.001298  0.002388  0.010175  0.009245  0.006218  0.010122  0.010398  0.009359  0.014703  0.016114  0.006502  0.011501  0.011067  0.031948  0.009937  0.008944  0.011153  0.002072  0.011059  0.005027  0.004595  0.010078  0.004174  0.002917  0.006645  0.005187  0.006386  0.005858  0.004928  0.004270  0.005306  0.004938  0.003721  0.002857  0.001978  0.003392\n",
       "21     21     24  0.014230  0.003411  0.002508  0.013786  0.000914  0.007365  0.002437  0.001702  0.001847  0.001780  0.001397  0.004002  0.002043  0.002572  0.002865  0.002955  0.002251  0.007467  0.009097  0.021376  0.001815  0.001160  0.004358  0.000078  0.000548  0.002239  0.001458  0.002947  0.003214  0.001007  0.000605  0.001112  0.001255  0.001082  0.002126  0.008882  0.000881  0.000602  0.002231  0.000377  0.001429  0.000000\n",
       "22     22     27  0.002683  0.004661  0.006640  0.003354  0.008078  0.055075  0.001709  0.007310  0.023617  0.003250  0.016900  0.004228  0.002600  0.004100  0.003040  0.002639  0.005194  0.004288  0.004948  0.005402  0.092877  0.004629  0.006612  0.000029  0.003281  0.011007  0.007077  0.003981  0.007563  0.012349  0.002931  0.005432  0.005722  0.003472  0.001567  0.006702  0.003558  0.006459  0.004056  0.003826  0.001329  0.008536\n",
       "23     23     10  0.001354  0.000304  0.000875  0.002261  0.002158  0.002361  0.000467  0.001740  0.008731  0.010474  0.006473  0.011513  0.009831  0.014886  0.008938  0.011900  0.003809  0.011397  0.001804  0.009321  0.008646  0.072724  0.012969  0.016780  0.045744  0.002649  0.003614  0.006558  0.003940  0.002177  0.001792  0.007997  0.004043  0.013227  0.005746  0.002288  0.002101  0.003233  0.004234  0.001462  0.001506  0.000493\n",
       "24     24     22  0.003512  0.001797  0.006996  0.010545  0.003899  0.006351  0.001495  0.007939  0.007813  0.016981  0.012437  0.014212  0.013349  0.013162  0.010733  0.016916  0.016444  0.011544  0.010653  0.014024  0.009882  0.018676  0.012843  0.013975  0.007757  0.013657  0.011312  0.010568  0.016055  0.011883  0.010033  0.017327  0.011642  0.016414  0.016756  0.010759  0.013410  0.011735  0.011622  0.012261  0.006354  0.008668\n",
       "25     25     38  0.004185  0.001063  0.017382  0.003546  0.003757  0.006947  0.000809  0.004403  0.011705  0.013858  0.008240  0.011908  0.009580  0.017305  0.014599  0.018611  0.016070  0.013315  0.003936  0.013528  0.006932  0.047670  0.130706  0.018829  0.009238  0.004914  0.004921  0.005044  0.002741  0.001215  0.002834  0.002470  0.004523  0.009235  0.000468  0.009032  0.005772  0.006210  0.004450  0.004748  0.001709  0.000774\n",
       "26     26     11  0.000187  0.000341  0.000689  0.001025  0.001039  0.000016  0.000628  0.008295  0.006209  0.019869  0.008744  0.004281  0.005974  0.004181  0.008371  0.005748  0.006617  0.003043  0.000494  0.003400  0.005184  0.009352  0.004364  0.070529  0.002310  0.004409  0.007405  0.003967  0.004333  0.002250  0.002204  0.002055  0.003401  0.057581  0.004898  0.003006  0.004091  0.005424  0.004159  0.002782  0.001443  0.000000\n",
       "27     27     17  0.002201  0.001178  0.002269  0.014309  0.002102  0.000914  0.000268  0.005459  0.006015  0.015455  0.002773  0.006548  0.031423  0.007452  0.007491  0.009152  0.004487  0.005050  0.006831  0.007292  0.002240  0.005093  0.009137  0.000155  0.003973  0.016346  0.109834  0.020293  0.004139  0.002736  0.005965  0.005581  0.003822  0.005232  0.034775  0.019280  0.011276  0.016469  0.007756  0.012745  0.003642  0.011905\n",
       "28     28     15  0.001789  0.001028  0.006392  0.009129  0.003607  0.003741  0.013584  0.005400  0.009437  0.016857  0.004649  0.013741  0.014057  0.019569  0.013560  0.020530  0.010357  0.021517  0.020184  0.012040  0.012948  0.010371  0.005416  0.009772  0.013113  0.025599  0.015745  0.033285  0.022916  0.013076  0.006687  0.002317  0.004701  0.005528  0.014408  0.002765  0.005213  0.006775  0.003060  0.003544  0.002612  0.000000\n",
       "29     29     16  0.000937  0.000630  0.000535  0.000825  0.001972  0.000029  0.000543  0.003294  0.003834  0.002327  0.005029  0.009226  0.004288  0.007035  0.003277  0.002729  0.011667  0.002862  0.003255  0.004246  0.005837  0.007345  0.005681  0.007338  0.000000  0.002214  0.002982  0.002776  0.005273  0.066736  0.008594  0.011952  0.006498  0.003910  0.000848  0.000611  0.017332  0.003494  0.005950  0.006150  0.005926  0.000000\n",
       "30     30     34  0.008123  0.000053  0.000890  0.001280  0.000843  0.000192  0.004338  0.005267  0.006013  0.004129  0.007397  0.005588  0.003633  0.003216  0.003850  0.002625  0.006201  0.002702  0.001757  0.002163  0.002122  0.004028  0.002368  0.002445  0.005330  0.004007  0.003864  0.003401  0.004497  0.006588  0.069085  0.002982  0.002995  0.004031  0.002591  0.000000  0.002657  0.003151  0.002370  0.000508  0.003147  0.005514\n",
       "31     31      3  0.000056  0.000060  0.001327  0.003947  0.001366  0.000251  0.001664  0.001427  0.007757  0.003122  0.002613  0.004882  0.005101  0.001910  0.003542  0.001020  0.003806  0.001092  0.000742  0.002914  0.002153  0.006092  0.003937  0.002330  0.000000  0.004313  0.002370  0.003616  0.004219  0.003715  0.003607  0.057531  0.041537  0.010550  0.000763  0.033676  0.033036  0.038028  0.050707  0.051120  0.007951  0.000211\n",
       "32     32     30  0.000931  0.000240  0.000881  0.004600  0.001340  0.001713  0.004288  0.003876  0.015760  0.004917  0.002521  0.005233  0.004108  0.004494  0.008892  0.005098  0.009367  0.003612  0.003543  0.002768  0.005906  0.004125  0.005377  0.002889  0.000559  0.001802  0.005588  0.004142  0.003426  0.018931  0.005315  0.017253  0.059550  0.007350  0.001201  0.003603  0.005939  0.002928  0.005325  0.003065  0.003471  0.000000\n",
       "33     33     39  0.000056  0.000330  0.000620  0.001359  0.000811  0.000074  0.012303  0.005098  0.001827  0.003070  0.004542  0.004536  0.004526  0.002729  0.002518  0.001871  0.003149  0.002062  0.001647  0.002788  0.001829  0.003019  0.011447  0.013265  0.001041  0.012021  0.001986  0.001038  0.006545  0.004477  0.006334  0.002452  0.002942  0.047082  0.004436  0.030895  0.001886  0.001442  0.009487  0.000303  0.002528  0.000712\n",
       "34     34     35  0.000743  0.000214  0.000013  0.001393  0.000299  0.000137  0.000189  0.001609  0.001631  0.001850  0.003208  0.001604  0.001663  0.000886  0.000557  0.000967  0.002256  0.000495  0.000020  0.001793  0.001115  0.001544  0.004485  0.000107  0.013318  0.001614  0.000982  0.000664  0.000567  0.006166  0.006684  0.002733  0.001361  0.001643  0.004137  0.001025  0.077990  0.003622  0.003034  0.004506  0.002422  0.000654\n",
       "35     35     14  0.000129  0.000473  0.000069  0.001421  0.001171  0.000000  0.000000  0.001214  0.024576  0.003422  0.001528  0.003505  0.005058  0.003679  0.002507  0.003901  0.012437  0.001526  0.000127  0.001927  0.001740  0.005186  0.004292  0.002494  0.006253  0.017856  0.006647  0.004378  0.000327  0.003497  0.001147  0.039206  0.011847  0.004305  0.002985  0.014899  0.012108  0.083011  0.023414  0.002185  0.003677  0.006431\n",
       "36     36     19  0.000689  0.000091  0.001020  0.000724  0.000715  0.000076  0.000399  0.000352  0.008649  0.002945  0.002185  0.003695  0.005659  0.001669  0.002610  0.000756  0.006628  0.000427  0.000000  0.002166  0.000255  0.005731  0.006024  0.002107  0.000440  0.002016  0.002130  0.003342  0.003556  0.001087  0.003233  0.045671  0.014393  0.002377  0.000240  0.009127  0.011592  0.019291  0.053616  0.010980  0.003919  0.000052\n",
       "37     37     37  0.006165  0.009026  0.005512  0.008860  0.005307  0.007822  0.000000  0.005387  0.005856  0.007440  0.006966  0.006915  0.008070  0.007327  0.008642  0.006114  0.011633  0.007928  0.005578  0.007088  0.005671  0.008363  0.006282  0.009893  0.015168  0.009801  0.004794  0.010178  0.013046  0.011742  0.020388  0.009209  0.007445  0.012406  0.008258  0.003029  0.011562  0.011967  0.007779  0.028095  0.012441  0.000867\n",
       "38     38     12  0.000094  0.000035  0.000025  0.000089  0.000772  0.000000  0.003800  0.001552  0.000862  0.000893  0.001014  0.004357  0.001830  0.002609  0.001078  0.000745  0.014223  0.000746  0.000063  0.001272  0.000034  0.001999  0.001231  0.000790  0.007844  0.000736  0.004759  0.001475  0.002430  0.008668  0.001131  0.007576  0.002300  0.002849  0.010990  0.000034  0.001776  0.007148  0.006007  0.011814  0.113994  0.012578\n",
       "39     39     32  0.001664  0.001950  0.002598  0.001310  0.001743  0.001419  0.000000  0.004525  0.004691  0.005733  0.001953  0.006194  0.005761  0.005471  0.004003  0.004138  0.009738  0.002092  0.001640  0.002767  0.003590  0.005277  0.002605  0.000137  0.000486  0.002881  0.001982  0.001519  0.001656  0.003967  0.001804  0.026264  0.025835  0.003492  0.016545  0.003580  0.008717  0.019599  0.028026  0.025574  0.030236  0.046758"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "\n",
    "# The list of texts has to be created from the same descriptions used to create the reduced vocab (simple preprocess).\n",
    "to_reduced_vocab = lambda t: \" \".join([transform.get(w,\"\") for w in t.split(\" \")])\n",
    "texts = [to_reduced_vocab(d) for d in descriptions_simple_preprocessing.values()]\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", max_df=0.95, min_df=2, lowercase=False)\n",
    "features = vectorizer.fit_transform(texts)\n",
    "cls = NMF(n_components=40, random_state=0)\n",
    "cls.fit(features)\n",
    "\n",
    "# Function for retrieving the topic vectors for a list of text descriptions.\n",
    "def get_topic_embeddings(texts, model, vectorizer):\n",
    "    ngrams_vectors = vectorizer.transform(texts).toarray()\n",
    "    topic_vectors = model.transform(ngrams_vectors)\n",
    "    return(topic_vectors)\n",
    "    \n",
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_topic_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    texts = [descriptions_simple_preprocessing[i] for i in ids]\n",
    "    texts = [to_reduced_vocab(d) for d in texts]\n",
    "    topic_vectors = get_topic_embeddings(texts, cls, vectorizer)\n",
    "    mean_topic_vector = np.mean(topic_vectors, axis=0)\n",
    "    group_to_topic_vector[group_id] = mean_topic_vector\n",
    "    \n",
    "df = pd.DataFrame(group_to_topic_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "lmdf = pd.read_csv(filename)\n",
    "df = df[lmdf[\"Subset Symbol\"].values]\n",
    "\n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "df[\"idxmax\"] = df.idxmax(axis = 1)\n",
    "df[\"idxmax\"] = df[\"idxmax\"].apply(lambda x: df.columns.get_loc(x))\n",
    "df = df.sort_values(by=\"idxmax\")\n",
    "df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "df = df.reset_index(drop=False).rename({\"index\":\"topic\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "df.to_csv(\"~/Desktop/hm.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_models\"></a>\n",
    "### Loading trained and saved NLP neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and models related to the machine learning text embedding methods.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "#bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "#bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "#bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "#bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "#bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "#bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vocab\"></a>\n",
    "### Buildling vocabularies\n",
    "Normalizing case, lemmatization, stemming, removing stopwords, removing punctuation, handling numerics, creating parse trees, part-of-speech tagging, and anything else necessary for a particular dataset of descripitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a vocabulary by looking at what words are overrepresented in domain specific text.\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "tokens = get_overrepresented_tokens(phenotypes_corpus, background_corpus, max_features=5000)\n",
    "vocabulary_from_text = build_vocabulary_from_tokens(tokens)\n",
    "\n",
    "# Constructing a vocabulary by assuming all words present in a given ontology are important.\n",
    "ontology = Ontology(ontology_filename)\n",
    "vocabulary_from_ontology = build_vocabulary_from_tokens(ontology.get_tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing text descriptions\n",
    "The preprocessing methods applied to the phenotype descriptions are a choice which impacts the subsequent vectorization and similarity methods which construct the pairwise distance matrix from each of these descriptions. The preprocessing methods that make sense are also highly dependent on the vectorization method or embedding method that is to be applied. For example, stemming (which is part of the full proprocessing done below using `gensim.preprocess_string` is useful for the n-grams and bag-of-words methods but not for the document embeddings methods which need each token to be in the vocabulary that was constructed and used when the model was trained. For this reason, embedding methods with pretrained models where the vocabulary is fixed should have a lighter degree of preprocessing not involving stemming or lemmatization but should involve things like removal of non-alphanumerics and normalizing case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a mapping between IDs and the raw text descriptions associated with that ID from the dataset.\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "\n",
    "# Preprocessing of the text descriptions. Different methods are necessary for different approaches.\n",
    "descriptions_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "descriptions_no_stopwords = {i:remove_stopwords(d) for i,d in descriptions.items()}\n",
    "\n",
    "# Generating random descriptions that are drawn from same token population and retain original lengths.\n",
    "#tokens = [w for w in itertools.chain.from_iterable(word_tokenize(desc) for desc in descriptions.values())]\n",
    "#descriptions_scrambled = {k:\" \".join(np.random.choice(tokens,len(word_tokenize(v)))) for k,v in descriptions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_tagging\"></a>\n",
    "### POS tagging the phenotype descriptions for nouns and adjectives\n",
    "Note that preprocessing of the descriptions should be done after part-of-speech tagging, because tokens that are removed during preprocessing before n-gram analysis contain information that the parser needs to accurately call parts-of-speech. This step should be done on the raw descriptions and then the resulting bags of words can be subset using additional preprocesssing steps before input in one of the vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "descriptions_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "descriptions_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "descriptions_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_adj_only.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annotation\"></a>\n",
    "### Annotating descriptions with ontology terms\n",
    "This section generates dictionaries that map gene IDs from the dataset to lists of strings, where those strings are ontology term IDs. How the term IDs are found for each gene entry with its corresponding phenotype description depends on the cell below. Firstly, the terms are found by using the NOBLE Coder annotation tool through these wrapper functions to identify the terms by looking for instances of the term's label or synonyms in the actual text of the phenotype descriptions. Secondly, the next cell just draws the terms directly from the dataset itself. In this case, these are high-confidence annotations done by curators for a comparison against what can be accomplished through computational analysis of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ontology term annotators over either raw or preprocessed text descriptions.\n",
    "annotations_noblecoder_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=1)\n",
    "annotations_noblecoder_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID to term list annotation dictionaries for each ontology in the dataset.\n",
    "annotations = dataset.get_annotations_dictionary()\n",
    "go_annotations = {k:[term for term in v if term[0:2]==\"GO\"] for k,v in annotations.items()}\n",
    "po_annotations = {k:[term for term in v if term[0:2]==\"PO\"] for k,v in annotations.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matrix\"></a>\n",
    "<a id=\"methods\"></a>\n",
    "<a id=\"run\"></a>\n",
    "<a id=\"merging\"></a>\n",
    "### Generating vector representations and pairwise distances matrices\n",
    "This section uses the text descriptions, preprocessed text descriptions, or ontology term annotations created or read in the previous sections to generate a vector representation for each gene and build a pairwise distance matrix for the whole dataset. Each method specified is a unique combination of a method of vectorization (bag-of-words, n-grams, document embedding model, etc) and distance metric (Euclidean, Jaccard, cosine, etc) applied to those vectors in constructing the pairwise matrix. The method of vectorization here is equivalent to feature selection, so the task is to figure out which type of vectors will encode features that are useful (n-grams, full words, only words from a certain vocabulary, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of different methods for calculating distance between text descriptions using the Methods object \n",
    "# defined in the utilities for this notebook. The constructor takes a string for the method name, a string defining\n",
    "# the hyperparameter choices for that method, a function to be called to run this method, a dictionary of arguments\n",
    "# by keyword that should be passed to that function, and a distance metric from scipy.spatial.distance to associate\n",
    "# with this method.\n",
    "\n",
    "methods = [\n",
    "\n",
    "    \n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    #Method(\"Doc2Vec Wikipedia\", \"Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    #Method(\"Doc2Vec PubMed\", \"Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "        \n",
    "    # Methods that use variations on the n-grams approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use variations on the n-grams approach with simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use variations on the n-grams approach selecting for specific parts-of-speech.\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    \n",
    "    # Methods that use terms inferred from automated annotation of the text.\n",
    "    #Method(\"NOBLE Coder\", \"Precise\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"NOBLE Coder\", \"Partial\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"NOBLE Coder\", \"Precise,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use terms assigned by humans that are present in the dataset.\n",
    "    #Method(\"GO\", \"Default\", pw.pairwise_square_annotations, {\"ids_to_annotations\":go_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"PO\", \"Default\", pw.pairwise_square_annotations, {\"ids_to_annotations\":po_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate all of the pairwise distance matrices in parallel.\n",
    "start_time_mp = time.perf_counter()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(function_wrapper_with_duration, args=(method.function, method.kwargs)) for method in methods]\n",
    "results = [result.get() for result in results]\n",
    "graphs = {method.name_with_hyperparameters:result[0] for method,result in zip(methods,results)}\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for tup in methods}\n",
    "durations = {method.name_with_hyperparameters:result[1] for method,result in zip(methods,results)}\n",
    "pool.close()\n",
    "pool.join()    \n",
    "total_time_mp = time.perf_counter()-start_time_mp\n",
    "\n",
    "# Reporting how long each matrix took to build and how much time parallel processing saved.\n",
    "print(\"Durations of generating each pairwise similarity matrix (hh:mm:ss)\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "savings = total_time_mp/sum(durations.values())\n",
    "for (name,duration) in durations.items():\n",
    "    print(\"{:50} {}\".format(name, to_hms(duration)))\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"{:15} {}\".format(\"total\", to_hms(sum(durations.values()))))\n",
    "print(\"{:15} {} ({:.2%} of single thread time)\".format(\"multiprocess\", to_hms(total_time_mp), savings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Wikipedia:Size=300,Mean                   00:00:02\n",
      "Word2Vec Wikipedia:Size=300,Max                    00:00:02\n",
      "N-Grams:Full,Words,1-grams,2-grams                 00:00:41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-39f865f61c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdurations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_wrapper_with_duration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_with_hyperparameters\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_with_hyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oats/oats/utils/utils.py\u001b[0m in \u001b[0;36mfunction_wrapper_with_duration\u001b[0;34m(function, args)\u001b[0m\n\u001b[1;32m    119\u001b[0m \t\"\"\"\n\u001b[1;32m    120\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oats/oats/graphs/pairwise.py\u001b[0m in \u001b[0;36mpairwise_square_ngrams\u001b[0;34m(ids_to_texts, metric, tfidf, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# Apply distance metric over all the vectors to yield a matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrings_to_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquareform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0medgelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_adjacency_matrix_to_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_in_matrix_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mid_to_vector_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex_in_matrix_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mpdist\u001b[0;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m             pdist_fn = getattr(_distance_wrap,\n\u001b[1;32m   2065\u001b[0m                                \"pdist_%s_%s_wrap\" % (metric_name, typ))\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[0mpdist_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate all the pairwise distance matrices (not in parallel).\n",
    "graphs = {}\n",
    "names = []\n",
    "durations = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    names.append(method.name_with_hyperparameters)\n",
    "    durations.append(to_hms(duration))\n",
    "    print(\"{:50} {}\".format(method.name_with_hyperparameters,to_hms(duration)))\n",
    "durations_df = pd.DataFrame({\"method\":names,\"duration\":durations})\n",
    "durations_df.to_csv(os.path.join(OUTPUT_DIR,\"durations.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Mean</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Max</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3270389</th>\n",
       "      <td>6024</td>\n",
       "      <td>6025</td>\n",
       "      <td>0.087342</td>\n",
       "      <td>0.053785</td>\n",
       "      <td>0.855134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270390</th>\n",
       "      <td>6024</td>\n",
       "      <td>6026</td>\n",
       "      <td>0.150344</td>\n",
       "      <td>0.189917</td>\n",
       "      <td>0.632311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270391</th>\n",
       "      <td>6024</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.111915</td>\n",
       "      <td>0.059905</td>\n",
       "      <td>0.790042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270392</th>\n",
       "      <td>6024</td>\n",
       "      <td>6028</td>\n",
       "      <td>0.113737</td>\n",
       "      <td>0.059905</td>\n",
       "      <td>0.815636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270394</th>\n",
       "      <td>6025</td>\n",
       "      <td>6026</td>\n",
       "      <td>0.291678</td>\n",
       "      <td>0.229865</td>\n",
       "      <td>0.735151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270395</th>\n",
       "      <td>6025</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.082383</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.765190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270396</th>\n",
       "      <td>6025</td>\n",
       "      <td>6028</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.785179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270398</th>\n",
       "      <td>6026</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.273921</td>\n",
       "      <td>0.187227</td>\n",
       "      <td>0.702469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270399</th>\n",
       "      <td>6026</td>\n",
       "      <td>6028</td>\n",
       "      <td>0.281280</td>\n",
       "      <td>0.187227</td>\n",
       "      <td>0.772890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270401</th>\n",
       "      <td>6027</td>\n",
       "      <td>6028</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         from    to  Word2Vec Wikipedia:Size=300,Mean  Word2Vec Wikipedia:Size=300,Max  N-Grams:Full,Words,1-grams,2-grams\n",
       "3270389  6024  6025                          0.087342                         0.053785                            0.855134\n",
       "3270390  6024  6026                          0.150344                         0.189917                            0.632311\n",
       "3270391  6024  6027                          0.111915                         0.059905                            0.790042\n",
       "3270392  6024  6028                          0.113737                         0.059905                            0.815636\n",
       "3270394  6025  6026                          0.291678                         0.229865                            0.735151\n",
       "3270395  6025  6027                          0.082383                         0.057436                            0.765190\n",
       "3270396  6025  6028                          0.086275                         0.057436                            0.785179\n",
       "3270398  6026  6027                          0.273921                         0.187227                            0.702469\n",
       "3270399  6026  6028                          0.281280                         0.187227                            0.772890\n",
       "3270401  6027  6028                          0.001311                         0.000000                            0.018594"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging all of the edgelist dataframes together.\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for method in methods}\n",
    "methods = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "df = pw.merge_edgelists(edgelists, default_value=0.000)\n",
    "df = pw.remove_self_loops(df)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding k nearest neighbors through each method over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3043, 4350, 5969, 3922, 3039, 5839, 4349, 3419, 3500, 3460]\n",
      "[3042, 5951, 5558, 638, 5275, 3882, 3460, 5487, 3546, 3141]\n",
      "[3145, 3729, 1393, 930, 3144, 4305, 5512, 3175, 4761, 3604]\n"
     ]
    }
   ],
   "source": [
    "some_text = \"plants are reduced in height\"\n",
    "for method in methods:\n",
    "    g = graphs[method]\n",
    "    nn = g.get_nearest_neighbor_ids(some_text, k=10)\n",
    "    print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapsing vocabulary with distance matrix between tokens and doing topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(set([w for w in flatten(word_tokenize(d) for d in descriptions_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "\n",
    "# Get the objects needed to collapse the vocabulary and make a new vocabulary.\n",
    "# Make sure that the tokens list is in the same order as the indices representing each word in the distance matrix.\n",
    "# This is only trivial here because the IDs used are ordered integers 0 to n, but this might not always be the case.\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.row_index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "threshold = 0.3\n",
    "vocab, transform, untransform =  collapse_vocabulary_by_distance(tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Some notes about this, making sure the vectorizer has the lowercase=False option, to retain groups at TOKEN_.\n",
    "# LDA should be run on the independent phenes, not the phenotypes?\n",
    "# Then the phenotypes themselves can be categorized as coming from \n",
    "\n",
    "\n",
    "\n",
    "# The list of texts has to be created from the same descriptions used to create the reduced vocab (simple preprocess).\n",
    "to_reduced_vocab = lambda t: \" \".join([transform.get(w,\"\") for w in t.split(\" \")])\n",
    "texts = [to_reduced_vocab(d) for d in descriptions_simple_preprocessing.values()]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", max_df=0.95, min_df=2, lowercase=False)\n",
    "features = vectorizer.fit_transform(texts)\n",
    "\n",
    "\n",
    "cls = NMF(n_components=10, random_state=0)\n",
    "cls.fit(features)\n",
    "\n",
    "\n",
    "n_top_words=5\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    print(i,end=\" \")\n",
    "    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:\n",
    "        \n",
    "        word = feature_names[fid]\n",
    "        #word = untransform[word]\n",
    "        word = \" \".join(untransform[word])\n",
    "        \n",
    "        \n",
    "        print(word, end=' ')\n",
    "    print()\n",
    "\n",
    "\n",
    "    \n",
    "# What's the fraction of each topic for a new phrase? (dimensionality reduction)\n",
    "\n",
    "\n",
    "something = texts[0]\n",
    "v = vectorizer.transform([something]).toarray()[0]\n",
    "topic_vector = cls.transform([v]) # one value per topic\n",
    "print(topic_vector)\n",
    "print(topic_vector.argsort(axis=1)[0][-5:]) # What are the top five topics for this new sentence?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def _infer_document_vector_from_lda(text, model, vectorizer):\n",
    "    \"\"\" \n",
    "    Text has to be preprocessed in the same way as the text used to fit the model.\n",
    "    So this includes things like replacing words with custom tokens to reduce the vocabulary size.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graphs[\"N-Grams:Full,Nouns,1-grams,Binary\"]\n",
    "identifier_list = list(g.vector_dictionary.keys())\n",
    "m = np.array([g.vector_dictionary[i] for i in identifier_list])\n",
    "\n",
    "\n",
    "cls = NMF(n_components=30, random_state=0)\n",
    "cls.fit(m)\n",
    "n_top_words=5\n",
    "feature_names = g.vectorizer_object.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    print(i,end=\" \")\n",
    "    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:\n",
    "        print(feature_names[fid], end=' ')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    \n",
    "# What's the fraction of each topic for a new phrase? (dimensionality reduction)\n",
    "v = g.get_vector(\"stress freez acclim salt germin embryo arrest stage growth lethal \")\n",
    "topic_vector = cls.transform([v]) # one value per topic\n",
    "\n",
    "print(topic_vector)\n",
    "print(topic_vector.argsort(axis=1)[0][-5:]) # What are the top five topics for this new sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v,td,utd = collapse_vocabulary_by_distance(tokens, distance_matrix, 0.5124)\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Create the n by n distance matrix. \n",
    "m = graphs[\"N-Grams:Full,Words,1-grams,2-grams\"].array\n",
    "to_id = graphs[\"N-Grams:Full,Words,1-grams,2-grams\"].row_index_to_id\n",
    "clustering = AffinityPropagation().fit(m)\n",
    "for c in pd.unique(clustering.labels_):\n",
    "    print(\"CLUSTER\",c)\n",
    "    print()\n",
    "    for i,label in enumerate(clustering.labels_):\n",
    "        if (label == c):\n",
    "            print(descriptions[to_id[i]])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create the n by n distance matrix. \n",
    "m = graphs[\"N-Grams:Full,Words,1-grams,2-grams\"].array\n",
    "to_id = graphs[\"N-Grams:Full,Words,1-grams,2-grams\"].row_index_to_id\n",
    "clustering = AgglomerativeClustering(n_clusters=10, linkage=\"complete\", affinity=\"precomputed\").fit(m)\n",
    "for c in pd.unique(clustering.labels_):\n",
    "    print(\"CLUSTER\",c)\n",
    "    print()\n",
    "    for i,label in enumerate(clustering.labels_):\n",
    "        if (label == c):\n",
    "            print(descriptions[to_id[i]])\n",
    "            print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary, the clustering stuff.\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0, )\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "knn = NearestNeighbors(n_neighbors=2, metric=\"cosine\")\n",
    "\n",
    "g = graphs[\"N-Grams:Full,Words,1-grams,2-grams,Binary\"]\n",
    "\n",
    "ids = list(g.vector_dictionary.keys())\n",
    "m = np.array([g.vector_dictionary[i] for i in ids])\n",
    "\n",
    "\n",
    "knn.fit(m)\n",
    "\n",
    "\n",
    "\n",
    "query_text = \"one leaf was green\"\n",
    "query_features = g.vectorizer_object.transform([query_text]).toarray()\n",
    "print(m.shape)\n",
    "print(query_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indices = knn.kneighbors(query_features, return_distance=False)[0]\n",
    "print(neighbor_indices)\n",
    "for i in neighbor_indices:\n",
    "    actual_id = ids[i]\n",
    "    print(descriptions[actual_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modelling?\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "g = graphs[\"N-Grams:Full,Nouns,1-grams,Binary\"]\n",
    "identifier_list = list(g.vector_dictionary.keys())\n",
    "m = np.array([g.vector_dictionary[i] for i in identifier_list])\n",
    "\n",
    "\n",
    "cls = NMF(n_components=30, random_state=0)\n",
    "cls.fit(m)\n",
    "n_top_words=5\n",
    "feature_names = g.vectorizer_object.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    print(i,end=\" \")\n",
    "    for fid in topic_vec.argsort()[-1:-n_top_words-1:-1]:\n",
    "        print(feature_names[fid], end=' ')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    \n",
    "# What's the fraction of each topic for a new phrase? (dimensionality reduction)\n",
    "v = g.get_vector(\"stress freez acclim salt germin embryo arrest stage growth lethal \")\n",
    "topic_vector = cls.transform([v]) # one value per topic\n",
    "\n",
    "print(topic_vector)\n",
    "print(topic_vector.argsort(axis=1)[0][-5:]) # What are the top five topics for this new sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Merging in the previously curated similarity values from the Oellrich, Walls et al. (2015) dataset\n",
    "This section reads in a file that contains the previously calculated distance values from the Oellrich, Walls et al. (2015) dataset, and merges it with the values which are obtained here for all of the applicable natural language processing or machine learning methods used, so that the graphs which are specified by these sets of distances values can be evaluated side by side in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that indicates the distance estimated using curated EQ statements.\n",
    "df = df.merge(right=pppn_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df.fillna(value=0.000,inplace=True)\n",
    "df.rename(columns={\"value\":\"EQs\"}, inplace=True)\n",
    "df[\"EQs\"] = 1-df[\"EQs\"]\n",
    "methods.append(\"EQs\")\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Merging with information about shared pathways or groupings.\n",
    "The relevant information for each edge includes questions like whether or not the two genes that edge connects share a group or biochemical pathway in common, or if those genes are from the same species. This information can then later be used as the target values for predictive models, or for filtering the graphs represented by these edge lists. Either the grouping information or the protein-protein interaction information should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367416 edges between genes that share this feature, 2900430 edges between genes that do not.\n",
      "3267846 edges between genes from the same species, 0 edges between genes that are not.\n"
     ]
    }
   ],
   "source": [
    "# Generate a column indicating whether or not the two genes share this features (e.g., pathway in common, same group).\n",
    "df[\"shared\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "counts = Counter(df[\"shared\"].values)\n",
    "print(\"{} edges between genes that share this feature, {} edges between genes that do not.\".format(counts.get(1,0),counts.get(0,0)))\n",
    "\n",
    "# Generate a column indicating whether or not the two genes are from the same species.\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)*1\n",
    "counts = Counter(df[\"same\"].values)\n",
    "print(\"{} edges between genes from the same species, {} edges between genes that are not.\".format(counts.get(1,0),counts.get(0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Merging with information about protein-protein interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging information from the protein-protein interaction database with this dataset.\n",
    "df = df.merge(right=string_data.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df.fillna(value=0,inplace=True)\n",
    "df[\"shared\"] = (df[\"combined_score\"] != 0.00)*1\n",
    "df.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "### Combining multiple distance methods with machine learning models\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating indicating how likely is it that two genes share atleast one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "method = \"Logistic Regression\"\n",
    "splits = 12\n",
    "kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "df[method] = pd.Series()\n",
    "for train,test in kf.split(df):\n",
    "    lr_model = train_logistic_regression_model(df=df.iloc[train], predictor_columns=methods, target_column=\"shared\")\n",
    "    df[method].iloc[test] = apply_logistic_regression_model(df=df.iloc[test], predictor_columns=methods, model=lr_model)\n",
    "df[method] = 1-df[method]\n",
    "methods.append(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "method = \"Random Forest\"\n",
    "splits = 2\n",
    "kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "df[method] = pd.Series()\n",
    "for train,test in kf.split(df):\n",
    "    rf_model = train_random_forest_model(df=df.iloc[train], predictor_columns=methods, target_column=\"shared\")\n",
    "    df[method].iloc[test] = apply_random_forest_model(df=df.iloc[test],predictor_columns=methods, model=rf_model)\n",
    "df[method] = 1-df[method]\n",
    "methods.append(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Don't do it iteratively, just treat this entire dataset as training data and save the models.\n",
    "# Logistic Regression\n",
    "lr_name = \"Logistic Regression\"\n",
    "model = train_logistic_regression_model(df=df, predictor_columns=methods, target_column=\"shared\")\n",
    "df[lr_name] = apply_logistic_regression_model(df=df, predictor_columns=methods, model=model)\n",
    "df[lr_name] = 1-df[lr_name]\n",
    "save_to_pickle(obj=model,path=\"../data/pickles/lr.model\")\n",
    "# Random Forest\n",
    "rf_name = \"Random Forest\"\n",
    "model = train_random_forest_model(df=df, predictor_columns=methods, target_column=\"shared\")\n",
    "df[rf_name] = apply_random_forest_model(df=df, predictor_columns=methods, model=model)\n",
    "df[rf_name] = 1-df[rf_name]\n",
    "save_to_pickle(obj=model,path=\"../data/pickles/rf.model\")\n",
    "# Add these methods to the method names to use for the below analysis.\n",
    "methods.append(lr_name)\n",
    "methods.append(rf_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ks\"></a>\n",
    "### Do the edges joining genes that share atleast one pathway come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation betwene the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "ppi_pos_dict = {name:(df[df[\"shared\"] > 0.00][name].values) for name in methods}\n",
    "ppi_neg_dict = {name:(df[df[\"shared\"] == 0.00][name].values) for name in methods}\n",
    "for name in methods:\n",
    "    stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "    pos_mean = np.average(ppi_pos_dict[name])\n",
    "    neg_mean = np.average(ppi_neg_dict[name])\n",
    "    pos_n = len(ppi_pos_dict[name])\n",
    "    neg_n = len(ppi_neg_dict[name])\n",
    "    TABLE[name].update({\"mean_1\":pos_mean, \"mean_0\":neg_mean, \"n_1\":pos_n, \"n_0\":neg_n})\n",
    "    TABLE[name].update({\"ks\":stat, \"ks_pval\":p})\n",
    "    \n",
    "    \n",
    "# Show the kernel estimates for each distribution of weights for each method.\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for name,ax in zip(methods,axs.flatten()):\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"kernel_density.png\"),dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"within\"></a>\n",
    "### Looking at within-group or within-pathway distances in each graph\n",
    "The purpose of this section is to determine which methods generated graphs which tightly group genes which share common pathways or group membership with one another. In order to compare across different methods where the distance value distributions are different, the mean distance values for each group for each method are convereted to percentile scores. Lower percentile scores indicate that the average distance value between any two genes that belong to that group is lower than most of the distance values in the entire distribution for that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>n</th>\n",
       "      <th>mean_percentile</th>\n",
       "      <th>mean_rank</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Mean</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Max</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CDR</td>\n",
       "      <td>Circadian rhythms</td>\n",
       "      <td>13</td>\n",
       "      <td>6.207825</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.6472</td>\n",
       "      <td>8.3347</td>\n",
       "      <td>3.8494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEM</td>\n",
       "      <td>Gametophyte, embryo defective (2-10% mutant se...</td>\n",
       "      <td>37</td>\n",
       "      <td>8.126050</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.5858</td>\n",
       "      <td>15.7159</td>\n",
       "      <td>1.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NHM</td>\n",
       "      <td>No homozygous mutant plants; Cause not determined</td>\n",
       "      <td>18</td>\n",
       "      <td>8.523033</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>14.0092</td>\n",
       "      <td>16.6486</td>\n",
       "      <td>1.1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAM</td>\n",
       "      <td>Gametophyte defective (&lt;2% mutant seeds)</td>\n",
       "      <td>82</td>\n",
       "      <td>10.537217</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>17.0872</td>\n",
       "      <td>19.6647</td>\n",
       "      <td>1.7303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EMG</td>\n",
       "      <td>Embryo, gametophyte defective (&gt;10% mutant seeds)</td>\n",
       "      <td>60</td>\n",
       "      <td>14.571617</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>18.6201</td>\n",
       "      <td>30.0952</td>\n",
       "      <td>1.9045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_id                                          full_name   n  mean_percentile  mean_rank  Word2Vec Wikipedia:Size=300,Mean  Word2Vec Wikipedia:Size=300,Max  N-Grams:Full,Words,1-grams,2-grams\n",
       "0      CDR                                  Circadian rhythms  13         6.207825   3.000000                            9.6472                           8.3347                              3.8494\n",
       "1      GEM  Gametophyte, embryo defective (2-10% mutant se...  37         8.126050   2.000000                           13.5858                          15.7159                              1.2025\n",
       "2      NHM  No homozygous mutant plants; Cause not determined  18         8.523033   2.333333                           14.0092                          16.6486                              1.1010\n",
       "3      GAM           Gametophyte defective (<2% mutant seeds)  82        10.537217   3.666667                           17.0872                          19.6647                              1.7303\n",
       "4      EMG  Embryo, gametophyte defective (>10% mutant seeds)  60        14.571617   7.666667                           18.6201                          30.0952                              1.9045"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the average within-pathway phenotype distance values for each method for each particular pathway.\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "within_percentiles_dict = defaultdict(lambda: defaultdict(list))\n",
    "all_weights_dict = {}\n",
    "for method in methods:\n",
    "    all_weights_dict[method] = df[method].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        mean_weight = np.mean((graph.get_values(within_pairs, kind=method)))\n",
    "        within_weights_dict[method][group] = mean_weight\n",
    "        within_percentiles_dict[method][group] = stats.percentileofscore(df[method].values, mean_weight, kind=\"rank\")\n",
    "\n",
    "# Generating a dataframe of percentiles of the mean in-group distance scores.\n",
    "heatmap_data = pd.DataFrame(within_percentiles_dict)\n",
    "heatmap_data = heatmap_data.dropna(axis=0, inplace=False)\n",
    "heatmap_data = heatmap_data.round(4)\n",
    "\n",
    "# Adding relevant information to this dataframe and saving.\n",
    "heatmap_data[\"mean_rank\"] = heatmap_data.rank().mean(axis=1)\n",
    "heatmap_data[\"mean_percentile\"] = heatmap_data.mean(axis=1)\n",
    "heatmap_data.sort_values(by=\"mean_percentile\", inplace=True)\n",
    "heatmap_data.reset_index(inplace=True)\n",
    "heatmap_data[\"group_id\"] = heatmap_data[\"index\"]\n",
    "heatmap_data[\"full_name\"] = heatmap_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "heatmap_data[\"n\"] = heatmap_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "heatmap_data = heatmap_data[flatten([\"group_id\",\"full_name\",\"n\",\"mean_percentile\",\"mean_rank\",methods])]\n",
    "heatmap_data.to_csv(os.path.join(OUTPUT_DIR,\"within_distances.csv\"), index=False)\n",
    "heatmap_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auc\"></a>\n",
    "### Predicting whether two genes belong to a common biochemical pathway\n",
    "The purpose of this section is to see if whether or not two genes share atleast one common pathway can be predicted from the distance scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_dict = {name:df[\"shared\"] for name in methods}\n",
    "y_prob_dict = {name:(1 - df[name].values) for name in methods}\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true, y_prob = y_true_dict[method], y_prob_dict[method]\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({\"auc\":area, \"baseline\":baseline, \"ratio\":auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_shared.png\"),dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"y\"></a>\n",
    "### Are genes in the same biochemical pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "# graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "# i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "# in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "# connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "df = pw.make_undirected(df)\n",
    "\n",
    "# What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "# each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "# recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "k = 10      # The threshold of interest for gene ranks.\n",
    "n = 100     # Number of Monte Carlo simulation iterations to complete.\n",
    "df[list(methods)] = df.groupby(\"from\")[list(methods)].rank()\n",
    "ys = df[df[\"shared\"]==1][list(methods)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "ymax = sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "# Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "# edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "ysims = [sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "for method in methods:\n",
    "    pvalue = len([ysim for ysim in ysims if ysim>=ys[method]])/float(n)\n",
    "    TABLE[method].update({\"y\":ys[method], \"y_max\":ymax, \"y_ratio\":ys[method]/ymax, \"y_pval\":pvalue})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean\"></a>\n",
    "### Predicting biochemical pathway or group membership based on mean vectors\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes asociated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "methods = list(vector_dicts.keys())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for method in methods:\n",
    "    for group in valid_group_ids:\n",
    "        ids = group_id_to_ids[group]\n",
    "        for identifier in valid_ids:\n",
    "            # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "            vectors = np.array([vector_dicts[method][some_id] for some_id in ids if not some_id==identifier])\n",
    "            mean_vector = vectors.mean(axis=0)\n",
    "            this_vector = vector_dicts[method][identifier]\n",
    "            pred_dict[method][identifier][group] = 1-metric_dict[method](mean_vector, this_vector)\n",
    "            true_dict[method][identifier][group] = (identifier in group_id_to_ids[group])*1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true = pd.DataFrame(true_dict[method]).as_matrix().flatten()\n",
    "    y_prob = pd.DataFrame(pred_dict[method]).as_matrix().flatten()\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({\"mean_auc\":area, \"mean_baseline\":baseline, \"mean_ratio\":auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method[:10], baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_mean_classifier.png\"),dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway membership based on mean similarity values\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representationt of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership with KNN classifier\n",
    "This section looks at how well the group(s) or biochemical pathway(s) that a particular gene belongs to can be predicted based on a KNN classifier generated using every other gene. For this section, only the groups or pathways which contain more than one gene, and the genes mapped to those groups or pathways, are of interest. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but the KNN classifier could never predict this because when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be members of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"output\"></a>\n",
    "### Summarizing the results for this notebook\n",
    "Write a large table of results to an output file. Columns are generally metrics and rows are generally methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "columns = flatten([\"Hyperparams\",\"Group\",\"Order\",\"Topic\",\"Data\",results.columns])\n",
    "results[\"Hyperparams\"] = \"\"\n",
    "results[\"Group\"] = \"\"\n",
    "results[\"Order\"] = np.arange(results.shape[0])\n",
    "results[\"Topic\"] = TOPIC\n",
    "results[\"Data\"] = DATA\n",
    "results = results[columns]\n",
    "results.reset_index(inplace=True)\n",
    "results = results.rename({\"index\":\"Method\"}, axis=\"columns\")\n",
    "hyperparam_sep = \":\"\n",
    "results[\"Hyperparams\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[1] if hyperparam_sep in x else \"-\")\n",
    "results[\"Method\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[0])\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"full_table.csv\"), index=False)\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
