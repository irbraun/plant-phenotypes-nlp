{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BioBERT Models Pretrained on PubMed\n",
    "This process involves downloading the BioBERT models which are in the keras format, and converting them to pytorch format and creating a new directory that has the renamed config file and bin file for the pytorch model. Then these models can be loaded using the pytorch_pretrained_bert module with `BertModel.from_pretrained(dir)` where that directory contains the `pytorch_model.bin` and `config.json` files. The following cell iterates through the downloaded directories for each BioBERT model and creates a directory `pytorch_model` inside of each that can be passed to `from_pretrained` method to successfully load these models.\n",
    "\n",
    "* https://github.com/naver/biobert-pretrained (Repository for the pre-trained BioBERT models)\n",
    "\n",
    "* https://github.com/dmis-lab/biobert/issues/2 (Python code for converting to pytorch models)\n",
    "* https://github.com/huggingface/transformers/issues/457#issuecomment-518403170 (How the directory should be organized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
    "import tensorflow as tf\n",
    "\n",
    "def convert_biobert_model_to_pytorch(path,name):\n",
    "    tf_path = os.path.join(path,\"{}.ckpt\".format(name))\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    excluded = ['BERTAdam','_power','global_step']\n",
    "    init_vars = list(filter(lambda x:all([True if e not in x[0] else False for e in excluded]),init_vars))\n",
    "    if (not os.path.exists(os.path.join(\"pytorch_model.bin\"))):\n",
    "        convert_tf_checkpoint_to_pytorch(\n",
    "            os.path.join(path,\"{}.ckpt\".format(name)),\n",
    "            os.path.join(path,\"bert_config.json\"),\n",
    "            os.path.join(path,\"pytorch_model.bin\"))\n",
    "    \n",
    "    pytorch_model_dir = os.path.join(path,\"pytorch_model\")\n",
    "    if not os.path.exists(pytorch_model_dir):\n",
    "        os.makedirs(pytorch_model_dir)\n",
    "        os.system(\"cp {} {}\".format(os.path.join(path,\"bert_config.json\"), os.path.join(pytorch_model_dir,\"config.json\")))\n",
    "        os.system(\"mv {} {}\".format(os.path.join(path,\"pytorch_model.bin\"), os.path.join(pytorch_model_dir,\"pytorch_model.bin\")))\n",
    "        os.system(\"cp {} {}\".format(os.path.join(path,\"vocab.txt\"), os.path.join(pytorch_model_dir,\"vocab.txt\")))\n",
    "    \n",
    "convert_biobert_model_to_pytorch(\"../gensim/biobert_v1.0_pmc\",\"biobert_model\")\n",
    "convert_biobert_model_to_pytorch(\"../gensim/biobert_v1.0_pubmed\",\"biobert_model\")\n",
    "convert_biobert_model_to_pytorch(\"../gensim/biobert_v1.0_pubmed_pmc\",\"biobert_model\")\n",
    "convert_biobert_model_to_pytorch(\"../gensim/biobert_v1.1_pubmed\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working through examples for obtaining word and sentence level embeddings from BERT models\n",
    "Notebook for how approaches to generating word embeddings and sentence embeddings with BERT pretrained models.\n",
    "Token embeddings with BERT are generated based on context, so the embeddings are unique to the sentence in which\n",
    "the word is occuring. These models should also be fine-tuned on domain sentences if possible in order to more \n",
    "accurately model words that weren't frequent in the general corpus. Code for the first section is from:\n",
    "\n",
    "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#33-creating-word-and-sentence-vectors-from-hidden-states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text1 = \"white embryo and seedling (albino), lethal hypocotyl\"\n",
    "text2 = \"Increased abundance of miRNA precursors.\"\n",
    "text3 = \"incomplete penetrance; increased aluminum resistance; accumulates lower levels of Al in the root tips.\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "marked_text = \"{} {} {}\".format(\"[CLS]\",text1,\"[SEP]\")\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# All 1's if just one sentence, otherwise 0's then 1's to indicate the two sentences.\n",
    "segments_ids = [1] * len(tokenized_text) \n",
    "\n",
    "print(tokenized_text)\n",
    "print(indexed_tokens) \n",
    "print(segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the token and segment lists into tensors.\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe the pretained BERT model.\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe a model that's been saved locally at a given path.\n",
    "model = BertModel.from_pretrained(\"/Users/irbraun/phenologs-with-oats/gensim/biobert_v1.0_pubmed/pytorch_model\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a forward pass (don't need gradients or backpropagation) for the tokens in this sentence.\n",
    "with torch.no_grad():\n",
    "    encoded_layers,_ = model(tokens_tensor,segments_tensor)\n",
    "\n",
    "# The dimensions of the encoded layers are [Layer, Batch, Token, Hidden Unit].\n",
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "print (\"Number of batches:\", len(encoded_layers[0]))\n",
    "print (\"Number of tokens:\", len(encoded_layers[0][0]))\n",
    "print (\"Number of hidden units:\", len(encoded_layers[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to rearrange the encoded layers nested list so that the dimensions are [Batch, Token, Layer, Hidden Unit].\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "print(token_embeddings.size())\n",
    "token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which layers to use as embeddings is a modeling choice that is context dependent, this uses the last four, either\n",
    "# concatenating all the layers together or summing them.\n",
    "token_vecs_cat = []\n",
    "token_vecs_sum = []\n",
    "batch = 0\n",
    "for token in token_embeddings[batch]:\n",
    "    concatenated_layer_vectors = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vecs_cat.append(concatenated_layer_vectors)\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "# The token_vecs_cat list now has dimensions [Tokens, Length of 4 Layers] for this one input sentence. \n",
    "# The token_vecs_sum list now has dimensions [Tokens, Length of 1 Layer] for this one input sentence.\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a vector to represent the entire sentence.\n",
    "layer_to_use = 10\n",
    "batch = 0\n",
    "token_vectors = encoded_layers[layer_to_use][batch]\n",
    "sentence_embedding = torch.mean(token_vectors,dim=0)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining some of the previous steps into a single function for generating sentence embeddings.\n",
    "def embed(text, model):\n",
    "    marked_text = \"{} {} {}\".format(\"[CLS]\",text,\"[SEP]\")\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    token_vecs_cat = []\n",
    "    token_vecs_sum = []\n",
    "    batch = 0\n",
    "    for token in token_embeddings[batch]:\n",
    "        concatenated_layer_vectors = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(np.array(concatenated_layer_vectors))\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(np.array(sum_vec))\n",
    " \n",
    "    a = np.array(token_vecs_sum)\n",
    "    return(np.mean(a,axis=0))\n",
    "        \n",
    "# Running the function on 1000 sentences to check the runtime.\n",
    "texts = [text1]*1000      \n",
    "vectors = [embed(text, model) for text in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
