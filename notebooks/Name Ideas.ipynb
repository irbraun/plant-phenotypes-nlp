{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading in the file and extracting the name ideas.\n",
    "df = pd.read_csv(\"~/Downloads/SurveyResults - CombResults.csv\", skiprows=[1,2,3])\n",
    "df = df[[\"Q20_5_TEXT\",\"Q20_6_TEXT\"]]\n",
    "name_ideas = flatten([df[df['Q20_5_TEXT'].notnull()]['Q20_5_TEXT'].values, df[df['Q20_6_TEXT'].notnull()]['Q20_6_TEXT'].values])\n",
    "name_ideas = [name.lower() for name in name_ideas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datanect',\n",
       " 'data on demand',\n",
       " 'data connection',\n",
       " 'serviceprovider',\n",
       " 'data crunchers',\n",
       " 'science link',\n",
       " 'do it',\n",
       " 'datapro',\n",
       " 'data wrangle connector']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first few name ideas.\n",
    "name_ideas[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total number of name ideas given.\n",
    "len(name_ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 64),\n",
       " ('science', 15),\n",
       " ('scientist', 14),\n",
       " ('connect', 12),\n",
       " ('datapro', 11),\n",
       " ('expert', 11),\n",
       " ('pro', 9),\n",
       " ('link', 8),\n",
       " ('experts', 8),\n",
       " ('citizen', 7),\n",
       " ('research', 6),\n",
       " ('analyst', 5),\n",
       " ('finder', 5),\n",
       " ('connection', 4),\n",
       " ('connector', 4),\n",
       " ('connections', 4),\n",
       " ('match', 4),\n",
       " ('work', 4),\n",
       " ('na', 4),\n",
       " ('a', 4),\n",
       " ('on', 3),\n",
       " ('it', 3),\n",
       " ('for', 3),\n",
       " ('hire', 3),\n",
       " ('solutions', 3),\n",
       " ('gig', 3),\n",
       " ('.', 3),\n",
       " ('helper', 3),\n",
       " ('analytics', 3),\n",
       " ('analysis', 3),\n",
       " ('r', 2),\n",
       " ('demand', 2),\n",
       " ('do', 2),\n",
       " ('help', 2),\n",
       " ('to', 2),\n",
       " ('citizens', 2),\n",
       " ('connectors', 2),\n",
       " ('dataconnect', 2),\n",
       " ('us', 2),\n",
       " ('dataguru', 2),\n",
       " ('contractor', 2),\n",
       " ('school', 2),\n",
       " ('the', 2),\n",
       " (',', 2),\n",
       " ('cit', 2),\n",
       " ('sci', 2),\n",
       " ('datalink', 2),\n",
       " ('gigs', 2),\n",
       " ('helping', 2),\n",
       " ('by', 2),\n",
       " ('none', 2),\n",
       " ('easy', 2),\n",
       " ('scientists', 2),\n",
       " ('prodata', 2),\n",
       " ('datagig', 2),\n",
       " ('get', 2),\n",
       " ('contract', 2),\n",
       " ('guru', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the words that showed up atleast 2 times, and how many times they showed up.\n",
    "words = flatten([word_tokenize(name) for name in name_ideas])\n",
    "words_and_counts = [(word,count) for (word,count) in Counter(words).most_common() if count>=2]\n",
    "pd.DataFrame(words_and_counts, columns=[\"word\",\"count\"]).to_csv(\"~/Desktop/words.csv\")\n",
    "words_and_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at substrings between 3 and 10 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced list of substrings from 656 down to 173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('data', 112),\n",
       " ('sci', 47),\n",
       " ('con', 44),\n",
       " ('ect', 41),\n",
       " ('pro', 40),\n",
       " ('scien', 38),\n",
       " ('conne', 37),\n",
       " ('nect', 37),\n",
       " ('connect', 36),\n",
       " ('per', 32),\n",
       " ('ert', 24),\n",
       " ('ent', 23),\n",
       " ('expert', 23),\n",
       " ('ist', 23),\n",
       " ('enti', 21),\n",
       " ('scient', 21),\n",
       " ('scienti', 20),\n",
       " ('nce', 19),\n",
       " ('scientist', 19),\n",
       " ('science', 17),\n",
       " ('ink', 16),\n",
       " ('cit', 15),\n",
       " ('gig', 15),\n",
       " ('ion', 15),\n",
       " ('link', 15),\n",
       " ('nal', 15),\n",
       " ('analy', 14),\n",
       " ('tion', 14),\n",
       " ('ear', 12),\n",
       " ('hel', 12),\n",
       " ('citizen', 11),\n",
       " ('datapro', 11),\n",
       " ('esearch', 11),\n",
       " ('help', 11),\n",
       " ('tor', 11),\n",
       " ('analys', 10),\n",
       " ('ctor', 10),\n",
       " ('ecti', 10),\n",
       " ('research', 10),\n",
       " ('connection', 9),\n",
       " ('ons', 9),\n",
       " ('wor', 9),\n",
       " ('atch', 8),\n",
       " ('experts', 8),\n",
       " ('ind', 8),\n",
       " ('work', 8),\n",
       " ('analyst', 7),\n",
       " ('and', 7),\n",
       " ('der', 7),\n",
       " ('ector', 7),\n",
       " ('find', 7),\n",
       " ('ing', 7),\n",
       " ('match', 7),\n",
       " ('tions', 7),\n",
       " ('ali', 6),\n",
       " ('connector', 6),\n",
       " ('ers', 6),\n",
       " ('her', 6),\n",
       " ('ntr', 6),\n",
       " ('tas', 6),\n",
       " ('contract', 5),\n",
       " ('crowd', 5),\n",
       " ('datag', 5),\n",
       " ('finder', 5),\n",
       " ('gigs', 5),\n",
       " ('helper', 5),\n",
       " ('sis', 5),\n",
       " ('sol', 5),\n",
       " ('spec', 5),\n",
       " ('tors', 5),\n",
       " ('cher', 4),\n",
       " ('ctors', 4),\n",
       " ('datalink', 4),\n",
       " ('guru', 4),\n",
       " ('ker', 4),\n",
       " ('man', 4),\n",
       " ('one', 4),\n",
       " ('onnections', 4),\n",
       " ('rke', 4),\n",
       " ('solution', 4),\n",
       " ('tan', 4),\n",
       " ('the', 4),\n",
       " ('tic', 4),\n",
       " ('analysis', 3),\n",
       " ('analytics', 3),\n",
       " ('col', 3),\n",
       " ('contractor', 3),\n",
       " ('datac', 3),\n",
       " ('datagig', 3),\n",
       " ('datama', 3),\n",
       " ('datas', 3),\n",
       " ('demand', 3),\n",
       " ('ectors', 3),\n",
       " ('ens', 3),\n",
       " ('every', 3),\n",
       " ('for', 3),\n",
       " ('gra', 3),\n",
       " ('hand', 3),\n",
       " ('hire', 3),\n",
       " ('hoo', 3),\n",
       " ('ins', 3),\n",
       " ('mar', 3),\n",
       " ('ool', 3),\n",
       " ('rod', 3),\n",
       " ('solutions', 3),\n",
       " ('ssi', 3),\n",
       " ('task', 3),\n",
       " ('tpro', 3),\n",
       " ('ade', 2),\n",
       " ('ance', 2),\n",
       " ('art', 2),\n",
       " ('assist', 2),\n",
       " ('ataconnect', 2),\n",
       " ('ato', 2),\n",
       " ('awork', 2),\n",
       " ('ble', 2),\n",
       " ('cen', 2),\n",
       " ('citizens', 2),\n",
       " ('coll', 2),\n",
       " ('connectors', 2),\n",
       " ('consu', 2),\n",
       " ('crunch', 2),\n",
       " ('data4', 2),\n",
       " ('dataconnec', 2),\n",
       " ('dataf', 2),\n",
       " ('dataguru', 2),\n",
       " ('datamatch', 2),\n",
       " ('datan', 2),\n",
       " ('easy', 2),\n",
       " ('ectpro', 2),\n",
       " ('eni', 2),\n",
       " ('geek', 2),\n",
       " ('get', 2),\n",
       " ('hat', 2),\n",
       " ('helping', 2),\n",
       " ('ice', 2),\n",
       " ('ick', 2),\n",
       " ('ico', 2),\n",
       " ('ide', 2),\n",
       " ('igh', 2),\n",
       " ('ime', 2),\n",
       " ('ine', 2),\n",
       " ('ite', 2),\n",
       " ('ler', 2),\n",
       " ('market', 2),\n",
       " ('master', 2),\n",
       " ('matcher', 2),\n",
       " ('mer', 2),\n",
       " ('need', 2),\n",
       " ('ner', 2),\n",
       " ('nex', 2),\n",
       " ('none', 2),\n",
       " ('now', 2),\n",
       " ('nsi', 2),\n",
       " ('pers', 2),\n",
       " ('proconne', 2),\n",
       " ('prodata', 2),\n",
       " ('prop', 2),\n",
       " ('ram', 2),\n",
       " ('school', 2),\n",
       " ('scientists', 2),\n",
       " ('special', 2),\n",
       " ('stan', 2),\n",
       " ('tech', 2),\n",
       " ('ten', 2),\n",
       " ('thin', 2),\n",
       " ('ting', 2),\n",
       " ('tsci', 2),\n",
       " ('worker', 2),\n",
       " ('works', 2),\n",
       " ('wrangle', 2),\n",
       " ('yda', 2),\n",
       " ('you', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking all the substrings that contain between 3 and 10 characters and show up atleast 2 times.\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", analyzer=\"char\", ngram_range=[3,10], min_df=2)\n",
    "substring_count_matrix = vectorizer.fit_transform(words)\n",
    "substring_count_matrix\n",
    "substrings = vectorizer.get_feature_names()\n",
    "substring_counts =  substring_count_matrix.toarray().sum(axis=0) \n",
    "\n",
    "# List of (substring,count) tuples.\n",
    "substrings_and_counts = [(s,c) for s,c in zip(substrings,substring_counts)]\n",
    "substrings_and_counts = sorted(substrings_and_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Mapping between a substring and it's count.\n",
    "substring_to_count = {s:c for s,c in substrings_and_counts}\n",
    "\n",
    "\n",
    "# Checks if a substring is part of another longer substring that showed up just as many (or more) times.\n",
    "# i.e., get rid of \"ata\" if \"data\" shows up just as many times, etc.\n",
    "def check_validity_of_substring(substring, big_list, to_count):\n",
    "    matches = [other_s for other_s in big_list if (substring in other_s) and (to_count[other_s]>=to_count[substring])]\n",
    "    return len(matches)==1\n",
    "\n",
    "\n",
    "# Use that function to find just the valid substrings.\n",
    "substrings = [s_and_c[0] for s_and_c in substrings_and_counts]\n",
    "valid_substrings = [s for s in substrings if check_validity_of_substring(s, substrings, substring_to_count)]\n",
    "print(\"reduced list of substrings from {} down to {}\".format(len(substrings),len(valid_substrings)))\n",
    "\n",
    "\n",
    "# Subsetting the list of substrings to only include those valid ones.\n",
    "substrings_and_counts = [tup for tup in substrings_and_counts if tup[0] in valid_substrings]\n",
    "pd.DataFrame(substrings_and_counts, columns=[\"substrings\",\"count\"]).to_csv(\"~/Desktop/substrings_3_to_10.csv\")\n",
    "substrings_and_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating for substrings between 5 and 10 characters (doesn't include 'gig' and 'pro' but the longer ones are more visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced list of substrings from 300 down to 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scien', 38),\n",
       " ('conne', 37),\n",
       " ('connect', 36),\n",
       " ('expert', 23),\n",
       " ('scient', 21),\n",
       " ('scienti', 20),\n",
       " ('scientist', 19),\n",
       " ('science', 17),\n",
       " ('analy', 14),\n",
       " ('citizen', 11),\n",
       " ('datapro', 11),\n",
       " ('esearch', 11),\n",
       " ('analys', 10),\n",
       " ('research', 10),\n",
       " ('connection', 9),\n",
       " ('experts', 8),\n",
       " ('analyst', 7),\n",
       " ('ector', 7),\n",
       " ('match', 7),\n",
       " ('tions', 7),\n",
       " ('connector', 6),\n",
       " ('contract', 5),\n",
       " ('crowd', 5),\n",
       " ('datag', 5),\n",
       " ('finder', 5),\n",
       " ('helper', 5),\n",
       " ('ctors', 4),\n",
       " ('datalink', 4),\n",
       " ('onnections', 4),\n",
       " ('solution', 4),\n",
       " ('analysis', 3),\n",
       " ('analytics', 3),\n",
       " ('contractor', 3),\n",
       " ('datac', 3),\n",
       " ('datagig', 3),\n",
       " ('datama', 3),\n",
       " ('datas', 3),\n",
       " ('demand', 3),\n",
       " ('ectors', 3),\n",
       " ('every', 3),\n",
       " ('solutions', 3),\n",
       " ('assist', 2),\n",
       " ('ataconnect', 2),\n",
       " ('awork', 2),\n",
       " ('citizens', 2),\n",
       " ('connectors', 2),\n",
       " ('consu', 2),\n",
       " ('crunch', 2),\n",
       " ('data4', 2),\n",
       " ('dataconnec', 2),\n",
       " ('dataf', 2),\n",
       " ('dataguru', 2),\n",
       " ('datamatch', 2),\n",
       " ('datan', 2),\n",
       " ('ectpro', 2),\n",
       " ('helping', 2),\n",
       " ('market', 2),\n",
       " ('master', 2),\n",
       " ('matcher', 2),\n",
       " ('proconne', 2),\n",
       " ('prodata', 2),\n",
       " ('school', 2),\n",
       " ('scientists', 2),\n",
       " ('special', 2),\n",
       " ('worker', 2),\n",
       " ('works', 2),\n",
       " ('wrangle', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking all the substrings that contain between 5 and 10 characters and show up atleast 2 times.\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", analyzer=\"char\", ngram_range=[5,10], min_df=2)\n",
    "substring_count_matrix = vectorizer.fit_transform(words)\n",
    "substring_count_matrix\n",
    "substrings = vectorizer.get_feature_names()\n",
    "substring_counts =  substring_count_matrix.toarray().sum(axis=0) \n",
    "\n",
    "# List of (substring,count) tuples.\n",
    "substrings_and_counts = [(s,c) for s,c in zip(substrings,substring_counts)]\n",
    "substrings_and_counts = sorted(substrings_and_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Mapping between a substring and it's count.\n",
    "substring_to_count = {s:c for s,c in substrings_and_counts}\n",
    "\n",
    "\n",
    "# Checks if a substring is part of another longer substring that showed up just as many (or more) times.\n",
    "# i.e., get rid of \"ata\" if \"data\" shows up just as many times, etc.\n",
    "def check_validity_of_substring(substring, big_list, to_count):\n",
    "    matches = [other_s for other_s in big_list if (substring in other_s) and (to_count[other_s]>=to_count[substring])]\n",
    "    return len(matches)==1\n",
    "\n",
    "\n",
    "# Use that function to find just the valid substrings.\n",
    "substrings = [s_and_c[0] for s_and_c in substrings_and_counts]\n",
    "valid_substrings = [s for s in substrings if check_validity_of_substring(s, substrings, substring_to_count)]\n",
    "print(\"reduced list of substrings from {} down to {}\".format(len(substrings),len(valid_substrings)))\n",
    "\n",
    "\n",
    "# Subsetting the list of substrings to only include those valid ones.\n",
    "substrings_and_counts = [tup for tup in substrings_and_counts if tup[0] in valid_substrings]\n",
    "pd.DataFrame(substrings_and_counts, columns=[\"substrings\",\"count\"]).to_csv(\"~/Desktop/substrings_5_to_10.csv\")\n",
    "substrings_and_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
