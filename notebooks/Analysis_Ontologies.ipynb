{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can machine learning approaches learn relationships between concepts that are in ontologies?\n",
    "If the neural network document encoding models (Doc2Vec) are being successfully trained, then they should be able to recapture some of the domain-specific information that is written into relationships present in biological ontologies. Specifically, two concepts which have a parent-child relationship in PATO or PO can be considered to be highly similar in this context. We compare the distances between the labels for these pairs of terms as inferred by both the general Doc2Vec model trained on the English Wikipedia corpus, as well as our own models trained specifically on abstracts from PubMed that are specific to plant phenotypes. Here we generate figures to compare the results for a specific set of handpicked phrase or term pairs, as well as a second figure over all pairs parsed from the hierarchies in each ontology to check whether the result generalizes to the ontologies as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pronto\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.datasets.edges import Edges\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.graphs import pairwise as pw\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.graphs.weighting import train_logistic_regression_model, apply_logistic_regression_model\n",
    "from oats.graphs.weighting import train_random_forest_model, apply_random_forest_model\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, build_vocabulary_from_tokens\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "from oats.nlp.preprocess import concatenate_with_bar_delim\n",
    "\n",
    "from _nb_utils import Method\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset of sibling and parent-child term label pairs\n",
    "The purpose of this cell is to create a dataframe that holds all of the pairs of term labels that represent either sibling (the two terms have the same parent term) or parent-child relationships in the PATO and PO ontologies. In other words, this is a dataset of all the pairs of term labels in those ontologies where the terms are connected by a single edge in the graph. These pairs are expected to be terms which are very similar and have a close semantic relationship in general. Therefore, this dataset can be used to evaluate which methods for measuring similarity between short text phrases are most useful for capturing the relationships that are encoded in these ontologies. We want to know if the distances found for these pairs are smaller than the general background distribution of distances between all the possible pairings of labels in these ontologies, or some other background distribution such as all of the phenotype descriptions in a dataset of descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ontology</th>\n",
       "      <th>relationship</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>mobility</td>\n",
       "      <td>physical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>speed</td>\n",
       "      <td>movement quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>age</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color</td>\n",
       "      <td>optical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color hue</td>\n",
       "      <td>chromatic property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color brightness</td>\n",
       "      <td>optical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color saturation</td>\n",
       "      <td>chromatic property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>fluorescence</td>\n",
       "      <td>luminous flux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color pattern</td>\n",
       "      <td>spatial pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>compatibility</td>\n",
       "      <td>behavioral quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ontology  relationship           label_1             label_2\n",
       "0     PATO  parent_child          mobility    physical quality\n",
       "1     PATO  parent_child             speed    movement quality\n",
       "2     PATO  parent_child               age                time\n",
       "3     PATO  parent_child             color     optical quality\n",
       "4     PATO  parent_child         color hue  chromatic property\n",
       "5     PATO  parent_child  color brightness     optical quality\n",
       "6     PATO  parent_child  color saturation  chromatic property\n",
       "7     PATO  parent_child      fluorescence       luminous flux\n",
       "8     PATO  parent_child     color pattern     spatial pattern\n",
       "9     PATO  parent_child     compatibility  behavioral quality"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary for quickly checking these. The two relationships are mutually exclusive so only need one dict.\n",
    "relationship_dict = defaultdict(dict)\n",
    "ontologies = {\"PATO\":pronto.Ontology(\"../ontologies/pato.obo\"), \"PO\":pronto.Ontology(\"../ontologies/po.obo\")}\n",
    "tuples = []\n",
    "for ont_name,ont in ontologies.items():\n",
    "    delim = \"[DELIM]\"\n",
    "    sibling_pairs = set()\n",
    "    for term in ont:\n",
    "        for parent in term.parents.id:\n",
    "            tuples.append((ont_name,\"parent_child\",term.name,ont[parent].name))   \n",
    "            relationship_dict[term.id][ont[parent].id] = \"parent_child\"\n",
    "            relationship_dict[ont[parent].id][term.id] = \"parent_child\"\n",
    "        sorted_id_pairs = [sorted(pair) for pair in list(itertools.combinations(term.children.id, 2))]\n",
    "        for sorted_id_pair in sorted_id_pairs:\n",
    "            relationship_dict[sorted_id_pair[0]][sorted_id_pair[1]] = \"sibling\"\n",
    "            relationship_dict[sorted_id_pair[1]][sorted_id_pair[0]] = \"sibling\"\n",
    "        sorted_pairs = [\"{}{}{}\".format(ont[pair[0]].name, delim, ont[pair[1]].name) for pair in sorted_id_pairs]\n",
    "        sibling_pairs.update(sorted_pairs)\n",
    "    for pair in list(sibling_pairs):\n",
    "        pair = pair.split(delim)\n",
    "        tuples.append((ont_name,\"sibling\",pair[0],pair[1]))        \n",
    "pairs_df = pd.DataFrame(tuples, columns=[\"ontology\",\"relationship\",\"label_1\",\"label_2\"])\n",
    "pairs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset of all possible label pairs and their Jaccard distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ontology</th>\n",
       "      <th>term_1</th>\n",
       "      <th>term_2</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>quality</td>\n",
       "      <td>quality</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000004</td>\n",
       "      <td>quality</td>\n",
       "      <td>mobility</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000008</td>\n",
       "      <td>quality</td>\n",
       "      <td>speed</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000011</td>\n",
       "      <td>quality</td>\n",
       "      <td>age</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000014</td>\n",
       "      <td>quality</td>\n",
       "      <td>color</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000015</td>\n",
       "      <td>quality</td>\n",
       "      <td>color hue</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000016</td>\n",
       "      <td>quality</td>\n",
       "      <td>color brightness</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000017</td>\n",
       "      <td>quality</td>\n",
       "      <td>color saturation</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000018</td>\n",
       "      <td>quality</td>\n",
       "      <td>fluorescence</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PATO</td>\n",
       "      <td>PATO:0000001</td>\n",
       "      <td>PATO:0000019</td>\n",
       "      <td>quality</td>\n",
       "      <td>color pattern</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ontology        term_1        term_2  label_1           label_2  distance\n",
       "0     PATO  PATO:0000001  PATO:0000001  quality           quality  0.000000\n",
       "1     PATO  PATO:0000001  PATO:0000004  quality          mobility  0.800000\n",
       "2     PATO  PATO:0000001  PATO:0000008  quality             speed  0.833333\n",
       "3     PATO  PATO:0000001  PATO:0000011  quality               age  0.833333\n",
       "4     PATO  PATO:0000001  PATO:0000014  quality             color  0.875000\n",
       "5     PATO  PATO:0000001  PATO:0000015  quality         color hue  0.888889\n",
       "6     PATO  PATO:0000001  PATO:0000016  quality  color brightness  0.875000\n",
       "7     PATO  PATO:0000001  PATO:0000017  quality  color saturation  0.888889\n",
       "8     PATO  PATO:0000001  PATO:0000018  quality      fluorescence  0.888889\n",
       "9     PATO  PATO:0000001  PATO:0000019  quality     color pattern  0.857143"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a dataset of all possible label pairs and their Jaccard distance between the label text.\n",
    "ontologies = {\"PATO\":pronto.Ontology(\"../ontologies/pato.obo\"), \"PO\":pronto.Ontology(\"../ontologies/po.obo\")}\n",
    "ontology = Ontology(\"../ontologies/mo.obo\")\n",
    "edgelists = []\n",
    "for ont_name,ont in ontologies.items():\n",
    "    annotations = {}\n",
    "    id_to_term_label = {}\n",
    "    id_to_term_id = {}\n",
    "    for i,term in enumerate(ont):\n",
    "        if not \"obsolete\" in term.name:\n",
    "            id_to_term_label[i] = term.name\n",
    "            id_to_term_id[i] = term.id\n",
    "            annotations[i] = [term.id]\n",
    "    edgelist = pw.pairwise_square_annotations(annotations, ontology, \"jaccard\").edgelist\n",
    "    edgelist[\"term_1\"] = edgelist[\"from\"].map(lambda x: id_to_term_id[x]) \n",
    "    edgelist[\"term_2\"] = edgelist[\"to\"].map(lambda x: id_to_term_id[x])\n",
    "    edgelist[\"label_1\"] = edgelist[\"from\"].map(lambda x: id_to_term_label[x]) \n",
    "    edgelist[\"label_2\"] = edgelist[\"to\"].map(lambda x: id_to_term_label[x])\n",
    "    edgelist[\"ontology\"] = ont_name\n",
    "    edgelist.rename(columns={\"value\":\"distance\"}, inplace=True)\n",
    "    edgelist = edgelist[[\"ontology\",\"term_1\",\"term_2\",\"label_1\",\"label_2\",\"distance\"]]\n",
    "    edgelists.append(edgelist)\n",
    "df = pd.concat(edgelists, ignore_index=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating all methods for recapturing relationships encoded in the ontologies\n",
    "This section is similar to the main analysis notebook that generates the pairwise distance matrices for all of the different NLP methods, and many of those cells have been copied and pasted here. The main difference is that the descriptions dataset is actually term labels from the ontologies, so the Jaccard similarity between each of possible pair of terms is treated as ground truth in order to evaluate how well these relationships are captured by each of the methods. This section uses only one ontology at a time because we are only interested in the pairs of terms that come from the same ontology and therefore have a meaningful distance measure between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionaries in the shape expected for running all the NLP methods for just one particular ontology.\n",
    "ont_name = \"PO\"\n",
    "ont = pronto.Ontology(\"../ontologies/po.obo\")\n",
    "ontology = Ontology(\"../ontologies/po.obo\")\n",
    "annotations = {}\n",
    "descriptions = {}\n",
    "id_to_term_id = {}\n",
    "for i,term in enumerate(ont):\n",
    "    if not \"obsolete\" in term.name:\n",
    "        descriptions[i] = term.name\n",
    "        annotations[i] = [term.id]\n",
    "        id_to_term_id[i] = term.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections borrowed from the main analysis notebook\n",
    "If the dictionary between IDs and term labels is stored as'descriptions', then the cells from the main analysis notebook can be borrowed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summarizing output dictionary has the shape TABLE[method][metric] --> value.\n",
    "TOPIC = \"Ontology Stuff\"\n",
    "DATA = \"PO\"\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# Paths\n",
    "dataset_filename = \"../data/pickles/text_plus_annotations_dataset.pickle\"        # The full dataset pickle.\n",
    "groupings_filename = \"../data/pickles/lloyd_subsets.pickle\"                      # The groupings pickle.\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"       # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_small.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/pubmed_dbow/doc2vec_2.bin\"                  # File holding saved Doc2Vec model.\n",
    "doc2vec_wikipedia_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                 # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                       # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                       # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                            # Jar for NOBLE Coder tool.\n",
    "biobert_pmc_path = \"../gensim/biobert_v1.0_pmc/pytorch_model\"                    # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_path = \"../gensim/biobert_v1.0_pubmed/pytorch_model\"              # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_pmc_path = \"../gensim/biobert_v1.0_pubmed_pmc/pytorch_model\"      # Path for PyTorch BioBERT model.\n",
    "\n",
    "# Files and models related to the machine learning text embedding methods.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "#bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "#bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "#bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "#bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "#bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "#bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)\n",
    "\n",
    "# Preprocessing of the text descriptions. Different methods are necessary for different approaches.\n",
    "descriptions_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "descriptions_no_stopwords = {i:remove_stopwords(d) for i,d in descriptions.items()}\n",
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "descriptions_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "descriptions_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "descriptions_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_adj_only.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of different methods for calculating distance between text descriptions using the Methods object \n",
    "# defined in the utilities for this notebook. The constructor takes a string for the method name, a string defining\n",
    "# the hyperparameter choices for that method, a function to be called to run this method, a dictionary of arguments\n",
    "# by keyword that should be passed to that function, and a distance metric from scipy.spatial.distance to associate\n",
    "# with this method.\n",
    "\n",
    "methods = [\n",
    "    \n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    Method(\"Doc2Vec Wikipedia\", \"Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Doc2Vec PubMed\", \"Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    \n",
    "    #(\"BERT Base:Layers=2,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #(\"BERT Base:Layers=3,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #(\"BERT Base:Layers=4,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #(\"BERT Base:Layers=2,Summed\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #(\"BERT Base:Layers=3,Summed\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #(\"BERT Base:Layers=4,Summed\", pw.pairwise_bert_onegroup, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #(\"BioBERT:PMC,Layers=2,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #(\"BioBERT:PMC,Layers=3,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #(\"BioBERT:PMC,Layers=4,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #(\"BioBERT:PubMed,Layers=4,Concatenated\", pw.pairwise_bert_onegroup, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "        \n",
    "    # Methods that use variations on the n-grams approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":100000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    \n",
    "    Method(\"Jaccard\", \"\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Wikipedia:Size=300                         00:00:02\n",
      "Doc2Vec PubMed:Size=100                            00:00:01\n",
      "Word2Vec Wikipedia:Size=300,Mean                   00:00:03\n",
      "Word2Vec Wikipedia:Size=300,Max                    00:00:03\n",
      "N-Grams:Full,Words,1-grams,2-grams                 00:00:05\n",
      "Jaccard:                                           00:00:01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Doc2Vec Wikipedia:Size=300</th>\n",
       "      <th>Doc2Vec PubMed:Size=100</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Mean</th>\n",
       "      <th>Word2Vec Wikipedia:Size=300,Max</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,2-grams</th>\n",
       "      <th>Jaccard:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1624489</th>\n",
       "      <td>1812</td>\n",
       "      <td>1813</td>\n",
       "      <td>1.050040</td>\n",
       "      <td>1.087368</td>\n",
       "      <td>0.535770</td>\n",
       "      <td>0.878026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624490</th>\n",
       "      <td>1812</td>\n",
       "      <td>1814</td>\n",
       "      <td>1.043532</td>\n",
       "      <td>1.052812</td>\n",
       "      <td>0.567360</td>\n",
       "      <td>0.867917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624491</th>\n",
       "      <td>1812</td>\n",
       "      <td>1815</td>\n",
       "      <td>1.008526</td>\n",
       "      <td>1.225449</td>\n",
       "      <td>0.633470</td>\n",
       "      <td>0.918647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624492</th>\n",
       "      <td>1812</td>\n",
       "      <td>1816</td>\n",
       "      <td>0.995252</td>\n",
       "      <td>1.014143</td>\n",
       "      <td>0.858569</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624494</th>\n",
       "      <td>1813</td>\n",
       "      <td>1814</td>\n",
       "      <td>0.150210</td>\n",
       "      <td>0.569704</td>\n",
       "      <td>0.192139</td>\n",
       "      <td>0.184564</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624495</th>\n",
       "      <td>1813</td>\n",
       "      <td>1815</td>\n",
       "      <td>0.265966</td>\n",
       "      <td>0.569928</td>\n",
       "      <td>0.367940</td>\n",
       "      <td>0.276213</td>\n",
       "      <td>0.825922</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624496</th>\n",
       "      <td>1813</td>\n",
       "      <td>1816</td>\n",
       "      <td>0.934372</td>\n",
       "      <td>1.108542</td>\n",
       "      <td>0.868246</td>\n",
       "      <td>0.852361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624498</th>\n",
       "      <td>1814</td>\n",
       "      <td>1815</td>\n",
       "      <td>0.188479</td>\n",
       "      <td>0.717914</td>\n",
       "      <td>0.226844</td>\n",
       "      <td>0.196430</td>\n",
       "      <td>0.477767</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624499</th>\n",
       "      <td>1814</td>\n",
       "      <td>1816</td>\n",
       "      <td>0.935248</td>\n",
       "      <td>0.950913</td>\n",
       "      <td>0.866944</td>\n",
       "      <td>0.877450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624501</th>\n",
       "      <td>1815</td>\n",
       "      <td>1816</td>\n",
       "      <td>0.978292</td>\n",
       "      <td>0.947450</td>\n",
       "      <td>0.822466</td>\n",
       "      <td>0.856101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         from    to  Doc2Vec Wikipedia:Size=300  Doc2Vec PubMed:Size=100  Word2Vec Wikipedia:Size=300,Mean  Word2Vec Wikipedia:Size=300,Max  N-Grams:Full,Words,1-grams,2-grams  Jaccard:\n",
       "1624489  1812  1813                    1.050040                 1.087368                          0.535770                         0.878026                            1.000000  0.625000\n",
       "1624490  1812  1814                    1.043532                 1.052812                          0.567360                         0.867917                            1.000000  0.500000\n",
       "1624491  1812  1815                    1.008526                 1.225449                          0.633470                         0.918647                            1.000000  0.571429\n",
       "1624492  1812  1816                    0.995252                 1.014143                          0.858569                         0.999921                            0.000000  1.000000\n",
       "1624494  1813  1814                    0.150210                 0.569704                          0.192139                         0.184564                            0.666667  0.250000\n",
       "1624495  1813  1815                    0.265966                 0.569928                          0.367940                         0.276213                            0.825922  0.125000\n",
       "1624496  1813  1816                    0.934372                 1.108542                          0.868246                         0.852361                            0.000000  1.000000\n",
       "1624498  1814  1815                    0.188479                 0.717914                          0.226844                         0.196430                            0.477767  0.142857\n",
       "1624499  1814  1816                    0.935248                 0.950913                          0.866944                         0.877450                            0.000000  1.000000\n",
       "1624501  1815  1816                    0.978292                 0.947450                          0.822466                         0.856101                            0.000000  1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate all the pairwise distance matrices (not in parallel).\n",
    "graphs = {}\n",
    "names = []\n",
    "durations = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    names.append(method.name_with_hyperparameters)\n",
    "    durations.append(to_hms(duration))\n",
    "    print(\"{:50} {}\".format(method.name_with_hyperparameters,to_hms(duration)))\n",
    "durations_df = pd.DataFrame({\"method\":names,\"duration\":durations})\n",
    "durations_df.to_csv(os.path.join(OUTPUT_DIR,\"durations.csv\"), index=False)\n",
    "\n",
    "# Merging all of the edgelist dataframes together.\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for tup in methods}\n",
    "methods = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "df = pw.merge_edgelists(edgelists, default_value=0.000)\n",
    "df = pw.remove_self_loops(df)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman rank-order correlation coefficient and p-value for each method\n",
    "The purpose of this section is to see how well each methods generated distance values between labels for all of the term pairs in the ontology correspond to the distance values generated by just looking just at the Jaccard distance between the terms themselves, ignoring the labels and just accounting directly for the specified ontology hierarchical graph. Spearman's œÅ is used evalute the correlation between these distributions of distance values, and the results are output to a table. The distributions are also subset to include only the pairs where the labels do not have one more words in common, and the correlation coefficient is recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "df_no_shared_words = df[df[\"N-Grams:Full,Words,1-grams,2-grams\"]==1]\n",
    "for method in methods:\n",
    "    sp = spearmanr(df[\"Jaccard:\"].values, df[method].values)\n",
    "    TABLE[method].update({\"rho\":sp.correlation,\"p\":sp.pvalue})\n",
    "    sp_no_shared = spearmanr(df_no_shared_words[\"Jaccard:\"].values, df_no_shared_words[method].values)\n",
    "    TABLE[method].update({\"rho_unshared\":sp_no_shared.correlation,\"p_unshared\":sp_no_shared.pvalue})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at distance distributions for specific term relationships\n",
    "The purpose of this section is to use the specific relationships between either sibling terms or parent-child term pairs and their labels in order to see how each method compares in capturing the relationships between these closely related terms. The distance values found by each method are converted to percentiles so that the distributions of scores between methods will be comparable, and then the dataframe is subset to only include the edges between term pairs that are siblings or parent-child pairs, and then the dataframe is written to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the distance values to all percentiles.\n",
    "df[methods] = df[methods].rank(pct=True)\n",
    "# Use the relationships dictionary saved above to find edges that correspond to specific relationships.\n",
    "df[\"term_1\"] = df[\"from\"].map(lambda x: id_to_term_id[x])\n",
    "df[\"term_2\"] = df[\"to\"].map(lambda x: id_to_term_id[x])\n",
    "df[\"relationship\"] = np.vectorize(lambda t1,t2: relationship_dict[t1].get(t2))(df[\"term_1\"], df[\"term_2\"])\n",
    "df = df[(df[\"relationship\"]==\"sibling\") | (df[\"relationship\"]==\"parent_child\")]\n",
    "df = df[flatten([\"from\",\"to\",\"relationship\",methods])]\n",
    "for method in methods:\n",
    "    TABLE[method].update({\"sib_mean\":df[df[\"relationship\"]==\"sibling\"][method].mean()})\n",
    "    TABLE[method].update({\"sib_median\":df[df[\"relationship\"]==\"sibling\"][method].median()})\n",
    "    TABLE[method].update({\"par_mean\":df[df[\"relationship\"]==\"parent_child\"][method].mean()})\n",
    "    TABLE[method].update({\"par_median\":df[df[\"relationship\"]==\"parent_child\"][method].median()})\n",
    "df.to_csv(os.path.join(OUTPUT_DIR,\"distance_percentiles.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Group</th>\n",
       "      <th>Order</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Data</th>\n",
       "      <th>rho</th>\n",
       "      <th>p</th>\n",
       "      <th>rho_unshared</th>\n",
       "      <th>p_unshared</th>\n",
       "      <th>sib_mean</th>\n",
       "      <th>sib_median</th>\n",
       "      <th>par_mean</th>\n",
       "      <th>par_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doc2Vec Wikipedia</td>\n",
       "      <td>Size=300</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>0.221711</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.195396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383934</td>\n",
       "      <td>0.322938</td>\n",
       "      <td>0.217130</td>\n",
       "      <td>0.012715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc2Vec PubMed</td>\n",
       "      <td>Size=100</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>3.054625e-239</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.511570</td>\n",
       "      <td>0.511206</td>\n",
       "      <td>0.425770</td>\n",
       "      <td>0.379653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>Size=300,Mean</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>0.161944</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.113303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414267</td>\n",
       "      <td>0.379388</td>\n",
       "      <td>0.229295</td>\n",
       "      <td>0.014544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>Size=300,Max</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>0.030972</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.032159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475103</td>\n",
       "      <td>0.515018</td>\n",
       "      <td>0.287947</td>\n",
       "      <td>0.097332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>Full,Words,1-grams,2-grams</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>0.140649</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.359152</td>\n",
       "      <td>0.537906</td>\n",
       "      <td>0.152134</td>\n",
       "      <td>0.009621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jaccard</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Ontology Stuff</td>\n",
       "      <td>PO</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164754</td>\n",
       "      <td>0.077613</td>\n",
       "      <td>0.113833</td>\n",
       "      <td>0.013554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Method                 Hyperparams Group  Order           Topic Data       rho              p  rho_unshared  p_unshared  sib_mean  sib_median  par_mean  par_median\n",
       "0   Doc2Vec Wikipedia                    Size=300            0  Ontology Stuff   PO  0.221711   0.000000e+00      0.195396    0.000000  0.383934    0.322938  0.217130    0.012715\n",
       "1      Doc2Vec PubMed                    Size=100            1  Ontology Stuff   PO  0.025925  3.054625e-239      0.003912    0.000002  0.511570    0.511206  0.425770    0.379653\n",
       "2  Word2Vec Wikipedia               Size=300,Mean            2  Ontology Stuff   PO  0.161944   0.000000e+00      0.113303    0.000000  0.414267    0.379388  0.229295    0.014544\n",
       "3  Word2Vec Wikipedia                Size=300,Max            3  Ontology Stuff   PO  0.030972   0.000000e+00     -0.032159    0.000000  0.475103    0.515018  0.287947    0.097332\n",
       "4             N-Grams  Full,Words,1-grams,2-grams            4  Ontology Stuff   PO  0.140649   0.000000e+00           NaN         NaN  0.359152    0.537906  0.152134    0.009621\n",
       "5             Jaccard                                        5  Ontology Stuff   PO  1.000000   0.000000e+00      1.000000    0.000000  0.164754    0.077613  0.113833    0.013554"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "columns = flatten([\"Hyperparams\",\"Group\",\"Order\",\"Topic\",\"Data\",results.columns])\n",
    "results[\"Hyperparams\"] = \"\"\n",
    "results[\"Group\"] = \"\"\n",
    "results[\"Order\"] = np.arange(results.shape[0])\n",
    "results[\"Topic\"] = TOPIC\n",
    "results[\"Data\"] = DATA\n",
    "results = results[columns]\n",
    "results.reset_index(inplace=True)\n",
    "results = results.rename({\"index\":\"Method\"}, axis=\"columns\")\n",
    "hyperparam_sep = \":\"\n",
    "results[\"Hyperparams\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[1] if hyperparam_sep in x else \"-\")\n",
    "results[\"Method\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[0])\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"full_table.csv\"), index=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
