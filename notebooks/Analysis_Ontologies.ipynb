{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can machine learning approaches learn relationships between concepts that are in ontologies?\n",
    "This notebooks is for evaluating to what extent a set of NLP and machine learning methods are capable of learning the relationships that are encoded in biological ontologies (specifically in this case PO and PATO) by domain experts who create the terms and relationships in those graphs. For example, by specifying that one term is the child of another term in the ontology, this is encoding the knowledge that these terms are closely related (the particular relationship is specified by whatever the edge connecting these terms is) and these two concepts are likely to have significant overlap in their meaning. If a NLP or machine learning approach for evaluating text similarity is useful for that particular domain, it should be able to learn (without accounting for the ontology directly) that the words related to those concepts are indeed similar. Therefore, this notebook uses the labels (names) of the ontology terms as a dataset of texts input into each NLP or machine learning method, and evluates the distance values for pairs of labels against the expected similarity given the ontology structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import pronto\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import spatial, stats\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.graphs import pairwise as pw\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "from oats.nlp.preprocess import concatenate_with_bar_delim\n",
    "from _utils import Method\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a dataset of parent-child and sibling term label pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ontology</th>\n",
       "      <th>relationship</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>mobility</td>\n",
       "      <td>physical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>speed</td>\n",
       "      <td>movement quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>age</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color</td>\n",
       "      <td>optical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color hue</td>\n",
       "      <td>chromatic property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color brightness</td>\n",
       "      <td>optical quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color saturation</td>\n",
       "      <td>chromatic property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>fluorescence</td>\n",
       "      <td>luminous flux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>color pattern</td>\n",
       "      <td>spatial pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PATO</td>\n",
       "      <td>parent_child</td>\n",
       "      <td>compatibility</td>\n",
       "      <td>behavioral quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ontology  relationship           label_1             label_2\n",
       "0     PATO  parent_child          mobility    physical quality\n",
       "1     PATO  parent_child             speed    movement quality\n",
       "2     PATO  parent_child               age                time\n",
       "3     PATO  parent_child             color     optical quality\n",
       "4     PATO  parent_child         color hue  chromatic property\n",
       "5     PATO  parent_child  color brightness     optical quality\n",
       "6     PATO  parent_child  color saturation  chromatic property\n",
       "7     PATO  parent_child      fluorescence       luminous flux\n",
       "8     PATO  parent_child     color pattern     spatial pattern\n",
       "9     PATO  parent_child     compatibility  behavioral quality"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The relationship dictionary is used later to identify the relationship between pairs of terms in the dataset.\n",
    "relationship_dict = defaultdict(dict)\n",
    "ontologies = {\"PATO\":pronto.Ontology(\"../ontologies/pato.obo\"), \"PO\":pronto.Ontology(\"../ontologies/po.obo\")}\n",
    "tuples = []\n",
    "for ont_name,ont in ontologies.items():\n",
    "    delim = \"[DELIM]\"\n",
    "    sibling_pairs = set()\n",
    "    for term in ont:\n",
    "        for parent in term.parents.id:\n",
    "            tuples.append((ont_name,\"parent_child\",term.name,ont[parent].name))   \n",
    "            relationship_dict[term.id][ont[parent].id] = \"parent_child\"\n",
    "            relationship_dict[ont[parent].id][term.id] = \"parent_child\"\n",
    "        sorted_id_pairs = [sorted(pair) for pair in list(itertools.combinations(term.children.id, 2))]\n",
    "        for sorted_id_pair in sorted_id_pairs:\n",
    "            relationship_dict[sorted_id_pair[0]][sorted_id_pair[1]] = \"sibling\"\n",
    "            relationship_dict[sorted_id_pair[1]][sorted_id_pair[0]] = \"sibling\"\n",
    "        sorted_pairs = [\"{}{}{}\".format(ont[pair[0]].name, delim, ont[pair[1]].name) for pair in sorted_id_pairs]\n",
    "        sibling_pairs.update(sorted_pairs)\n",
    "    for pair in list(sibling_pairs):\n",
    "        pair = pair.split(delim)\n",
    "        tuples.append((ont_name,\"sibling\",pair[0],pair[1]))        \n",
    "pairs_df = pd.DataFrame(tuples, columns=[\"ontology\",\"relationship\",\"label_1\",\"label_2\"])\n",
    "pairs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a dataset of all possible label pairs and their Jaccard distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of all possible label pairs and their Jaccard distance based on the ontology structure.\n",
    "ontologies = {\"PATO\":pronto.Ontology(\"../ontologies/pato.obo\"), \"PO\":pronto.Ontology(\"../ontologies/po.obo\")}\n",
    "ontology = Ontology(\"../ontologies/mo.obo\")\n",
    "edgelists = []\n",
    "for ont_name,ont in ontologies.items():\n",
    "    annotations = {}\n",
    "    id_to_term_label = {}\n",
    "    id_to_term_id = {}\n",
    "    for i,term in enumerate(ont):\n",
    "        if not \"obsolete\" in term.name:\n",
    "            id_to_term_label[i] = term.name\n",
    "            id_to_term_id[i] = term.id\n",
    "            annotations[i] = [term.id]\n",
    "    edgelist = pw.pairwise_square_annotations(annotations, ontology, \"jaccard\").edgelist\n",
    "    edgelist[\"term_1\"] = edgelist[\"from\"].map(lambda x: id_to_term_id[x]) \n",
    "    edgelist[\"term_2\"] = edgelist[\"to\"].map(lambda x: id_to_term_id[x])\n",
    "    edgelist[\"label_1\"] = edgelist[\"from\"].map(lambda x: id_to_term_label[x]) \n",
    "    edgelist[\"label_2\"] = edgelist[\"to\"].map(lambda x: id_to_term_label[x])\n",
    "    edgelist[\"ontology\"] = ont_name\n",
    "    edgelist.rename(columns={\"value\":\"distance\"}, inplace=True)\n",
    "    edgelist = edgelist[[\"ontology\",\"term_1\",\"term_2\",\"label_1\",\"label_2\",\"distance\"]]\n",
    "    edgelists.append(edgelist)\n",
    "all_pairs_df = pd.concat(edgelists, ignore_index=True)\n",
    "all_pairs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating all methods for recapturing relationships encoded in the ontologies\n",
    "This section is similar to the main analysis notebook that generates the pairwise distance matrices for all of the different NLP methods, and many of those cells have been copied and pasted here. The main difference is that the dataset of text descriptions is actually term labels from the ontologies, so the Jaccard similarity between each of possible pair of terms is treated as ground truth in order to evaluate how well these relationships are captured by each of the methods. This section uses only one ontology at a time because we are only interested in the pairs of terms that come from the same ontology and therefore have a meaningful distance measure between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionaries in the shape expected for running all the NLP methods for just one particular ontology.\n",
    "ontology_name = \"PATO\"\n",
    "ontology = Ontology(\"../ontologies/pato.obo\")\n",
    "ont = pronto.Ontology(\"../ontologies/pato.obo\")\n",
    "annotations = {}\n",
    "descriptions = {}\n",
    "id_to_term_id = {}\n",
    "for i,term in enumerate(ont):\n",
    "    if not \"obsolete\" in term.name:\n",
    "        descriptions[i] = term.name\n",
    "        annotations[i] = [term.id]\n",
    "        id_to_term_id[i] = term.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections borrowed from the main analysis notebook\n",
    "If the dictionary between IDs and term labels is stored as'descriptions', then the cells from the main analysis notebook can be borrowed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summarizing output dictionary has the shape TABLE[method][metric] --> value.\n",
    "TOPIC = \"ontology structure\"\n",
    "DATA = ontology_name\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# Paths\n",
    "dataset_filename = \"../data/pickles/text_plus_annotations_dataset.pickle\"        # The full dataset pickle.\n",
    "groupings_filename = \"../data/pickles/lloyd_subsets.pickle\"                      # The groupings pickle.\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"       # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_small.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/pubmed_dbow/doc2vec_2.bin\"                  # File holding saved Doc2Vec model.\n",
    "doc2vec_wikipedia_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                 # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                       # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                       # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                            # Jar for NOBLE Coder tool.\n",
    "biobert_pmc_path = \"../gensim/biobert_v1.0_pmc/pytorch_model\"                    # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_path = \"../gensim/biobert_v1.0_pubmed/pytorch_model\"              # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_pmc_path = \"../gensim/biobert_v1.0_pubmed_pmc/pytorch_model\"      # Path for PyTorch BioBERT model.\n",
    "\n",
    "# Files and models related to the machine learning text embedding methods.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)\n",
    "\n",
    "# Preprocessing of the text descriptions. Different methods are necessary for different approaches.\n",
    "descriptions_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "descriptions_no_stopwords = {i:remove_stopwords(d) for i,d in descriptions.items()}\n",
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "descriptions_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "descriptions_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "descriptions_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_adj_only.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of different methods for calculating distance between text descriptions using the Methods object \n",
    "# defined in the utilities for this notebook. The constructor takes a string for the method name, a string defining\n",
    "# the hyperparameter choices for that method, a function to be called to run this method, a dictionary of arguments\n",
    "# by keyword that should be passed to that function, and a distance metric from scipy.spatial.distance to associate\n",
    "# with this method.\n",
    "\n",
    "methods = [\n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    Method(\"Doc2Vec Wikipedia\", \"Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Doc2Vec PubMed\", \"Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec Wikipedia\", \"Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    Method(\"BERT\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "        \n",
    "    # Methods that use variations on the n-grams approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use variations on the n-grams approach with simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use variations on the n-grams approach selecting for specific parts-of-speech.\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    # Methods that use terms inferred from automated annotation of the text.\n",
    "    #Method(\"NOBLE Coder\", \"Precise\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"NOBLE Coder\", \"Partial\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"NOBLE Coder\", \"Precise,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    \n",
    "    Method(\"Jaccard\", \"Default\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairwise distance matrices (not in parallel).\n",
    "graphs = {}\n",
    "names = []\n",
    "durations = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    names.append(method.name_with_hyperparameters)\n",
    "    durations.append(to_hms(duration))\n",
    "    print(\"{:50} {}\".format(method.name_with_hyperparameters,to_hms(duration)))\n",
    "durations_df = pd.DataFrame({\"method\":names,\"duration\":durations})\n",
    "durations_df.to_csv(os.path.join(OUTPUT_DIR,\"{}_durations.csv\".format(ontology_name.lower())), index=False)\n",
    "\n",
    "# Merging all of the edgelist dataframes together.\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for tup in methods}\n",
    "methods = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "df = pw.merge_edgelists(edgelists, default_value=0.000)\n",
    "df = pw.remove_self_loops(df)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Spearman rank-order correlation coefficient and p-value for each method\n",
    "The purpose of this section is to see how well each methods generated distance values between labels for all of the term pairs in the ontology correspond to the distance values generated by just looking just at the Jaccard distance between the terms themselves, ignoring the labels and just accounting directly for the specified ontology hierarchical graph. Spearman's ρ is used evalute the correlation between these distributions of distance values, and the results are output to a table. The distributions are also subset to include only the pairs where the labels do not have one more words in common, and the correlation coefficient is recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shared_words = df[df[\"N-Grams:Full,Words,1-grams,Binary\"]==1]\n",
    "for method in methods:\n",
    "    sp = spearmanr(df[\"Jaccard:Default\"].values, df[method].values)\n",
    "    sp_no_shared = spearmanr(df_no_shared_words[\"Jaccard:Default\"].values, df_no_shared_words[method].values)\n",
    "    TABLE[method].update({\"rho_all\":sp.correlation,\"p_all\":sp.pvalue})\n",
    "    TABLE[method].update({\"rho_unshared\":sp_no_shared.correlation,\"p_unshared\":sp_no_shared.pvalue})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Look at distance distributions for specific term relationships\n",
    "The purpose of this section is to use the specific relationships between either sibling terms or parent-child term pairs and their labels in order to see how each method compares in capturing the relationships between these closely related terms. The distance values found by each method are converted to percentiles so that the distributions of scores between methods will be comparable, and then the dataframe is subset to only include the edges between term pairs that are siblings or parent-child pairs, and then the dataframe is written to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the distance values to all percentiles of the distance values against the background distribution.\n",
    "df[methods] = df[methods].rank(pct=True)\n",
    "\n",
    "# Also generate columns that aggregate among all hyperparameter choices for a method.\n",
    "groups = list(set([method.split(\":\")[0] for method in methods]))\n",
    "methods.extend(groups)\n",
    "for group in groups:\n",
    "    cols = [c for c in df.columns if c.split(\":\")[0]==group]\n",
    "    df[group] = df[cols].mean(axis=1)\n",
    "    \n",
    "# Use the relationships dictionary saved above to find edges that correspond to specific relationships.\n",
    "df[\"term_1\"] = df[\"from\"].map(lambda x: id_to_term_id[x])\n",
    "df[\"term_2\"] = df[\"to\"].map(lambda x: id_to_term_id[x])\n",
    "df[\"relationship\"] = np.vectorize(lambda t1,t2: relationship_dict[t1].get(t2))(df[\"term_1\"], df[\"term_2\"])\n",
    "df[\"ontology\"] = ontology_name\n",
    "df_sub = df[(df[\"relationship\"]==\"sibling\") | (df[\"relationship\"]==\"parent_child\")]\n",
    "\n",
    "#Update the table of results to include statistics about each distances distribution.\n",
    "for method in methods:\n",
    "    TABLE[method].update({\"sibling_mean\":df_sub[df_sub[\"relationship\"]==\"sibling\"][method].mean()})\n",
    "    TABLE[method].update({\"sibling_median\":df_sub[df_sub[\"relationship\"]==\"sibling\"][method].median()})\n",
    "    TABLE[method].update({\"parent_mean\":df_sub[df_sub[\"relationship\"]==\"parent_child\"][method].mean()})\n",
    "    TABLE[method].update({\"parent_median\":df_sub[df_sub[\"relationship\"]==\"parent_child\"][method].median()})\n",
    "df_sub[flatten([\"term_1\",\"term_2\",\"ontology\",\"relationship\",methods])].to_csv(os.path.join(OUTPUT_DIR,\"{}_distance_percentiles_all.csv\".format(ontology_name.lower())), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Saving a smaller table of just the distance percentiles for hand-picked term pairs\n",
    "The purpose of this section is to compare how similar each method considers a set of hand-picked term pairs that are either parent-child pairs sibling pairs. These are read from a file and are selected to try and highlight interesting differences between the different embedding techniques, training methods, or hyperparamter choices for the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv(\"../data/corpus_related_files/ontology_knowledge/phrase_pairs.csv\")\n",
    "term_id_tuples = [(term_id_1,term_id_2) for term_id_1,term_id_2 in zip(pairs[\"Term 1\"].values,pairs[\"Term 2\"].values)]\n",
    "handpicked_dict = defaultdict(dict)\n",
    "for ids in term_id_tuples:\n",
    "    handpicked_dict[ids[0]][ids[1]] = True\n",
    "    handpicked_dict[ids[1]][ids[0]] = True\n",
    "df[\"handpicked\"] = np.vectorize(lambda t1,t2: handpicked_dict[t1].get(t2))(df[\"term_1\"], df[\"term_2\"])\n",
    "df_sub = df[df[\"handpicked\"]==True]\n",
    "df_sub[flatten([\"term_1\",\"term_2\",methods])].to_csv(os.path.join(OUTPUT_DIR,\"{}_distance_percentiles_handpicked.csv\".format(ontology_name.lower())), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the results for this notebook\n",
    "Write a large table of results to an output file. Columns are generally metrics and rows are generally methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "columns = flatten([\"Hyperparams\",\"Group\",\"Order\",\"Topic\",\"Data\",results.columns])\n",
    "results[\"Hyperparams\"] = \"\"\n",
    "results[\"Group\"] = \"\"\n",
    "results[\"Order\"] = np.arange(results.shape[0])\n",
    "results[\"Topic\"] = TOPIC\n",
    "results[\"Data\"] = DATA\n",
    "results = results[columns]\n",
    "results.reset_index(inplace=True)\n",
    "results = results.rename({\"index\":\"Method\"}, axis=\"columns\")\n",
    "hyperparam_sep = \":\"\n",
    "results[\"Hyperparams\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[1] if hyperparam_sep in x else \"-\")\n",
    "results[\"Method\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[0])\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"{}_full_table.csv\".format(ontology_name.lower())), index=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
