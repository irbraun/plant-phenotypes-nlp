{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Biochemical Pathways in Phenotypic Description Data\n",
    "The purpose of this notebook is to answer the question of how networks genereated using phenotypic-text similarity based approaches through either embedding, vocabulary presence, or ontology annotation compare to or relate to networks that specify known protein-protein interactions. The hypothesis that these networks are potentially related is based on the idea that if two proteins interact, they are likely to be acting in a common pathway with a common biological function. If the phenotypic outcome of this pathway is observable and documented, then similarites between text describing the mutant phenotype for these genes may coincide with direct protein-protein interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.datasets.string import get_stringdb_information\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import write_annotations_to_tsv_file, read_annotations_from_tsv_file\n",
    "from oats.graphs.pairwise import pairwise_edgelist_doc2vec, pairwise_edgelist_counting, pairwise_edgelist_annotations\n",
    "from oats.graphs.pairwise import merge_edgelists, subset_edgelist_with_ids\n",
    "from oats.graphs.pairwise import remove_self_loops\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested dictionary to summarize output with shape dict[method][(tag,metric)] --> value\n",
    "TAG = \"pathways\"\n",
    "OUTPUT = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading in dataset and subsetting based on membership in biochemical pathways\n",
    "Note that the subsetting is based on if the gene is mapped to atleast one pathway, but not subsetting based on only including genes that map to a pathway that has atleast two genes mapped to it. This is the same strategy taken with the STRING data, where we included genes in the subset data if they were mentioend in STRING at all, they didn't have to be shown to be interacting with another protein in teh dataset. THis allows for NAs when finding the within group mean similarity, because some groups might only have one gene in them, which is also the case here and that's good, just account for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the entire dataset, subsetting for Arabidosis and all annotation types.\n",
    "dataset = load_from_pickle(\"../data/pickles/full_dataset.pickle\")\n",
    "dataset.describe()\n",
    "dataset.filter_by_species(\"ath\")\n",
    "dataset.collapse_by_all_gene_names()\n",
    "dataset.filter_has_description()\n",
    "dataset.filter_has_annotation()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the dataset to include only those genes that map to atleast one group from some classification source.\n",
    "groups = load_from_pickle(path=\"../data/pickles/pmn_pathways.pickle\")\n",
    "id_to_group_ids = groups.get_forward_dict(dataset.get_gene_dictionary())\n",
    "group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>1]    \n",
    "dataset.filter_with_ids(group_mapped_ids)\n",
    "dataset.filter_random_k(400)\n",
    "dataset.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the mappings again now that the data has been subset.\n",
    "id_to_group_ids = groups.get_forward_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_reverse_dict(dataset.get_gene_dictionary())\n",
    "sorted_group_tuples = sorted(group_id_to_ids.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "print(\"{:<20}{:<20}{:<20}\".format(\"Num Genes Mapped\",\"Pathway ID\", \"Pathway Name\"))\n",
    "for group in sorted_group_tuples[:10]:\n",
    "    print(\"{:<20}{:<20}{:<20}\".format(len(group_id_to_ids[group[0]]), group[0], groups.get_long_name(group[0]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating edgelists specifying the pairwise similarities from this dataset\n",
    "This includes things like whether or not to do capitilization removal, lemmatization, stemming, etc. on the descriptions present in the dataset. This could also included things like scrambling the contexts of each description to establish a baseline performance measure. This could also include things like reducing the vocabulary size through the preprocessing methods given here but also through additional means such as provided a reduced (more specialized) vocabulary dictionary to the vectorizing functions so that only those words which are most likely to have meaning have positions with those vectors. Should also test other vectorization methods such as term-frequency inverse-document-frequency for weighting. Can also change how the feature selection is done for those vectors by altering whether the *n*-grams are based on word or characters, and what the range of *n* is.\n",
    "\n",
    "Also add the thing about combining the terms annotations and text when using the bag-of-words approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of descriptions with same quantity of words but randomly selected from the vocabulary.\n",
    "from nltk.tokenize import word_tokenize\n",
    "#descriptions = dataset.get_description_dictionary()\n",
    "#annotations = dataset.get_annotations_dictionary()\n",
    "#go_annotations = {k:[term for term in v if term[0:2]==\"GO\"] for k,v in annotations.items()}\n",
    "#po_annotations = {k:[term for term in v if term[0:2]==\"PO\"] for k,v in annotations.items()}\n",
    "#tokens = [w for w in itertools.chain.from_iterable(word_tokenize(desc) for desc in descriptions.values())]\n",
    "#scrambled_descriptions = {k:\" \".join(np.random.choice(tokens,len(word_tokenize(v)))) for k,v in descriptions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects and dictionaries needed to build the list of edges for the full graph.\n",
    "doc2vec_model_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"\n",
    "doc2vec_model = gensim.models.Doc2Vec.load(doc2vec_model_filename)\n",
    "ontology_filename = \"../ontologies/mo.obo\"\n",
    "ontology = Ontology(ontology_filename)\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "annotations = dataset.get_annotations_dictionary()\n",
    "vocabulary = ontology.get_all_tokens_as_ordered_vocabulary()\n",
    "\n",
    "# Generating the pairwise edgelist for some vanilla methods.\n",
    "name_to_df_mapping = {}\n",
    "name_to_df_mapping[\"doc2vec\"] = pairwise_edgelist_doc2vec(doc2vec_model, descriptions, metric=\"cosine\")\n",
    "name_to_df_mapping[\"bagofwords\"] = pairwise_edgelist_counting(descriptions, binary=False, metric=\"cosine\") \n",
    "name_to_df_mapping[\"setofwords\"] = pairwise_edgelist_counting(descriptions, binary=True, metric=\"cosine\")\n",
    "name_to_df_mapping[\"ontology\"] = pairwise_edgelist_annotations(annotations, ontology, binary=True, metric=\"cosine\")\n",
    "print(\"{} methods completed\".format(len(name_to_df_mapping)))\n",
    "\n",
    "# Generating the pairwise edgelists for some additional methods.\n",
    "name_to_df_mapping[\"bag_w12gram\"] = pairwise_edgelist_counting(descriptions, metric=\"cosine\", binary=False, analyzer=\"word\", ngram_range=(1,2))\n",
    "name_to_df_mapping[\"bag_c36gram\"] = pairwise_edgelist_counting(descriptions, metric=\"cosine\", binary=False, analyzer=\"char\", ngram_range=(3,6))\n",
    "name_to_df_mapping[\"bag_reduced\"] = pairwise_edgelist_counting(descriptions, metric=\"cosine\", binary=False, vocabulary=vocabulary)\n",
    "name_to_df_mapping[\"set_reduced\"] = pairwise_edgelist_counting(descriptions, metric=\"cosine\", binary=True, vocabulary=vocabulary)\n",
    "print(\"{} methods completed\".format(len(name_to_df_mapping)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all of the edgelist dataframes together.\n",
    "methods = list(name_to_df_mapping.keys())\n",
    "df = merge_edgelists(name_to_df_mapping, default_value=0.000)\n",
    "df = remove_self_loops(df)\n",
    "print(df.head(10))\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merging the dataset with information about biochemical pathway membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column that says whether or not the two genes have atleast one pathway in common.\n",
    "df[\"common\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "print(df.head(5))\n",
    "print(Counter(df[\"common\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Do the edges joining genes that share atleast one pathways come from a different distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = name_to_df_mapping.keys()\n",
    "\n",
    "# Approach 1, using the column generated for the dataframe.\n",
    "ppi_pos_dict = {name:(df[df[\"common\"] > 0.00][name].values) for name in METHODS}\n",
    "ppi_neg_dict = {name:(df[df[\"common\"] == 0.00][name].values) for name in METHODS}\n",
    "\n",
    "\n",
    "# Approach 2, using the multi-indexed dataframe graph object.\n",
    "METHODS = name_to_df_mapping.keys()\n",
    "group_id_to_ids = groups.get_reverse_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(list)\n",
    "all_weights_dict = {}\n",
    "for method in METHODS:\n",
    "    all_weights_dict[method] = df[method].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        within_weights_dict[method].extend(graph.get_values(within_pairs, kind=method))\n",
    "\n",
    "\n",
    "# Display the plots, using either the distributions found with approach 1 or 2 above.\n",
    "num_plots, plots_per_row, row_width, row_height = (len(METHODS), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for name,ax in zip(METHODS,axs.flatten()):\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Which individual biochemical pathways are the most 'phenotypically visible' in this dataset?\n",
    "This is a method of sorting the individual pathways by which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the within-group similarity values for each method for each particular pathway.\n",
    "METHODS = name_to_df_mapping.keys()\n",
    "group_id_to_ids = groups.get_reverse_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "all_weights_dict = {}\n",
    "for method in METHODS:\n",
    "    all_weights_dict[method] = df[method].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        within_weights_dict[method][group] = np.mean((graph.get_values(within_pairs, kind=method)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the average rank across methods of each pathway in terms of how low the mean within-group distance values were.\n",
    "ranks = pd.DataFrame(within_weights_dict).rank()\n",
    "ranks[\"average\"] = ranks.mean(axis=1)\n",
    "ranks.sort_values(by=\"average\", inplace=True)\n",
    "ranks.reset_index(inplace=True)\n",
    "ranks[\"group_id\"] = ranks[\"index\"]\n",
    "ranks[\"full_name\"] = ranks[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "ranks[\"n\"] = ranks[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "ranks = ranks[[\"group_id\", \"n\", \"average\",\"full_name\",]]\n",
    "ranks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting whether two genes share or do not share a functional classification or pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the targe class values, 1 indicating common function and 0 indicating no common function.\n",
    "df.loc[:,\"class\"] = [int(len(set(id_to_group_ids[id1]).intersection(set(id_to_group_ids[id2])))>0) \n",
    "    for (id1,id2) in zip(df[\"from\"].values,df[\"to\"].values)]\n",
    "print(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_dict = {name:df[\"class\"] for name in METHODS}\n",
    "y_prob_dict = {name:(1 - df[name].values) for name in METHODS}\n",
    "results = {}\n",
    "\n",
    "num_plots, plots_per_row, row_width, row_height = (len(METHODS), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(METHODS, axs.flatten()):\n",
    "    # Obtaining the values and metrics.\n",
    "    y_true, y_prob = y_true_dict[method], y_prob_dict[method]\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    results[method] = {\"auc\":\"{:.4f}\".format(area), \n",
    "                       \"baseline\":\"{:.4f}\".format(baseline), \n",
    "                       \"n_shared\":\"{:.0f}\".format(n_pos), \n",
    "                       \"n_not\":\"{:.0f}\".format(n_neg)}\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "print(pd.DataFrame(results).transpose())\n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting which functional group or pathway a specific gene belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we learn associations between biochemical pathways and text descriptions?\n",
    "# The non ml ways of doing this would be:\n",
    "#    1 create a representative datapoint for each pathway by first dropping the specific sample then mapping.\n",
    "#    need to implement a function for dropping k samples from the matrix calculation and then classifying them?\n",
    "#    2 just use the mean similarity to other members of that thing.\n",
    "#    3 The thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
