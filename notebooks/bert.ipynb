{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertForMaskedLM, BertConfig\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from ipywidgets import IntProgress\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths to text datasets.\n",
    "plant_abstracts_corpus_path = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\"\n",
    "plant_phenotype_descriptions_path = \"../../plant-data/genes_texts_annots.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346894\n",
      "172374\n",
      "34904\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset that combines the dataset of plant phenotype descriptions and scrapped abstracts.\n",
    "corpus = open(plant_abstracts_corpus_path, 'r').read()\n",
    "sentences_from_corpus = sent_tokenize(corpus)\n",
    "phenotype_descriptions = \" \".join(pd.read_csv(plant_phenotype_descriptions_path)[\"descriptions\"].values)\n",
    "times_to_duplicate_phenotype_dataset = 5\n",
    "sentences_from_descriptions = sent_tokenize(phenotype_descriptions)\n",
    "sentences_from_descriptions_duplicated = list(np.repeat(sentences_from_descriptions, times_to_duplicate_phenotype_dataset))\n",
    "sentences_from_corpus_and_descriptions = sentences_from_corpus+sentences_from_descriptions_duplicated\n",
    "random.shuffle(sentences_from_corpus_and_descriptions)\n",
    "random.shuffle(sentences_from_corpus)\n",
    "random.shuffle(sentences_from_descriptions)\n",
    "sentences_from_corpus_and_descriptions = [preprocess_string(s) for s in sentences_from_corpus_and_descriptions]\n",
    "sentences_from_corpus = [preprocess_string(s) for s in sentences_from_corpus]\n",
    "sentences_from_descriptions = [preprocess_string(s) for s in sentences_from_descriptions]\n",
    "assert len(sentences_from_corpus_and_descriptions) == len(sentences_from_corpus)+(times_to_duplicate_phenotype_dataset*len(sentences_from_descriptions))\n",
    "print(len(sentences_from_corpus_and_descriptions))\n",
    "print(len(sentences_from_corpus))\n",
    "print(len(sentences_from_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genet molecular analys cbp overlap function development pathwai hyl',\n",
       " 'addit meristem gene like sequenc us phylogenet analys bamboo speci',\n",
       " 'advanc backcross progeni resist phenotyp tag marker us accumul blast resist upland rice',\n",
       " 'ran gtpase activ protein rangap import ran signal involv nucleocytoplasm transport spindl organ postmitot nuclear assembl',\n",
       " 'reduc size leaf stage usual small plant']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\" \".join(s) for s in sentences_from_corpus_and_descriptions]\n",
    "#sentences = [\" \".join(s) for s in sentences_from_corpus_and_descriptions[:50]]\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35788\n",
      "['vitl', 'aodelta', 'doq', 'mon', 'supersensit', 'fruitless', 'riesl', 'panorama', 'phapb', 'thermocycl']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Preparing a vocabulary file based on these sentences for BERT.\n",
    "vocabulary_file_path = \"../data/corpus_related_files/vocabulary/vocab.txt\"\n",
    "vocabulary = set()\n",
    "for s in sentences_from_corpus_and_descriptions:\n",
    "    vocabulary.update(s)\n",
    "vocabulary.update([\"[PAD]\",\"[SEP]\",\"[UNK]\",\"[MASK]\"])\n",
    "print(len(vocabulary))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(list(vocabulary)[:10])\n",
    "with open(vocabulary_file_path, \"w\") as f:\n",
    "    for token in list(vocabulary):\n",
    "        f.write(token+\"\\n\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 42 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                   (35788, 50)\n",
      "bert.embeddings.position_embeddings.weight                 (200, 50)\n",
      "bert.embeddings.token_type_embeddings.weight                 (2, 50)\n",
      "bert.embeddings.LayerNorm.weight                               (50,)\n",
      "bert.embeddings.LayerNorm.bias                                 (50,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight            (50, 50)\n",
      "bert.encoder.layer.0.attention.self.query.bias                 (50,)\n",
      "bert.encoder.layer.0.attention.self.key.weight              (50, 50)\n",
      "bert.encoder.layer.0.attention.self.key.bias                   (50,)\n",
      "bert.encoder.layer.0.attention.self.value.weight            (50, 50)\n",
      "bert.encoder.layer.0.attention.self.value.bias                 (50,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight          (50, 50)\n",
      "bert.encoder.layer.0.attention.output.dense.bias               (50,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight         (50,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias           (50,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight             (100, 50)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                  (100,)\n",
      "bert.encoder.layer.0.output.dense.weight                   (50, 100)\n",
      "bert.encoder.layer.0.output.dense.bias                         (50,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                   (50,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                     (50,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "cls.predictions.transform.dense.weight                      (50, 50)\n",
      "cls.predictions.transform.dense.bias                           (50,)\n",
      "cls.predictions.transform.LayerNorm.weight                     (50,)\n",
      "cls.predictions.transform.LayerNorm.bias                       (50,)\n"
     ]
    }
   ],
   "source": [
    "# Creating and parameratizing the small BERT architecture.\n",
    "vocab_size = vocabulary_size\n",
    "small_bert_configuration = BertConfig(\n",
    "    vocab_size=vocab_size, \n",
    "    hidden_size=50, \n",
    "    num_hidden_layers=2, \n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=100,\n",
    "    max_position_embeddings=200,\n",
    "    return_dict=True,   \n",
    ")\n",
    "model = BertForMaskedLM(small_bert_configuration)\n",
    "\n",
    "# An easier to read description of the model, from BERT fine-tuning with PyTorch.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='', vocab_size=35788, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# Creating the tokenizer using the provided vocabulary.\n",
    "tokenizer = BertTokenizer(vocab_file=vocabulary_file_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19100, 26733,  6707, 35559,   370, 16016, 30503, 29005, 12615, 18478,\n",
      "         25404,  7320,  7320,  7320,  7320],\n",
      "        [19100,  7391, 30381,  6617, 32827, 25279, 14731, 17372, 35559,  1675,\n",
      "         10033, 25404,  7320,  7320,  7320],\n",
      "        [19100, 16392,  6437,  3006, 24441, 31378, 27568, 25983, 14731, 17132,\n",
      "         21417, 24441, 29076, 28620, 25404]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([3, 15])\n",
      "torch.Size([3, 15])\n",
      "torch.Size([3, 15])\n"
     ]
    }
   ],
   "source": [
    "# Testing out the tokenizer with the first few sentences from the dataset.\n",
    "encoding = tokenizer(sentences[0:3], return_tensors='pt', padding=True, truncation=True)\n",
    "print(encoding.input_ids)\n",
    "print(encoding.token_type_ids)\n",
    "print(encoding.attention_mask)\n",
    "print(encoding.input_ids.shape)\n",
    "print(encoding.token_type_ids.shape)\n",
    "print(encoding.attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genet molecular analys [MASK] overlap function development pathwai hyl',\n",
       " 'addit [MASK] gene like sequenc us phylogenet analys bamboo speci',\n",
       " 'advanc backcross progeni resist [MASK] tag marker us accumul blast resist upland rice',\n",
       " 'ran gtpase activ [MASK] rangap import ran [MASK] involv nucleocytoplasm transport spindl organ postmitot nuclear assembl',\n",
       " 'reduc size leaf stage usual small plant']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Producing the corresponding dataset of sentences with masked tokens for training.\n",
    "masked = [\" \".join([np.random.choice(['[MASK]',token],p=[0.15,0.85]) for token in s.split()]) for s in sentences]\n",
    "masked[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-f18cc01611b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preparing the dataset object that can be read in as batches during the training loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2212\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2213\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2214\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2215\u001b[0m             )\n\u001b[1;32m   2216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2399\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2400\u001b[0m         )\n\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtok_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mwhitespace_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparing the dataset object that can be read in as batches during the training loop.\n",
    "inputs_dict = tokenizer(masked, return_tensors='pt', padding=True, truncation=True)\n",
    "labels_dict = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "print(inputs_dict[\"input_ids\"].shape)\n",
    "print(labels_dict[\"input_ids\"].shape)\n",
    "dataset = TensorDataset(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"], labels_dict[\"input_ids\"])\n",
    "\n",
    "\n",
    "# Pick the batch size here.\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and parameterizing the necessary objects for the optimizer and learning rate scheduler.\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps=total_steps)\n",
    "print(total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4577, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4520, grad_fn=<NllLossBackward>)\n",
      "10.454821109771729\n",
      "tensor(10.4580, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4543, grad_fn=<NllLossBackward>)\n",
      "10.456151962280273\n",
      "tensor(10.4523, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4578, grad_fn=<NllLossBackward>)\n",
      "10.455024719238281\n",
      "tensor(10.4567, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4540, grad_fn=<NllLossBackward>)\n",
      "10.455362319946289\n",
      "tensor(10.4596, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4545, grad_fn=<NllLossBackward>)\n",
      "10.457032203674316\n",
      "tensor(10.4544, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4598, grad_fn=<NllLossBackward>)\n",
      "10.45707893371582\n",
      "tensor(10.4534, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4582, grad_fn=<NllLossBackward>)\n",
      "10.455801010131836\n",
      "tensor(10.4576, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4599, grad_fn=<NllLossBackward>)\n",
      "10.458718299865723\n",
      "tensor(10.4571, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4578, grad_fn=<NllLossBackward>)\n",
      "10.457447052001953\n",
      "tensor(10.4591, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4555, grad_fn=<NllLossBackward>)\n",
      "10.457303047180176\n",
      "tensor(10.4524, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4553, grad_fn=<NllLossBackward>)\n",
      "10.453835487365723\n",
      "tensor(10.4563, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4565, grad_fn=<NllLossBackward>)\n",
      "10.456398963928223\n",
      "tensor(10.4580, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4492, grad_fn=<NllLossBackward>)\n",
      "10.453604698181152\n",
      "tensor(10.4576, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4507, grad_fn=<NllLossBackward>)\n",
      "10.45416259765625\n",
      "tensor(10.4529, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4626, grad_fn=<NllLossBackward>)\n",
      "10.457756519317627\n",
      "tensor(10.4583, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4531, grad_fn=<NllLossBackward>)\n",
      "10.455724716186523\n",
      "tensor(10.4533, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4578, grad_fn=<NllLossBackward>)\n",
      "10.455581665039062\n",
      "tensor(10.4583, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4527, grad_fn=<NllLossBackward>)\n",
      "10.455492496490479\n",
      "tensor(10.4587, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4514, grad_fn=<NllLossBackward>)\n",
      "10.455066204071045\n",
      "tensor(10.4585, grad_fn=<NllLossBackward>)\n",
      "tensor(10.4534, grad_fn=<NllLossBackward>)\n",
      "10.455946922302246\n"
     ]
    }
   ],
   "source": [
    "# The training loop that uses batches from that data loader.\n",
    "epochs = 20\n",
    "for epoch_i in range(0, epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0 \n",
    "    for step,batch in enumerate(train_dataloader):    \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        print(step, loss)\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "    print(avg_train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
