{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertForMaskedLM, BertConfig\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from ipywidgets import IntProgress\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "using_gpu = False\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    using_gpu = True\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths to text datasets.\n",
    "plant_abstracts_corpus_path = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\"\n",
    "plant_phenotype_descriptions_path = \"../../plant-data/genes_texts_annots.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356714\n",
      "172374\n",
      "36868\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset that combines the dataset of plant phenotype descriptions and scrapped abstracts.\n",
    "corpus = open(plant_abstracts_corpus_path, 'r').read()\n",
    "sentences_from_corpus = sent_tokenize(corpus)\n",
    "phenotype_descriptions = \" \".join(pd.read_csv(plant_phenotype_descriptions_path)[\"descriptions\"].values)\n",
    "times_to_duplicate_phenotype_dataset = 5\n",
    "sentences_from_descriptions = sent_tokenize(phenotype_descriptions)\n",
    "sentences_from_descriptions_duplicated = list(np.repeat(sentences_from_descriptions, times_to_duplicate_phenotype_dataset))\n",
    "sentences_from_corpus_and_descriptions = sentences_from_corpus+sentences_from_descriptions_duplicated\n",
    "random.shuffle(sentences_from_corpus_and_descriptions)\n",
    "random.shuffle(sentences_from_corpus)\n",
    "random.shuffle(sentences_from_descriptions)\n",
    "sentences_from_corpus_and_descriptions = [preprocess_string(s) for s in sentences_from_corpus_and_descriptions]\n",
    "sentences_from_corpus = [preprocess_string(s) for s in sentences_from_corpus]\n",
    "sentences_from_descriptions = [preprocess_string(s) for s in sentences_from_descriptions]\n",
    "assert len(sentences_from_corpus_and_descriptions) == len(sentences_from_corpus)+(times_to_duplicate_phenotype_dataset*len(sentences_from_descriptions))\n",
    "print(len(sentences_from_corpus_and_descriptions))\n",
    "print(len(sentences_from_corpus))\n",
    "print(len(sentences_from_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phenotyp provid stephen grigg',\n",
       " 'reduc seed product wild type plant',\n",
       " 'ring contain protein function ubiquitin ligas wav protein show activ vitro',\n",
       " 'reduc seed volum',\n",
       " 'larger infloresc assum rhip background']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the sentences to be stemmed tokens separated by whitespace and show the first few of them.\n",
    "sentences = [\" \".join(s) for s in sentences_from_corpus_and_descriptions]\n",
    "sentences = sentences[:50]\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are now 35788 words in the vocabulary including special tokens\n",
      "the first few are\n",
      "['nhanc', 'kallar', 'deliv', 'maropg', 'atrzfp', 'yearli', 'npgut', 'bioprocess']\n",
      "done writing to the vocabulary file\n"
     ]
    }
   ],
   "source": [
    "# Preparing a vocabulary file based on these sentences for BERT.\n",
    "vocabulary_file_path = \"../data/corpus_related_files/vocabulary/vocab.txt\"\n",
    "vocabulary = set()\n",
    "for s in sentences_from_corpus_and_descriptions:\n",
    "    vocabulary.update(s)\n",
    "vocabulary.update([\"[PAD]\",\"[SEP]\",\"[UNK]\",\"[MASK]\"])\n",
    "print(\"there are now {} words in the vocabulary including special tokens\".format(len(vocabulary)))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"the first few are\")\n",
    "print(list(vocabulary)[:8])\n",
    "with open(vocabulary_file_path, \"w\") as f:\n",
    "    for token in list(vocabulary):\n",
    "        f.write(token+\"\\n\")\n",
    "print(\"done writing to the vocabulary file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 42 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                   (35788, 50)\n",
      "bert.embeddings.position_embeddings.weight                 (200, 50)\n",
      "bert.embeddings.token_type_embeddings.weight                 (2, 50)\n",
      "bert.embeddings.LayerNorm.weight                               (50,)\n",
      "bert.embeddings.LayerNorm.bias                                 (50,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight            (50, 50)\n",
      "bert.encoder.layer.0.attention.self.query.bias                 (50,)\n",
      "bert.encoder.layer.0.attention.self.key.weight              (50, 50)\n",
      "bert.encoder.layer.0.attention.self.key.bias                   (50,)\n",
      "bert.encoder.layer.0.attention.self.value.weight            (50, 50)\n",
      "bert.encoder.layer.0.attention.self.value.bias                 (50,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight          (50, 50)\n",
      "bert.encoder.layer.0.attention.output.dense.bias               (50,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight         (50,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias           (50,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight             (100, 50)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                  (100,)\n",
      "bert.encoder.layer.0.output.dense.weight                   (50, 100)\n",
      "bert.encoder.layer.0.output.dense.bias                         (50,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                   (50,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                     (50,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "cls.predictions.transform.dense.weight                      (50, 50)\n",
      "cls.predictions.transform.dense.bias                           (50,)\n",
      "cls.predictions.transform.LayerNorm.weight                     (50,)\n",
      "cls.predictions.transform.LayerNorm.bias                       (50,)\n"
     ]
    }
   ],
   "source": [
    "# Creating and parameratizing the small BERT architecture.\n",
    "vocab_size = vocabulary_size\n",
    "small_bert_configuration = BertConfig(\n",
    "    vocab_size=vocab_size, \n",
    "    hidden_size=50, \n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=100,\n",
    "    max_position_embeddings=200,\n",
    "    return_dict=True,   \n",
    ")\n",
    "model = BertForMaskedLM(small_bert_configuration)\n",
    "if using_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# An easier to read description of the model, from BERT fine-tuning with PyTorch.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='', vocab_size=35788, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# Creating the tokenizer using the provided vocabulary.\n",
    "tokenizer = BertTokenizer(vocab_file=vocabulary_file_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9373, 32908, 20201,  6702, 17764, 12501, 12162, 12162, 12162, 12162,\n",
      "         12162, 12162, 12162],\n",
      "        [ 9373, 23841, 13238, 20374,  9383, 31270, 31779, 12501, 12162, 12162,\n",
      "         12162, 12162, 12162],\n",
      "        [ 9373,  6389, 18419, 16614, 26502, 34661, 20273,  2589, 16614, 33429,\n",
      "          3867, 34372, 12501]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([3, 13])\n",
      "torch.Size([3, 13])\n",
      "torch.Size([3, 13])\n"
     ]
    }
   ],
   "source": [
    "# Testing out the tokenizer with the first few sentences from the dataset.\n",
    "# Note that all three arrays for all the sentences should be of the same length.\n",
    "# The attention mask should indicate whether a position is padding token or not, and token type IDs are not used.\n",
    "# The input IDs refer to the vocabulary ID of that particular word.\n",
    "encoding = tokenizer(sentences[0:3], return_tensors='pt', padding=True, truncation=True)\n",
    "print(encoding.input_ids)\n",
    "print(encoding.token_type_ids)\n",
    "print(encoding.attention_mask)\n",
    "print(encoding.input_ids.shape)\n",
    "print(encoding.token_type_ids.shape)\n",
    "print(encoding.attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phenotyp provid stephen [MASK]',\n",
       " 'reduc seed product wild [MASK] plant',\n",
       " 'ring contain protein function ubiquitin ligas wav protein show activ vitro',\n",
       " 'reduc seed volum',\n",
       " 'larger infloresc assum [MASK] background']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Producing the corresponding dataset of sentences with masked tokens for training.\n",
    "# Probability of a token getting masked should be set here.\n",
    "# Show the first few entries in the masked dataset to verify that some tokens are swapped with [MASK].\n",
    "prob = 0.15\n",
    "masked = [\" \".join([np.random.choice(['[MASK]',token],p=[prob,1-prob]) for token in s.split()]) for s in sentences]\n",
    "masked[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 23])\n",
      "torch.Size([20, 23])\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset object that can be read in as batches during the training loop.\n",
    "inputs_dict = tokenizer(masked, return_tensors='pt', padding=True, truncation=True)\n",
    "labels_dict = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "print(inputs_dict[\"input_ids\"].shape)\n",
    "print(labels_dict[\"input_ids\"].shape)\n",
    "dataset = TensorDataset(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"], labels_dict[\"input_ids\"])\n",
    "\n",
    "# Pick the batch size here.\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Creating and parameterizing the necessary objects for the optimizer and learning rate scheduler.\n",
    "epochs = 5\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps=total_steps)\n",
    "print(total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a validation task to monitor to avoid overfitting to the language modeling task.\n",
    "validation_df = pd.read_csv(\"../data/corpus_related_files/closely_related/concepts_multi_word.csv\")\n",
    "concepts_1 = list(validation_df[\"concept_1\"].values)\n",
    "concepts_2 = list(validation_df[\"concept_2\"].values)\n",
    "random.shuffle(concepts_2)\n",
    "validation_df_shuffled = pd.DataFrame({\"concept_1\":concepts_1,\"concept_2\":concepts_2})\n",
    "validation_df[\"class\"] = 1\n",
    "validation_df_shuffled[\"class\"] = 0\n",
    "df = pd.concat([validation_df,validation_df_shuffled])\n",
    "output_path_for_results = \"../models/bert_small/{}_validation.csv\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "output_path_for_results_summary = \"../models/bert_small/{}_validation_summary.csv\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up function to get sentence embeddings out of the word model.\n",
    "def vectorize_with_bert(text, model, tokenizer, method=\"sum\", layers=4):\n",
    "\n",
    "    #This function uses a pretrained BERT model to infer a document level vector for a collection \n",
    "    #of one or more sentences. The sentence are defined using the nltk sentence parser. This is \n",
    "    #done because the BERT encoder expects either a single sentence or a pair of sentences. The\n",
    "    #internal representations are drawn from the last n layers as specified by the layers argument, \n",
    "    #and represent a particular token but account for the context that it is in because the entire\n",
    "    #sentence is input simultanously. The vectors for the layers can concatentated or summed \n",
    "    #together based on the method argument. The vector obtained for each token then are averaged\n",
    "    #together to for the document level vector.\n",
    "    # This is just copied and pasted from the oats function.\n",
    "    #Args:\n",
    "        #text (str):  Any arbitrary text string.\n",
    "        #model (pytorch model): An already loaded BERT PyTorch model from a file or other source.\n",
    "        #tokenizer (bert tokenizer): Object which handles how tokenization specific to BERT is done. \n",
    "        #method (str): A string indicating how layers for a token should be combined (concat or sum).\n",
    "        #layers (int): An integer saying how many layers should be used for each token.\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    token_vecs_cat = []\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    for text in sentences:\n",
    "        marked_text = \"{} {} {}\".format(\"[CLS]\",text,\"[SEP]\")\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensor = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            #encoded_layers,_ = model(tokens_tensor,segments_tensor)\n",
    "            \n",
    "            \n",
    "            # Because the model configuration is a little bit different, the forward pass call is modified.\n",
    "            outputs = model(tokens_tensor,segments_tensor, output_hidden_states=True)\n",
    "            encoded_layers = outputs.hidden_states\n",
    "            \n",
    "            \n",
    "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "        token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "        batch = 0\n",
    "        for token in token_embeddings[batch]:\n",
    "            concatenated_layer_vectors = torch.cat(tuple(token[-layers:]), dim=0)\n",
    "            summed_layer_vectors = torch.sum(token[-layers:], dim=0)\n",
    "            token_vecs_cat.append(np.array(concatenated_layer_vectors))\n",
    "            token_vecs_sum.append(np.array(summed_layer_vectors))\n",
    "\n",
    "    # Check to make sure atleast one token was found with an embedding to use as a the \n",
    "    # vector representation. If there wasn't found, this is because of the combination\n",
    "    # of what the passed in description was, and how it was handled by either the sentence\n",
    "    # tokenizing step or the BERT tokenizer methods. Handle this by generating a random\n",
    "    # vector. This makes the embedding meaningless but prevents multiple instances that\n",
    "    # do not have embeddings from clustering together in downstream analysis. An expected\n",
    "    # layer size is hardcoded for this section based on the BERT architecture.\n",
    "    expected_layer_size = 50\n",
    "    if len(token_vecs_cat) == 0:\n",
    "        print(\"no embeddings found for input text '{}', generating random vector\".format(description))\n",
    "        random_concat_vector = np.random.rand(expected_layer_size*layers)\n",
    "        random_summed_vector = np.random.rand(expected_layer_size)\n",
    "        token_vecs_cat.append(random_concat_vector)\n",
    "        token_vecs_sum.append(random_summed_vector)\n",
    "\n",
    "    # Average the vectors obtained for each token across all the sentences present in the input text.\n",
    "    if method == \"concat\":\n",
    "        embedding = np.mean(np.array(token_vecs_cat),axis=0)\n",
    "    elif method == \"sum\":\n",
    "        embedding = np.mean(np.array(token_vecs_sum),axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"method argument is invalid\")\n",
    "    return(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(10.5696, grad_fn=<NllLossBackward>)\n",
      "0 10.569646835327148\n",
      "0 tensor(10.5724, grad_fn=<NllLossBackward>)\n",
      "1 10.572421073913574\n",
      "0 tensor(10.5705, grad_fn=<NllLossBackward>)\n",
      "2 10.570488929748535\n",
      "0 tensor(10.5725, grad_fn=<NllLossBackward>)\n",
      "3 10.572501182556152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(10.5730, grad_fn=<NllLossBackward>)\n",
      "4 10.573005676269531\n",
      "0 tensor(10.5683, grad_fn=<NllLossBackward>)\n",
      "5 10.56827163696289\n",
      "0 tensor(10.5679, grad_fn=<NllLossBackward>)\n",
      "6 10.567930221557617\n",
      "0 tensor(10.5700, grad_fn=<NllLossBackward>)\n",
      "7 10.569979667663574\n",
      "0 tensor(10.5688, grad_fn=<NllLossBackward>)\n",
      "8 10.568787574768066\n",
      "0 tensor(10.5698, grad_fn=<NllLossBackward>)\n",
      "9 10.569754600524902\n",
      "0 tensor(10.5703, grad_fn=<NllLossBackward>)\n",
      "10 10.570331573486328\n",
      "0 tensor(10.5691, grad_fn=<NllLossBackward>)\n",
      "11 10.569090843200684\n",
      "0 tensor(10.5682, grad_fn=<NllLossBackward>)\n",
      "12 10.568188667297363\n",
      "0 tensor(10.5710, grad_fn=<NllLossBackward>)\n",
      "13 10.570975303649902\n",
      "0 tensor(10.5712, grad_fn=<NllLossBackward>)\n",
      "14 10.571195602416992\n",
      "0 tensor(10.5643, grad_fn=<NllLossBackward>)\n",
      "15 10.564336776733398\n",
      "0 tensor(10.5726, grad_fn=<NllLossBackward>)\n",
      "16 10.572640419006348\n",
      "0 tensor(10.5682, grad_fn=<NllLossBackward>)\n",
      "17 10.568151473999023\n",
      "0 tensor(10.5704, grad_fn=<NllLossBackward>)\n",
      "18 10.570369720458984\n",
      "0 tensor(10.5684, grad_fn=<NllLossBackward>)\n",
      "19 10.568415641784668\n"
     ]
    }
   ],
   "source": [
    "# The training loop that uses batches from that data loader.\n",
    "for epoch_i in range(0, epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0 \n",
    "    for step,batch in enumerate(train_dataloader):    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        if using_gpu:\n",
    "            bi = batch[0].to(device)\n",
    "            bm = batch[1].to(device)\n",
    "            bl = batch[2].to(device)\n",
    "        else:\n",
    "            bi = batch[0]\n",
    "            bm = batch[1]\n",
    "            bl = batch[2]\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = model(input_ids=bi, attention_mask=bm, labels=bl)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        print(step, loss)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) \n",
    "    print(\"{} {}\".format(epoch_i, avg_train_loss))\n",
    "    \n",
    "    \n",
    "    # Keeping track of performance on the simple validation task by getting a precision recall curve.\n",
    "    model_name = \"bert_small\"\n",
    "    vectorize = lambda x: vectorize_with_bert(\" \".join(preprocess_string(x)), model, tokenizer, \"sum\", 1)\n",
    "    get_similarity = lambda s1,s2: 1-cosine(vectorize(s1),vectorize(s2))\n",
    "    df[model_name] = df.apply(lambda x: get_similarity(x[\"concept_1\"],x[\"concept_2\"]),axis=1)\n",
    "    y_true = list(df[\"class\"].values)\n",
    "    y_prob = list(df[model_name].values)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f_1_scores = f_beta(precision,recall,beta=1)\n",
    "    f_1_max = np.nanmax(f_1_scores)\n",
    "    rows.append((model_name, epoch_i, loss.item(), f_1_max))\n",
    "          \n",
    "\n",
    "\n",
    "# Writing results of the validation to those files.\n",
    "df.to_csv(output_path_for_results, index=False)\n",
    "header = [\"model\",\"epoch\",\"training_loss\",\"f1_max\"]\n",
    "pd.DataFrame(rows, columns=header).to_csv(output_path_for_results_summary, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert_small/model_save_11_15_2020_h01m16s45/tokenizer_config.json',\n",
       " '../models/bert_small/model_save_11_15_2020_h01m16s45/special_tokens_map.json',\n",
       " '../models/bert_small/model_save_11_15_2020_h01m16s45/vocab.txt',\n",
       " '../models/bert_small/model_save_11_15_2020_h01m16s45/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"../models/bert_small/model_save_{}/\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
