{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.utils.utils import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the section here for creating the small validation set.\n",
    "# This creates a dataframe that has all the parent child term pairs in it including synonyms, and excluding\n",
    "# the term pairs that have an explicit overlap in a token because that's not useful for embedding validation.\n",
    "\n",
    "# Load the ontology and term information.\n",
    "path = \"../ontologies/pato.obo\"\n",
    "ont = Ontology(path)\n",
    "term_ids_and_names = [(t.id,t.name) for t in ont.terms() if \"obsolete\" not in t.name]\n",
    "\n",
    "# Also including all the synonym information here.\n",
    "term_ids_and_names_with_synonyms = []\n",
    "for i,name in term_ids_and_names:\n",
    "    term_ids_and_names_with_synonyms.append((i,\" \".join(ont.term_to_tokens[i])))\n",
    "\n",
    "key_to_annotations = {i:[x[0]] for i,x in enumerate(term_ids_and_names)}\n",
    "key_to_term_id = {i:x[0] for i,x in enumerate(term_ids_and_names)}\n",
    "key_to_text_string = {i:x[1] for i,x in enumerate(term_ids_and_names_with_synonyms)}\n",
    "key_to_preprocessed_text_string = {i:\" \".join(preprocess_string(s)) for i,s in key_to_text_string.items()}\n",
    "\n",
    "# Get mappings that define which terms are very close to which others ones in the ontology structure.\n",
    "parents = {}\n",
    "children = {}\n",
    "for term in ont.terms():\n",
    "    parents[term.id] = [t.id for t in term.superclasses(with_self=False, distance=1)]\n",
    "    children[term.id] = [t.id for t in term.subclasses(with_self=False, distance=1)]\n",
    "siblings = {}\n",
    "for term in ont.terms():\n",
    "    siblings[term.id] = flatten([[t for t in children[parent_id] if t!=term.id] for parent_id in parents[term.id]])\n",
    "assert len(parents) == len(children)\n",
    "assert len(parents) == len(siblings)\n",
    "any_close = {}\n",
    "for key in parents.keys():\n",
    "    any_close[key] = flatten([parents[key],children[key],siblings[key]])\n",
    "    any_close[key] = flatten([parents[key],children[key]])\n",
    "\n",
    "df = pw.with_annotations(key_to_annotations, ont, \"jaccard\", tfidf=False).edgelist\n",
    "df = df[df[\"from\"]!=df[\"to\"]]\n",
    "df[\"from_id\"] = df[\"from\"].map(lambda x: key_to_term_id[x])\n",
    "df[\"to_id\"] = df[\"to\"].map(lambda x: key_to_term_id[x])\n",
    "df[\"from_text\"] = df[\"from\"].map(lambda x: key_to_text_string[x])\n",
    "df[\"to_text\"] = df[\"to\"].map(lambda x: key_to_text_string[x])\n",
    "df[\"close\"] = df.apply(lambda x: x[\"to_id\"] in any_close[x[\"from_id\"]], axis=1)\n",
    "df[\"token_overlap\"] = df.apply(lambda x: len(set(x[\"from_text\"].split()).intersection(set(x[\"to_text\"].split())))>0, axis=1)\n",
    "df = df[(df[\"token_overlap\"]==False) & (df[\"close\"]==True)]\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_threshold = 2\n",
    "df[\"candidates\"] = df.apply(lambda x: (len(x[\"from_text\"].split())>=word_threshold) and (len(x[\"to_text\"].split())>=word_threshold), axis=1)\n",
    "df = df[df[\"candidates\"]]\n",
    "df.to_csv(\"../data/corpus_related_files/closely_related/concepts_candidates.csv\", index=False)\n",
    "df.head(10)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
