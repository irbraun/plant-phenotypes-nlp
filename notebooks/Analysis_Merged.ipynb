{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "\n",
    "- [Links of Interest](#links)\n",
    "\n",
    "- [Part 1. Loading and Filtering Data](#part_1)\n",
    "    - [Setting input and output paths](#paths)\n",
    "    - [Reading in genes, annotations, and phenotype descriptions](#read_this_data)\n",
    "    - [KEGG biochemcial pathways](#read_in_kegg)\n",
    "    - [PlantCyc biochemcial pathways](#read_in_plantcyc)\n",
    "    - [Phenotype hierarchical subsets](#read_in_lloyd_meinke_subsets)\n",
    "    - [Phenotype hierarchical classes](#read_in_lloyd_meinke_classes)\n",
    "    - [Relating the group datasets to genes](#relating)\n",
    "    - [Filtering the datasets](#filtering)\n",
    "    \n",
    "- [Part 2. NLP Models](#part_2)\n",
    "    - [Word2Vec and Doc2Vec](#word2vec_doc2vec)\n",
    "    - [BERT and BioBERT](#bert_biobert)\n",
    "    - [Loading models](#load_models)\n",
    "\n",
    "- [Part 3. NLP Choices](#part_3)\n",
    "    - [Preprocessing descriptions](#preprocessing)\n",
    "    - [POS Tagging](#pos_tagging)\n",
    "    - [Reducing vocabulary size](#vocab)\n",
    "    - [Annotating with biological ontologies](#annotation)\n",
    "    - [Splitting into phene descriptions](#phenes)\n",
    "    \n",
    "- [Part 4. Generating Vectors and Distance Matrices](#part_4)\n",
    "    - [Defining methods to use](#methods)\n",
    "    - [Running all methods](#running)\n",
    "    - [Merging distances into an edgelist](#merging)\n",
    "    - [Adding edge information](#merging)\n",
    "\n",
    "- [Part 5. Clustering Analysis](#part_5)\n",
    "    - [Topic modeling](#topic_modeling)\n",
    "    - [Agglomerative clustering](#clustering)\n",
    "    - [Phenologs for OMIM disease phenotypes](#phenologs)\n",
    "    \n",
    "- [Part 6. Supervised Tasks](#part_6)\n",
    "    - [Distributions of distance values](#ks)\n",
    "    - [Within-group distance values](#within)\n",
    "    - [Predictions and AUC for shared pathways or interactions](#auc)\n",
    "    - [Tests for querying to recover related genes](#y)\n",
    "    - [Producing output summary table](#output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction: Text Mining Analysis of Phenotype Descriptions in Plants\n",
    "The purpose of this notebook is to evaluate what can be learned from a natural language processing approach to analyzing free-text descriptions of phenotype descriptions of plants. The approach is to generate pairwise distances matrices between a set of plant phenotype descriptions across different species, sourced from academic papers and online model organism databases. These pairwise distance matrices can be constructed using any vectorization method that can be applied to natural language. In this notebook, we specifically evaluate the use of n-gram and bag-of-words techniques, word and document embedding using Word2Vec and Doc2Vec, context-dependent word-embeddings using BERT and BioBERT, and ontology term annotations with automated annotation tools such as NOBLE Coder.\n",
    "\n",
    "Loading, manipulation, and filtering of the dataset of phenotype descriptions associated with genes across different plant species is largely handled through a Python package created for this purpose called OATS (Ontology Annotation and Text Similarity) which is available [here](https://github.com/irbraun/oats). Preprocessing of the descriptions, mapping the dataset to additional resources such as protein-protein interaction databases and biochemical pathway databases are handled in this notebook using that package as well. In the evaluation of each of these natural language processing approaches to analyzing this dataset of descriptions, we compare performance against a dataset generated through manual annotation of a similar dataset in Oellrich Walls et al. (2015) and against manual annotations with experimentally determined terms from the Gene Ontology (PO) and the Plant Ontology (PO).\n",
    "\n",
    "<a id=\"links\"></a>\n",
    "### Relevant links of interest:\n",
    "- Paper describing comparison of NLP and ontology annotation approaches to curation: [Braun, Lawrence-Dill (2019)](https://doi.org/10.3389/fpls.2019.01629)\n",
    "- Paper describing results of manual phenotype description curation: [Oellrich, Walls et al. (2015](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0053-y)\n",
    "- Plant databases with phenotype description text data available: [TAIR](https://www.arabidopsis.org/), [SGN](https://solgenomics.net/), [MaizeGDB](https://www.maizegdb.org/)\n",
    "- Python package for working with phenotype descriptions: [OATS](https://github.com/irbraun/oats)\n",
    "- Python package used for general NLP functions: [NLTK](https://www.nltk.org/), [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "- Python package used for working with biological ontologies: [Pronto](https://pronto.readthedocs.io/en/latest/)\n",
    "- Python package for loading pretrained BERT models: [PyTorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/)\n",
    "- For BERT Models pretrained on PubMed and PMC: [BioBERT Paper](https://arxiv.org/abs/1901.08746), [BioBERT Models](https://github.com/naver/biobert-pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 4.613815069198608 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp, hypergeom, pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.distances.edgelists import merge_edgelists, make_undirected, remove_self_loops, subset_with_ids\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocab_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocab_connected_components, reduce_vocab_linares_pontes\n",
    "from oats.nlp.preprocess import concatenate_with_bar_delim\n",
    "\n",
    "from _utils import Method\n",
    "from _utils import IndexedGraph\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_1\"></a>\n",
    "# Part 1. Loading and Filtering Data\n",
    "<a id=\"paths\"></a>\n",
    "### Setting up the input and output paths and summarizing output table\n",
    "This section defines some constants which are used for creating a uniquely named directory to contain all the outputs from running this instance of this notebook. The naming scheme is based on the time that the notebook is run. The other constants are used for specifying information in the output table about what the topic was for this notebook when it was run, such as looking at KEGG biochemical pathways or STRING protein-protein interaction data some other type of gene function grouping or hierarchy. These values are arbitrary and are just for keeping better notes about what the output of the notebook corresponds to. All the input and output file paths for loading datasets or models are also contained within this cell, so that if anything is moved the directories and file names should only have to be changed at this point and nowhere else further into the notebook. If additional files are added to the notebook cells they should be put here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and name an output directory according to when the notebooks or script was run.\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_filename = \"../data/pickles/gene_phenotype_dataset_all_text_and_annotations.pickle\"          # The full dataset pickle.\n",
    "kegg_pathways_filename = \"../data/pickles/groupings_from_kegg_pathways.pickle\"                       # The pathway groupings from KEGG.\n",
    "pmn_pathways_filename = \"../data/pickles/groupings_from_pmn_pathways.pickle\"                         # The pahway groupings from Plant Metabolic Network.\n",
    "lloyd_subsets_filename = \"../data/pickles/groupings_from_lloyd_subsets.pickle\"                       # The functional subsets defined by Lloyd and Meinke (2012).\n",
    "lloyd_classes_filename = \"../data/pickles/groupings_from_lloyd_classes.pickle\"                       # The functional classes defined by Lloyd and Meinke (2012).\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"     # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/pubmed_dbow/doc2vec_2.bin\"                                      # File holding saved Doc2Vec model trained on PubMed.\n",
    "doc2vec_wikipedia_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                                     # File holding saved Doc2Vec model trained on Wikipedia.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                                           # File holding saved Word2Vec model trained on Wikipedia.\n",
    "go_filename = \"../ontologies/go.obo\"                                                                 # Gene Ontology file in OBO format.\n",
    "po_filename = \"../ontologies/po.obo\"                                                                 # Plant Ontology file in OBO format.\n",
    "pato_filename = \"../ontologies/pato.obo\"                                                             # Phenotype and Trait Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                                                # Jar for NOBLE Coder annotation tool.\n",
    "biobert_pmc_path = \"../gensim/biobert_v1.0_pmc/pytorch_model\"                                        # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_path = \"../gensim/biobert_v1.0_pubmed/pytorch_model\"                                  # Path for PyTorch BioBERT model.\n",
    "biobert_pubmed_pmc_path = \"../gensim/biobert_v1.0_pubmed_pmc/pytorch_model\"                          # Path for PyTorch BioBERT model.\n",
    "panther_to_omim_filename = \"../data/orthology_related_files/ath_to_hsa/pantherdb_omim_df.csv\"        # File with mappings to human orthologs and disease phenotypes.\n",
    "\n",
    "\n",
    "pppn_edgelist_path = \"../data/supplemental_files_oellrich_walls/13007_2015_53_MOESM9_ESM.txt\"\n",
    "ortholog_file_path = \"../data/orthology_related_files/pantherdb/PlantGenomeOrthologs_IRB_Modified.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_this_data\"></a>\n",
    "### Reading in the dataset of genes and their associated phenotype descriptions and annotations\n",
    "The actual choice of pickle that is loaded here depends on the objective that is being used for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6364</td>\n",
       "      <td>3813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1406</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7999</td>\n",
       "      <td>4839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_genes  unique_descriptions\n",
       "0     ath       6364                 3813\n",
       "1     gmx         30                   24\n",
       "2     mtr         37                   36\n",
       "3     osa         92                   85\n",
       "4     sly         70                   70\n",
       "5     zma       1406                  811\n",
       "6   total       7999                 4839"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_pickle(dataset_filename)\n",
    "dataset.filter_has_description()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relating\"></a>\n",
    "### Relating the dataset of genes to the dataset of groupings or categories\n",
    "This section generates tables that indicate how the genes present in the dataset were mapped to the defined pathways or groups. This includes a summary table that indicates how many genes by species were succcessfully mapped to atleast one pathway or group, as well as a more detailed table describing how many genes from each species were mapped to each particular pathway or group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_groupings_object_and_write_summary_tables(dataset, groupings_filename, name):\n",
    "\n",
    "    # Load the groupings object.\n",
    "    groups = load_from_pickle(groupings_filename)\n",
    "    id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "    group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "    group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "    groups.to_csv(os.path.join(OUTPUT_DIR,\"part_1_{}_groupings.csv\".format(name)))\n",
    "\n",
    "    # Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "    summary = defaultdict(dict)\n",
    "    species_dict = dataset.get_species_dictionary()\n",
    "    for species in dataset.get_species():\n",
    "        summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "        summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "    table = pd.DataFrame(summary).transpose()\n",
    "    table.loc[\"total\"]= table.sum()\n",
    "    table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "    table = table.reset_index(inplace=False)\n",
    "    table = table.rename({\"index\":\"species\"}, axis=\"columns\")\n",
    "    table.to_csv(os.path.join(OUTPUT_DIR,\"part_1_{}_mappings_summary.csv\".format(name)), index=False)\n",
    "\n",
    "    \n",
    "    # Generate a table describing how many genes from each species map to which particular group.\n",
    "    summary = defaultdict(dict)\n",
    "    for group_id,ids in group_id_to_ids.items():\n",
    "        summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "        summary[group_id][\"total\"] = len([x for x in ids])\n",
    "    table = pd.DataFrame(summary).transpose()\n",
    "    table = table.sort_values(by=\"total\", ascending=False)\n",
    "    table = table.reset_index(inplace=False)\n",
    "    table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "    table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "    table.loc[\"total\"] = table.sum()\n",
    "    table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "    table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "    table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "    table.to_csv(os.path.join(OUTPUT_DIR,\"part_1_{}_mappings_by_group.csv\".format(name)), index=False)\n",
    "    \n",
    "    \n",
    "    # What are the similarites between the groups for the genes present in this dataset?\n",
    "    group_sims = defaultdict(dict)\n",
    "    for group_id_1,ids_1 in group_id_to_ids.items():\n",
    "        for group_id_2,ids_2 in group_id_to_ids.items():\n",
    "            jaccard_sim = len(set(ids_1).intersection(set(ids_2)))/len(set(ids_1).union(set(ids_2)))\n",
    "            group_sims[group_id_1][group_id_2] = jaccard_sim\n",
    "    table = pd.DataFrame(group_sims)\n",
    "    \n",
    "    \n",
    "    # Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency, special case.\n",
    "    if name == \"subsets\":\n",
    "        filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "        lmtm_df = pd.read_csv(filename)    \n",
    "        subsets_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in table.columns]\n",
    "        table = table[subsets_in_order]\n",
    "        table = table.reindex(subsets_in_order)\n",
    "        \n",
    "        \n",
    "    # Formatting the column names for this table correctly and outputting to a file.\n",
    "    table = table.reset_index(drop=False).rename({\"index\":\"group\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "    table.to_csv(os.path.join(OUTPUT_DIR,\"part_1_{}_similarity_matrix.csv\".format(name)), index=False)\n",
    "    \n",
    "\n",
    "    # Returning the groupings object and the list of IDs for genes that were mapped to one or more groups.\n",
    "    return(groups, group_mapped_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_in_kegg\"></a>\n",
    "### Reading in the dataset of groupings from KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>pathway_name</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>ncbi_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>ko_number</th>\n",
       "      <th>ec_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>hkl3|hexokinase-like 3</td>\n",
       "      <td>at4g37840</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>hxk3|hexokinase 3</td>\n",
       "      <td>at1g47840</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>hxk2|hexokinase 2</td>\n",
       "      <td>at2g19860</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>hxk1|hexokinase 1</td>\n",
       "      <td>at4g29130</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>hkl1|hexokinase-like 1</td>\n",
       "      <td>at1g50460</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>athxk4|hexokinase</td>\n",
       "      <td>at3g20040</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00844</td>\n",
       "      <td>EC:2.7.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pgi1|phosphoglucose isomerase 1</td>\n",
       "      <td>at4g24620</td>\n",
       "      <td></td>\n",
       "      <td>KO:K01810</td>\n",
       "      <td>EC:5.3.1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>sugar isomerase (sis) family protein</td>\n",
       "      <td>at5g42740</td>\n",
       "      <td></td>\n",
       "      <td>KO:K01810</td>\n",
       "      <td>EC:5.3.1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk6|phosphofructokinase 6</td>\n",
       "      <td>at4g32840</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk7|phosphofructokinase 7</td>\n",
       "      <td>at5g56630</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk4|phosphofructokinase 4</td>\n",
       "      <td>at5g61580</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk1|phosphofructokinase 1</td>\n",
       "      <td>at4g29220</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk5|phosphofructokinase 5</td>\n",
       "      <td>at2g22480</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk3|phosphofructokinase 3</td>\n",
       "      <td>at4g26270</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>pfk2|phosphofructokinase 2</td>\n",
       "      <td>at5g47810</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00850</td>\n",
       "      <td>EC:2.7.1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>phosphofructokinase family protein</td>\n",
       "      <td>at1g76550</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00895</td>\n",
       "      <td>EC:2.7.1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>mee51|phosphofructokinase family protein</td>\n",
       "      <td>at4g04040</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00895</td>\n",
       "      <td>EC:2.7.1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>phosphofructokinase family protein</td>\n",
       "      <td>at1g12000</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00895</td>\n",
       "      <td>EC:2.7.1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>phosphofructokinase family protein</td>\n",
       "      <td>at1g20950</td>\n",
       "      <td></td>\n",
       "      <td>KO:K00895</td>\n",
       "      <td>EC:2.7.1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00010</td>\n",
       "      <td>path:ath00010\\tGlycolysis / Gluconeogenesis - ...</td>\n",
       "      <td>fbp|inositol monophosphatase family protein</td>\n",
       "      <td>at1g43670</td>\n",
       "      <td></td>\n",
       "      <td>KO:K03841</td>\n",
       "      <td>EC:3.1.3.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species pathway_id                                       pathway_name                                   gene_names    ncbi_id uniprot_id  ko_number    ec_number\n",
       "0      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                       hkl3|hexokinase-like 3  at4g37840             KO:K00844   EC:2.7.1.1\n",
       "1      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                            hxk3|hexokinase 3  at1g47840             KO:K00844   EC:2.7.1.1\n",
       "2      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                            hxk2|hexokinase 2  at2g19860             KO:K00844   EC:2.7.1.1\n",
       "3      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                            hxk1|hexokinase 1  at4g29130             KO:K00844   EC:2.7.1.1\n",
       "4      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                       hkl1|hexokinase-like 1  at1g50460             KO:K00844   EC:2.7.1.1\n",
       "5      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                            athxk4|hexokinase  at3g20040             KO:K00844   EC:2.7.1.1\n",
       "6      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...              pgi1|phosphoglucose isomerase 1  at4g24620             KO:K01810   EC:5.3.1.9\n",
       "7      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...         sugar isomerase (sis) family protein  at5g42740             KO:K01810   EC:5.3.1.9\n",
       "8      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk6|phosphofructokinase 6  at4g32840             KO:K00850  EC:2.7.1.11\n",
       "9      ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk7|phosphofructokinase 7  at5g56630             KO:K00850  EC:2.7.1.11\n",
       "10     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk4|phosphofructokinase 4  at5g61580             KO:K00850  EC:2.7.1.11\n",
       "11     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk1|phosphofructokinase 1  at4g29220             KO:K00850  EC:2.7.1.11\n",
       "12     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk5|phosphofructokinase 5  at2g22480             KO:K00850  EC:2.7.1.11\n",
       "13     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk3|phosphofructokinase 3  at4g26270             KO:K00850  EC:2.7.1.11\n",
       "14     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...                   pfk2|phosphofructokinase 2  at5g47810             KO:K00850  EC:2.7.1.11\n",
       "15     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...           phosphofructokinase family protein  at1g76550             KO:K00895  EC:2.7.1.90\n",
       "16     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...     mee51|phosphofructokinase family protein  at4g04040             KO:K00895  EC:2.7.1.90\n",
       "17     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...           phosphofructokinase family protein  at1g12000             KO:K00895  EC:2.7.1.90\n",
       "18     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...           phosphofructokinase family protein  at1g20950             KO:K00895  EC:2.7.1.90\n",
       "19     ath    ko00010  path:ath00010\\tGlycolysis / Gluconeogenesis - ...  fbp|inositol monophosphatase family protein  at1g43670             KO:K03841  EC:3.1.3.11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kegg_groups, kegg_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, kegg_pathways_filename, \"kegg\")\n",
    "kegg_groups.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_in_plantcyc\"></a>\n",
    "### Reading in the dataset of groupings from PlantCyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>pathway_name</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>ec_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at1g52400-monomer|abscisic acid glucose ester ...</td>\n",
       "      <td>EC-3.2.1.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at4g15550-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at4g15260-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at3g21790-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at3g21760-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at2g23210-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at1g05530-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at1g05560-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at4g34138-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at2g23250-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at2g23260-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>at3g21780-monomer|abscisic acid glycosyltransf...</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at1g09420-monomer|glucose-6-phosphate dehydrog...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at5g35790-monomer|glucose-6-phosphate 1-dehydr...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at5g13110-monomer|glucose-6-phosphate 1-dehydr...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at5g40760-monomer|glucose-6-phosphate 1-dehydr...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at3g27300-monomer|glucose-6-phosphate 1-dehydr...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at1g24280-monomer|glucose-6-phosphate 1-dehydr...</td>\n",
       "      <td>EC-1.1.1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at5g41670-monomer|phosphogluconate dehydrogena...</td>\n",
       "      <td>EC-1.1.1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ath</td>\n",
       "      <td>OXIDATIVEPENT-PWY</td>\n",
       "      <td>pentose phosphate pathway (oxidative branch) I</td>\n",
       "      <td>at1g64190-monomer|phosphogluconate dehydrogena...</td>\n",
       "      <td>EC-1.1.1.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species         pathway_id                                    pathway_name                                         gene_names     ec_number\n",
       "0      ath           PWY-5272      abscisic acid degradation by glucosylation  at1g52400-monomer|abscisic acid glucose ester ...  EC-3.2.1.175\n",
       "1      ath           PWY-5272      abscisic acid degradation by glucosylation  at4g15550-monomer|abscisate &beta;-glucosyltra...  EC-2.4.1.263\n",
       "2      ath           PWY-5272      abscisic acid degradation by glucosylation  at4g15260-monomer|abscisate &beta;-glucosyltra...  EC-2.4.1.263\n",
       "3      ath           PWY-5272      abscisic acid degradation by glucosylation  at3g21790-monomer|abscisate &beta;-glucosyltra...  EC-2.4.1.263\n",
       "4      ath           PWY-5272      abscisic acid degradation by glucosylation  at3g21760-monomer|abscisate &beta;-glucosyltra...  EC-2.4.1.263\n",
       "5      ath           PWY-5272      abscisic acid degradation by glucosylation  at2g23210-monomer|abscisate &beta;-glucosyltra...  EC-2.4.1.263\n",
       "6      ath           PWY-5272      abscisic acid degradation by glucosylation  at1g05530-monomer|abscisic acid glucosyltransf...  EC-2.4.1.263\n",
       "7      ath           PWY-5272      abscisic acid degradation by glucosylation  at1g05560-monomer|abscisic acid glucosyltransf...  EC-2.4.1.263\n",
       "8      ath           PWY-5272      abscisic acid degradation by glucosylation  at4g34138-monomer|abscisic acid glucosyltransf...  EC-2.4.1.263\n",
       "9      ath           PWY-5272      abscisic acid degradation by glucosylation  at2g23250-monomer|abscisic acid glucosyltransf...  EC-2.4.1.263\n",
       "10     ath           PWY-5272      abscisic acid degradation by glucosylation  at2g23260-monomer|abscisic acid glucosyltransf...  EC-2.4.1.263\n",
       "11     ath           PWY-5272      abscisic acid degradation by glucosylation  at3g21780-monomer|abscisic acid glycosyltransf...  EC-2.4.1.263\n",
       "12     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at1g09420-monomer|glucose-6-phosphate dehydrog...   EC-1.1.1.49\n",
       "13     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at5g35790-monomer|glucose-6-phosphate 1-dehydr...   EC-1.1.1.49\n",
       "14     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at5g13110-monomer|glucose-6-phosphate 1-dehydr...   EC-1.1.1.49\n",
       "15     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at5g40760-monomer|glucose-6-phosphate 1-dehydr...   EC-1.1.1.49\n",
       "16     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at3g27300-monomer|glucose-6-phosphate 1-dehydr...   EC-1.1.1.49\n",
       "17     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at1g24280-monomer|glucose-6-phosphate 1-dehydr...   EC-1.1.1.49\n",
       "18     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at5g41670-monomer|phosphogluconate dehydrogena...   EC-1.1.1.44\n",
       "19     ath  OXIDATIVEPENT-PWY  pentose phosphate pathway (oxidative branch) I  at1g64190-monomer|phosphogluconate dehydrogena...   EC-1.1.1.44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmn_groups, pmn_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, pmn_pathways_filename, \"pmn\")\n",
    "pmn_groups.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_in_lloyd_meinke_subsets\"></a>\n",
    "### Reading in the dataset of phenotype subsets from Lloyd and Meinke, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_id</th>\n",
       "      <th>gene_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>FSM</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB|FSM|OVP|SRF</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>CDR|LIT</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|WAT</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP|SRF</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>CHS</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>NLS|GRS|IST</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>LEF|FSM</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRF</td>\n",
       "      <td>at1g01690|prd3|putative recombination initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ath</td>\n",
       "      <td>TMP</td>\n",
       "      <td>at1g01860|pfc1|paleface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>ROT</td>\n",
       "      <td>at1g01950|ark2|armadillo repeat kinesin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP</td>\n",
       "      <td>at1g02050|lap6|pksa|less adhesive pollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRF</td>\n",
       "      <td>at1g02065|spl8|squamosa promoter binding prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ath</td>\n",
       "      <td>PIG|LIT</td>\n",
       "      <td>at1g02090|fus5|cop15|fusca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ath</td>\n",
       "      <td>MSL|PTH</td>\n",
       "      <td>at1g02120|vad1|vascular-associated death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ath</td>\n",
       "      <td>GAM</td>\n",
       "      <td>at1g02140|hap1|mago|mee63|hapless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|FSM|WAT</td>\n",
       "      <td>at1g02205|cer1|eceriferum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ath</td>\n",
       "      <td>PIG</td>\n",
       "      <td>at1g02280|ppi1|toc33|plastid protein import</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species                         group_id                                         gene_names\n",
       "0      ath                              FSM                         at1g01030|nga3|top1|ngatha\n",
       "1      ath                  EMB|FSM|OVP|SRF    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2      ath                          CDR|LIT             at1g01060|lhy|late elongated hypocotyl\n",
       "3      ath                          IST|WAT   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4      ath                          OVP|SRF                 at1g01280|cyp703a2|cytochrome p450\n",
       "5      ath                              EMB        at1g01370|cenh3|centromere-specific histone\n",
       "6      ath                              CHS  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7      ath                      NLS|GRS|IST  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8      ath                          LEF|FSM                          at1g01510|an|angustifolia\n",
       "9      ath  SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP                              at1g01550|bps1|bypass\n",
       "10     ath                              SRF  at1g01690|prd3|putative recombination initiati...\n",
       "11     ath                              TMP                            at1g01860|pfc1|paleface\n",
       "12     ath                              ROT            at1g01950|ark2|armadillo repeat kinesin\n",
       "13     ath                              OVP           at1g02050|lap6|pksa|less adhesive pollen\n",
       "14     ath                              SRF  at1g02065|spl8|squamosa promoter binding prote...\n",
       "15     ath                          PIG|LIT                         at1g02090|fus5|cop15|fusca\n",
       "16     ath                          MSL|PTH           at1g02120|vad1|vascular-associated death\n",
       "17     ath                              GAM                  at1g02140|hap1|mago|mee63|hapless\n",
       "18     ath                      IST|FSM|WAT                          at1g02205|cer1|eceriferum\n",
       "19     ath                              PIG        at1g02280|ppi1|toc33|plastid protein import"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phe_subsets_groups, subsets_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, lloyd_subsets_filename, \"subsets\")\n",
    "phe_subsets_groups.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_in_lloyd_meinke_classes\"></a>\n",
    "### Reading in the dataset of phenotype classes from Lloyd and Meinke, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_id</th>\n",
       "      <th>gene_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>S</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>T</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>S</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>H</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>L</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g01690|prd3|putative recombination initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ath</td>\n",
       "      <td>P</td>\n",
       "      <td>at1g01860|pfc1|paleface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01950|ark2|armadillo repeat kinesin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g02050|lap6|pksa|less adhesive pollen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g02065|spl8|squamosa promoter binding prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g02090|fus5|cop15|fusca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g02120|vad1|vascular-associated death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ath</td>\n",
       "      <td>G</td>\n",
       "      <td>at1g02140|hap1|mago|mee63|hapless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g02205|cer1|eceriferum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g02280|ppi1|toc33|plastid protein import</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species group_id                                         gene_names\n",
       "0      ath        R                         at1g01030|nga3|top1|ngatha\n",
       "1      ath        S    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2      ath        T             at1g01060|lhy|late elongated hypocotyl\n",
       "3      ath        V   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4      ath        R                 at1g01280|cyp703a2|cytochrome p450\n",
       "5      ath        S        at1g01370|cenh3|centromere-specific histone\n",
       "6      ath        H  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7      ath        V  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8      ath        V                          at1g01510|an|angustifolia\n",
       "9      ath        L                              at1g01550|bps1|bypass\n",
       "10     ath        R  at1g01690|prd3|putative recombination initiati...\n",
       "11     ath        P                            at1g01860|pfc1|paleface\n",
       "12     ath        V            at1g01950|ark2|armadillo repeat kinesin\n",
       "13     ath        R           at1g02050|lap6|pksa|less adhesive pollen\n",
       "14     ath        R  at1g02065|spl8|squamosa promoter binding prote...\n",
       "15     ath        V                         at1g02090|fus5|cop15|fusca\n",
       "16     ath        V           at1g02120|vad1|vascular-associated death\n",
       "17     ath        G                  at1g02140|hap1|mago|mee63|hapless\n",
       "18     ath        V                          at1g02205|cer1|eceriferum\n",
       "19     ath        V        at1g02280|ppi1|toc33|plastid protein import"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phe_classes_groups, classes_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, lloyd_classes_filename, \"classes\")\n",
    "phe_classes_groups.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>\n",
    "### Option 1: Filtering the dataset based on presence in the curated Oellrich, Walls et al. (2015) dataset\n",
    "TODO maybe this part should not actually subset data? because only the non zero interactions are listed in that file? So the IDs shouldn't be used to do subsetting here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5959</td>\n",
       "      <td>1799</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5959</td>\n",
       "      <td>1840</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5959</td>\n",
       "      <td>443</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5959</td>\n",
       "      <td>1766</td>\n",
       "      <td>0.516393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5959</td>\n",
       "      <td>1838</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5959</td>\n",
       "      <td>2035</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5959</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.417219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5959</td>\n",
       "      <td>1795</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5959</td>\n",
       "      <td>1894</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5959</td>\n",
       "      <td>6227</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to     value\n",
       "0  5959  1799  1.000000\n",
       "1  5959  1840  1.000000\n",
       "2  5959   443  0.926471\n",
       "3  5959  1766  0.516393\n",
       "4  5959  1838  1.000000\n",
       "5  5959  2035  0.954545\n",
       "6  5959  2059  0.417219\n",
       "7  5959  1795  1.000000\n",
       "8  5959  1894  1.000000\n",
       "9  5959  6227  0.900000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ow_edgelist = AnyInteractions(dataset.get_name_to_id_dictionary(), pppn_edgelist_path)\n",
    "ow_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Filtering the dataset based on protein-protein interactions\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mentioned atleast one time in the STRING database for a given species. If a gene is not mentioned at all in STRING, there is no information available for whether or not it interacts with any other proteins in the dataset so choose to not include it in the analysis. Only genes that have atleast one true positive are included because these are the only ones for which the missing information (negatives) is meaningful. This should be run instead of the subsequent cell, or the other way around, based on whether or not protein-protein interactions is the prediction goal for the current analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>73.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>73.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4925.0</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4488.0</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>73.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4275.0</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>73.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>73.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>73.0</td>\n",
       "      <td>5818.0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4542.0</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     from      to  combined_score\n",
       "346  73.0    49.0             161\n",
       "348  73.0   948.0             412\n",
       "351  73.0  4925.0             198\n",
       "352  73.0  4488.0             223\n",
       "353  73.0   132.0             838\n",
       "355  73.0  4275.0             410\n",
       "357  73.0   473.0             201\n",
       "358  73.0  5461.0             619\n",
       "359  73.0  5818.0             268\n",
       "360  73.0  4542.0             272"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis\n",
    "    \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\", # Maize\n",
    "    \"../data/group_related_files/string/4530.protein.links.detailed.v11.0.txt\", # Tomato \n",
    "    \"../data/group_related_files/string/4081.protein.links.detailed.v11.0.txt\", # Medicago\n",
    "    \"../data/group_related_files/string/3880.protein.links.detailed.v11.0.txt\", # Rice \n",
    "    \"../data/group_related_files/string/3847.protein.links.detailed.v11.0.txt\", # Soybean\n",
    "]\n",
    "genes = dataset.get_gene_dictionary()\n",
    "string_edgelist = ProteinInteractions(genes, naming_file, *interaction_files)\n",
    "string_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Filtering the dataset based on orthologous genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>5730.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179180</th>\n",
       "      <td>1898.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190004</th>\n",
       "      <td>3038.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192404</th>\n",
       "      <td>1685.0</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198820</th>\n",
       "      <td>3864.0</td>\n",
       "      <td>4708.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413820</th>\n",
       "      <td>3880.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415020</th>\n",
       "      <td>2564.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431292</th>\n",
       "      <td>3652.0</td>\n",
       "      <td>2225.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470160</th>\n",
       "      <td>2552.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470668</th>\n",
       "      <td>1017.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          from      to  value\n",
       "692     5730.0    23.0    1.0\n",
       "179180  1898.0  1898.0    1.0\n",
       "190004  3038.0  2411.0    1.0\n",
       "192404  1685.0  1867.0    1.0\n",
       "198820  3864.0  4708.0    1.0\n",
       "413820  3880.0   427.0    1.0\n",
       "415020  2564.0  1155.0    1.0\n",
       "431292  3652.0  2225.0    1.0\n",
       "470160  2552.0   575.0    1.0\n",
       "470668  1017.0  1130.0    1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panther_edgelist = AnyInteractions(dataset.get_name_to_id_dictionary(), ortholog_file_path)\n",
    "panther_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4: Filtering the dataset based on membership in pathways or phenotype category\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mapped to atleast one pathway in whatever the source of pathway membership we are using is (KEGG, Plant Metabolic Network, etc). This is because for these genes, it will be impossible to correctly predict their pathway membership, and we have no evidence that they belong or do not belong in certain pathways so they can not be identified as being true or false negatives in any case. This should not actually be necessary if the dataset used to start the notebook analysis has already be subset for just the genes that either have pathway information of phenotype classification information, this should just be used to double check that the numbers make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088\n"
     ]
    }
   ],
   "source": [
    "# ADD A ROW TO THE TABLE THAT SHOWS WHAT THIS VALUE IS (SOMETHING LIKE TOTAL) ON TEH BOTTOM\n",
    "# TO THE TABLE OF BIOLOGICAL QUESTIONS THIS IS THE N\n",
    "\n",
    "# ADD SOMETHING ELSE THAT TAKES ABOUT WHAT THE OVERLAP BETWEEN THESE GENES FOR EACH QUESTION ARE.\n",
    "\n",
    "# Get the list of all the IDs in this dataset that have any relevant mapping at all to the biological questions.\n",
    "ids_with_any_mapping = list(set(flatten([\n",
    "    kegg_mapped_ids,\n",
    "    pmn_mapped_ids,\n",
    "    subsets_mapped_ids,\n",
    "    classes_mapped_ids,\n",
    "    string_edgelist.ids,\n",
    "    panther_edgelist.ids\n",
    "])))\n",
    "print(len(ids_with_any_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2634\n"
     ]
    }
   ],
   "source": [
    "# WHAT ARE THE IDS THAT HAVE ALL THE REQUIRED ANNOTATIONS FOR COMPARISON\n",
    "annots = dataset.get_annotations_dictionary()\n",
    "go_mapped_ids = [i for i in dataset.get_ids() if \"GO\" in annots[i]]\n",
    "po_mapped_ids = [i for i in dataset.get_ids() if \"PO\" in annots[i]]\n",
    "ids_with_all_annotations = list(set(flatten([\n",
    "    go_mapped_ids,\n",
    "    po_mapped_ids,\n",
    "    ow_edgelist.ids\n",
    "])))\n",
    "print(len(ids_with_all_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>270</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mtr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>osa</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sly</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zma</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total</td>\n",
       "      <td>300</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_genes  unique_descriptions\n",
       "0     ath        270                  240\n",
       "1     mtr          1                    1\n",
       "2     osa          9                    9\n",
       "3     sly          2                    2\n",
       "4     zma         18                   18\n",
       "5   total        300                  270"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALWAYS DO THIS\n",
    "dataset.filter_with_ids(ids_with_any_mapping)\n",
    "dataset.filter_random_k(300)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_2\"></a>\n",
    "# Part 2. NLP Models\n",
    "\n",
    "\n",
    "<a id=\"word2vec_doc2vec\"></a>\n",
    "### Word2Vec and Doc2Vec\n",
    "Word2Vec is a word embedding technique using a neural network trained on a so-called *false task*, namely either predicting a missing word from within a sequence of context words drawn from a sentence or phrase, or predicting which contexts words surround some given input word drawn from a sentence or phrase. Each of these tasks are supervised (the correct answer is fixed and known), but can be generated from unlabelled text data such as a collection of books or wikipedia articles, meaning that even though the task itself is supervised the training data can be generated automatically, enabling the creation of enormous training sets. The internal representation for particular words learned during the training process contain semantically informative features related to that given word, and can therefore be used as embeddings used downstream for tasks such as finding similarity between words or as input into additional models. Doc2Vec is an extension of this technique that determines vector embeddings for entire documents (strings containing multiple words, could be sentences, paragraphs, or documents).\n",
    "\n",
    "\n",
    "<a id=\"bert_biobert\"></a>\n",
    "### BERT and BioBERT\n",
    "BERT ('Bidirectional Encoder Representations from Transformers') is another neueral network-based model trained on two different false tasks, namely predicting the subsequent sentence given some input sentence, or predicting the identity of a set of words masked from an input sentence. Like Word2Vec, this architecture can be used to generate vector embeddings for a particular input word by extracting values from a subset of the encoder layers that correspond to that input word. Practically, a major difference is that because the input word is input in the context of its surrounding sentence, the embedding reflects the meaning of a particular word in a particular context (such as the difference in the meaning of *root* in the phrases *plant root* and *root of the problem*. BioBERT refers to a set of BERT models which have been finetuned on the PubMed and PMC corpora. See the list of relevant links for the publications and pages associated with these models.\n",
    "\n",
    "<a id=\"load_models\"></a>\n",
    "### Loading trained and saved models\n",
    "Versions of the architectures discussed above which have been saved as trained models are loaded here. Some of these models are loaded as pretrained models from the work of other groups, and some were trained on data specific to this notebook and loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Files and models related to the machine learning text embedding methods used here.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the descriptions from hand-picked dataset of phenotype pairs\n",
    "See the other notebook for the creation of this dataset. This is included in this notebook instead of a separated notebook because we want the treatment of the individual phenotype text instances to be the same as is done for the descriptions from the real dataset of plant phenotypes. The list of computational approaches being evaluated for this task is the same in both cases so all of the cells between the point where the descriptions are read in and when the distance matrices are found using all those methods are the same for this task as any of the biological questions that this notebook is focused on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'long hypocotyl under far-red light; partially unfolded cotyledons under far-red light',\n",
       " 1: 'embryo defective; seedling defective',\n",
       " 2: 'increased lateral root number; late flowering',\n",
       " 3: 'embryo defective-globular; embryo defective; globular',\n",
       " 4: 'abnormal microtubule development; abnormal pavement cell morphology',\n",
       " 5: 'albino; dwarf',\n",
       " 6: 'pale green leaves; reduced rosette size',\n",
       " 7: 'pale green seedlings; pale green seeds',\n",
       " 8: 'embryo defective; seedling defective',\n",
       " 9: 'large rosette leaves',\n",
       " 10: 'chlorotic leaves; dwarf',\n",
       " 11: 'abnormal cotyledon morphology; collapsed root tip',\n",
       " 12: 'dwarf; fragile inflorescence stems',\n",
       " 13: 'anther defects; dwarf',\n",
       " 14: 'curled leaves; increased rosette leaf number',\n",
       " 15: 'dark green; dwarf',\n",
       " 16: 'resistant to anthranilate analogs (herbicide); seedling lethal without exogenous tryptophan',\n",
       " 17: 'elevated cobalt levels in shoots; sensitive to cobalt',\n",
       " 18: 'dwarf; sensitive to oxidative stress',\n",
       " 19: 'altered response to wounding; insensitive to jasmonate',\n",
       " 20: 'sensitive to zinc',\n",
       " 21: 'embryo defective (inferred); female gametophyte defective',\n",
       " 22: 'pale green; slow growth',\n",
       " 23: 'complete female gametophyte defective; male gametophyte defective',\n",
       " 24: 'abnormal flower morphology; abnormal leaf morphology',\n",
       " 25: 'right-handed helical growth in the root and other rapidly elongating organs; strong severity of cell expansion defects',\n",
       " 26: 'late flowering; late flowering; strong influence of short days on flowering time; small but significant effect of vernalization on flowering time; narrow leaves, increased number of cauline leaves; flowering occurs about 13 days later than wild type',\n",
       " 27: 'sensitive to avirulent pseudomonas syringae; sensitive to bth (sa mimic)',\n",
       " 28: 'abnormal flower morphology; abnormal leaf morphology',\n",
       " 29: 'decreased potassium permeability of the plasma membrane in leaf mesophyll cell layers',\n",
       " 30: 'altered fatty acid composition; altered systemic acquired resistance response',\n",
       " 31: 'abnormal flower morphology; abnormal leaf morphology',\n",
       " 32: 'embryo defective-globular; embryo defective; globular',\n",
       " 33: 'elevated starch levels',\n",
       " 34: 'embryo defective-cotyledon; embryo defective; cotyledon',\n",
       " 35: 'embryo defective-preglobular / globular; embryo defective; preglobular / globular',\n",
       " 36: 'abnormal mitochondria morphology; abnormal peroxisome morphology',\n",
       " 37: 'complete sterility; defects in meiosis',\n",
       " 38: 'albino seedlings; albino seeds',\n",
       " 39: 'embryo defective-globular; embryo defective; globular',\n",
       " 40: 'embryo defective-cotyledon; embryo defective; cotyledon',\n",
       " 41: 'embryo defective-cotyledon; embryo defective; cotyledon',\n",
       " 42: 'sensitive to manganese starvation; under mn-deficient conditions, roots are shorter, cell elongation was inhibited, and root hair elongation is markedly compromised',\n",
       " 43: 'completely sterile; dwarf',\n",
       " 44: 'decreased stomatal closure; decreased stomatal closure; no other phenotypes detected',\n",
       " 45: 'reduced nitrate uptake; resistant to chlorate',\n",
       " 46: 'abnormal cell plates; abnormal cell walls',\n",
       " 47: 'embryo defective-preglobular; embryo defective; preglobular',\n",
       " 48: 'abnormal pith cell morphology; dwarf',\n",
       " 49: 'low penetrance of heart-shaped cotyledons; semi-dominant; cotyledons fuse along one side at very low frequency (approx 0.5 %), slight fusion of sepals, stamen fusion at very low frequency',\n",
       " 50: 'sensitive to uv-b light',\n",
       " 51: 'embryo defective-cotyledon; embryo defective; cotyledon',\n",
       " 52: 'insensitive to cytokinin; short roots',\n",
       " 53: 'embryo defective-transition; embryo defective; transition',\n",
       " 54: 'albino seedlings; pale green seedlings',\n",
       " 55: 'homozygous plants have small and hyponastic cotyledons, some seedlings lack the primary root; 80% show early seedling lethality; epinastic cotyledons; wavy leaf margins; increased number of axillary shoots; reduced fertility',\n",
       " 56: 'blunt siliques; short inflorescence stems',\n",
       " 57: 'anthocyanin accumulation; dark-grown seedlings are de-etiolated',\n",
       " 58: 'abnormal rosette leaf morphology; decreased leaf number',\n",
       " 59: 'abnormal leaf morphology; dwarf',\n",
       " 60: 'reduced lateral root formation',\n",
       " 61: 'abnormal chloroplast morphology; albino',\n",
       " 62: 'altered response to red:far-red light; decreased branching',\n",
       " 63: 'fewer lateral roots; semi-dwarf',\n",
       " 64: 'abnormal flower morphology; blister-like outgrowths on cotyledons',\n",
       " 65: 'elongated hypocotyl in far-red light',\n",
       " 66: 'dwarf; pale green rosette',\n",
       " 67: 'this mutant has about 1/10 of the ornithine aminotransferase activity detected in wild type plants, but it has no other obvious phenotypic defects when grown under greenhouse conditions; in response to salt-stress, the mutant appears to produce wild-type levels of proline',\n",
       " 68: 'insensitive to brassinosteroids; semi-dwarf',\n",
       " 69: 'defects in meiosis; severely reduced fertility',\n",
       " 70: 'mutants did not grow on medium  containing 2.5 mm or 5 mm of l-glutamine as the sole nitrogen source',\n",
       " 71: 'embryo defective; developmental arrest of mutant embryos',\n",
       " 72: 'pigment defective embryo',\n",
       " 73: 'female gametophyte defective; rare embryo defective (inferred)',\n",
       " 74: 'embryo defective; root meristemless; the mutant exhibits a significant flg22-induced reduction in bacterial growth, indicating that the gene is most probably not required for flg22-induced bacterial resistance',\n",
       " 75: 'delayed germination; germination of the mutants exhibited hypersensitivity to exogenously applied abscisic acid and paclobutrazol, an inhibitor of gibberellin biosynthesis',\n",
       " 76: 'the morphology of mutant plants was similar and to some extent weaker than ssm (cs68717) plants; ectopic occurrence of myrosin cells is found in the leaves (increased number of myrosin cells)',\n",
       " 77: 'sensitive to oxidative stress',\n",
       " 78: 'elevated threonine levels; short roots',\n",
       " 79: 'chlorate resistance due to a reduced uptake of chlorate; chlorate resistant due to reduced chlorate uptake',\n",
       " 80: 'late flowering; short leaf blades',\n",
       " 81: 'slow growth',\n",
       " 82: 'embryo defective; developmental arrest of mutant embryos occurs at cotyledon stage',\n",
       " 83: 'low hydroxyl fatty acid levels in seeds',\n",
       " 84: 'embryo defective-globular; embryo defective; globular',\n",
       " 85: 'altered seed storage composition, reduced oil and protein body contents with concurrent accumulation of starch; developing embryos have white colored appearance; cotyledons and hypocotyl cells contain no recognizable protein bodies and few oil bodies; starch granules, membrane stacks, vesicles, and vacuoles are present in cells (all of which are absent in wt cells); oil bodies contain higher electron density substances than wt; mature seeds shrink upon desiccation (as a consequence of insufficient deposition of storage molecules) and are lethal; kanamycin resistant',\n",
       " 86: 'abnormal mitochondria morphology; abnormal peroxisome morphology',\n",
       " 87: 'chromosomes do not synapse during both male and female meiosis; there is a total absence of chiasmata',\n",
       " 88: 'pale green; seedling lethal',\n",
       " 89: 'embryo defective; leafy cotyledons',\n",
       " 90: 'homozygotes are embryo lethal; homozygotes are embryo lethal',\n",
       " 91: 'embryo defective-globular; embryo defective; globular',\n",
       " 92: 'etiolated seedlings of both wild type and mtk mutants have very low emission of ethylene; the eto3 mutant on the other hand has much higher emission of ethylene',\n",
       " 93: 'dwarf; pale green leaves',\n",
       " 94: 'decreased stomatal density; reduced number of stomata and non protruding cells on hypocotyl epidermis',\n",
       " 95: 'resistant to tunicamycin; small seedlings',\n",
       " 96: 'resistant to tunicamycin; small seedlings',\n",
       " 97: 'crinkled leaves; embryo lethal',\n",
       " 98: 'dwarf in the dark; no apical hook in the dark',\n",
       " 99: 'embryo defective; variegated seedlings'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This would only impact the notebook if run in the absence of the subsequent cells, never matters in the script.\n",
    "num_pairs = 50\n",
    "mupdata = pd.read_csv(\"../data/corpus_related_files/phenotype_pairs/scored.csv\")\n",
    "assert num_pairs == mupdata.shape[0]\n",
    "descriptions = mupdata[\"Phenotype 1\"].values.tolist()\n",
    "descriptions.extend(mupdata[\"Phenotype 2\"].values.tolist())\n",
    "descriptions = {i:description for i,description in enumerate(descriptions)}\n",
    "pair_to_score = {(i,i+num_pairs):s for i,s in enumerate(mupdata[\"Score\"].values)}\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_3\"></a>\n",
    "# Part 3. NLP Choices\n",
    "\n",
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing text descriptions\n",
    "The preprocessing methods applied to the phenotype descriptions are a choice which impacts the subsequent vectorization and similarity methods which construct the pairwise distance matrix from each of these descriptions. The preprocessing methods that make sense are also highly dependent on the vectorization method or embedding method that is to be applied. For example, stemming (which is part of the full proprocessing done below using the Gensim preprocessing function) is useful for the n-grams and bag-of-words methods but not for the document embeddings methods which need each token to be in the vocabulary that was constructed and used when the model was trained. For this reason, embedding methods with pretrained models where the vocabulary is fixed should have a lighter degree of preprocessing not involving stemming or lemmatization but should involve things like removal of non-alphanumerics and normalizing case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain a mapping between IDs and the raw text descriptions associated with that ID from the dataset.\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "\n",
    "# Preprocessing of the text descriptions. Different methods are necessary for different approaches.\n",
    "descriptions_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "descriptions_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "descriptions_no_stopwords = {i:remove_stopwords(d) for i,d in descriptions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_tagging\"></a>\n",
    "### POS tagging the phenotype descriptions for nouns and adjectives\n",
    "Note that preprocessing of the descriptions should be done after part-of-speech tagging, because tokens that are removed during preprocessing before n-gram analysis contain information that the parser needs to accurately call parts-of-speech. This step should be done on the raw descriptions and then the resulting bags of words can be subset using additional preprocesssing steps before input in one of the vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "descriptions_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "descriptions_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_noun_only.items()}\n",
    "descriptions_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "descriptions_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions_adj_only.items()}\n",
    "descriptions_noun_adj = {i:\"{} {}\".format(descriptions_noun_only[i],descriptions_adj_only[i]) for i in descriptions.keys()}\n",
    "descriptions_noun_adj_full_preprocessing = {i:\"{} {}\".format(descriptions_noun_only_full_preprocessing[i],descriptions_adj_only_full_preprocessing[i]) for i in descriptions.keys()}\n",
    "descriptions_noun_adj_simple_preprocessing = {i:\"{} {}\".format(descriptions_noun_only_simple_preprocessing[i],descriptions_adj_only_simple_preprocessing[i]) for i in descriptions.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vocab\"></a>\n",
    "### Reducing the vocabulary size using a word distance matrix\n",
    "These approaches for reducing the vocabulary size of the dataset work by replacing multiple words that occur throughout the dataset of descriptions with an identical word that is representative of this larger group of words. The total number of unique words across all descriptions is therefore reduced, and when observing n-gram overlaps between vector representations of these descriptions, overlaps will now occur between descriptions that included different but similar words. These methods work by actually generating versions of these descriptions that have the word replacements present. The returned objects for these methods are the revised description dictionary, a dictionary mapping tokens in the full vocabulary to tokens in the reduced vocabulary, and a dictionary mapping tokens in the reduced vocabulary to a list of tokens in the full vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reducing the size of the vocabulary for descriptions treated with simple preprocessing.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in descriptions_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "\n",
    "# Make sure that the tokens list is in the same order as the indices representing each word in the distance matrix.\n",
    "# This is only trivial here because the IDs used are ordered integers 0 to n, but this might not always be the case.\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "threshold = 0.2\n",
    "descriptions_linares_pontes, reduce_lp, unreduce_lp = reduce_vocab_linares_pontes(descriptions_simple_preprocessing, tokens, distance_matrix, n)\n",
    "descriptions_connected_components, reduce_cc, unreduce_cc = reduce_vocab_connected_components(descriptions_simple_preprocessing, tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing vocabulary size based on identifying important words\n",
    "These approcahes for reducing the vocabulary size of the dataset work by identifying which words in the descriptions are likely to be the most important for identifying differences between the phenotypes and meaning of the descriptions. One approach is to determine which words occur at a higher rate in text of interest such as articles about plant phenotypes as compared to their rates in more general texts such as a corpus of news articles. These approaches do not create modified versions of the descriptions but rather provide vocabulary objects that can be passed to the sklearn vectorizer or constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing a vocabulary by looking at what words are overrepresented in domain specific text.\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "tokens = get_overrepresented_tokens(phenotypes_corpus, background_corpus, max_features=5000)\n",
    "vocabulary_from_text = get_vocab_from_tokens(tokens)\n",
    "\n",
    "# Constructing a vocabulary by assuming all words present in a given ontology are important.\n",
    "ontology = Ontology(pato_filename)\n",
    "vocabulary_from_ontology = get_vocab_from_tokens(ontology.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annotation\"></a>\n",
    "### Annotating descriptions with ontology terms\n",
    "This section generates dictionaries that map gene IDs from the dataset to lists of strings, where those strings are ontology term IDs. How the term IDs are found for each gene entry with its corresponding phenotype description depends on the cell below. Firstly, the terms are found by using the NOBLE Coder annotation tool through these wrapper functions to identify the terms by looking for instances of the term's label or synonyms in the actual text of the phenotype descriptions. Secondly, the next cell just draws the terms directly from the dataset itself. In this case, these are high-confidence annotations done by curators for a comparison against what can be accomplished through computational analysis of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ontology objects for all the biological ontologies being used.\n",
    "pato = Ontology(pato_filename)\n",
    "po = Ontology(po_filename)\n",
    "go = Ontology(go_filename)\n",
    "\n",
    "# Run the NOBLE Coder annotator over the raw input text descriptions, which handles things like case normalization.\n",
    "direct_annots_nc_go_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"go\", precise=1)\n",
    "direct_annots_nc_go_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"go\", precise=0)\n",
    "direct_annots_nc_po_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"po\", precise=1)\n",
    "direct_annots_nc_po_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"po\", precise=0)\n",
    "direct_annots_nc_pato_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"pato\", precise=1)\n",
    "direct_annots_nc_pato_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"pato\", precise=0)\n",
    "\n",
    "# Use the ontology hierarchies to add terms that are inherited by the terms that were annotated to the text.\n",
    "inherited_annots_nc_go_precise = {i:go.inherited(term_id_list) for i,term_id_list in direct_annots_nc_go_precise.items()}\n",
    "inherited_annots_nc_go_partial = {i:go.inherited(term_id_list) for i,term_id_list in direct_annots_nc_go_partial.items()}\n",
    "inherited_annots_nc_po_precise = {i:po.inherited(term_id_list) for i,term_id_list in direct_annots_nc_po_precise.items()}\n",
    "inherited_annots_nc_po_partial = {i:po.inherited(term_id_list) for i,term_id_list in direct_annots_nc_po_partial.items()}\n",
    "inherited_annots_nc_pato_precise = {i:pato.inherited(term_id_list) for i,term_id_list in direct_annots_nc_pato_precise.items()}\n",
    "inherited_annots_nc_pato_partial = {i:pato.inherited(term_id_list) for i,term_id_list in direct_annots_nc_pato_partial.items()}\n",
    "\n",
    "# Merge the ontology term annotations for each descritpion into a single dictionary for the precise and partial levels.\n",
    "all_precise_annotations = {i:flatten([inherited_annots_nc_go_precise[i],inherited_annots_nc_po_precise[i],inherited_annots_nc_pato_precise[i]]) for i in descriptions.keys()}\n",
    "all_partial_annotations = {i:flatten([inherited_annots_nc_go_partial[i],inherited_annots_nc_po_partial[i],inherited_annots_nc_pato_partial[i]]) for i in descriptions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create description strings with all ontology term anntotations concatenated to the end of the descriptions.\n",
    "descriptions_with_precise_terms = {i:\" \".join(flatten([text,all_precise_annotations[i]])) for i,text in descriptions.items()}\n",
    "descriptions_with_partial_terms = {i:\" \".join(flatten([text,all_partial_annotations[i]])) for i,text in descriptions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ontology term annotations dictionaries for all the high confidence annotations present in the dataset.\n",
    "curated_go_annotations = dataset.get_annotations_dictionary(\"go\")\n",
    "curated_po_annotations = dataset.get_annotations_dictionary(\"po\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phenes\"></a>\n",
    "### Splitting the descriptions into individual phenes\n",
    "As a preprocessing step, split into a new set of descriptions that's larger. Note that phenotypes are split into phenes, and the phenes that are identical are retained as separate entries in the dataset. This makes the distance matrix calculation more needlessly expensive, because vectors need to be found for the same string more than once, but it simplifies converting the edgelist back to having IDs that reference the genes (full phenotypes) instead of the smaller phenes. If anything, that problem should be addressed in the pairwise functions, not here. (The package should handle it, not when creating input data for those methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of phene descriptions and a dictionary to convert back to the phenotype/gene IDs.\n",
    "phenes = {}\n",
    "phene_id_to_id = {}\n",
    "phene_id = 0\n",
    "for i,phene_list in {i:sent_tokenize(d) for i,d in descriptions.items()}.items():\n",
    "    for phene in phene_list:\n",
    "        phenes[phene_id] = phene\n",
    "        phene_id_to_id[phene_id] = i\n",
    "        phene_id = phene_id+1\n",
    "        \n",
    "# Repeating the reprocessing options for the individual phenes instead of the full phenotype descriptions.\n",
    "phenes_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes.items()}\n",
    "phenes_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes.items()}\n",
    "phenes_no_stopwords = {i:remove_stopwords(d) for i,d in phenes.items()}\n",
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "phenes_noun_only =  {i:get_pos_tokens(d,\"NN\") for i,d in phenes.items()}\n",
    "phenes_noun_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes_noun_only.items()}\n",
    "phenes_noun_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes_noun_only.items()}\n",
    "phenes_adj_only =  {i:get_pos_tokens(d,\"JJ\") for i,d in phenes.items()}\n",
    "phenes_adj_only_full_preprocessing = {i:\" \".join(preprocess_string(d)) for i,d in phenes_adj_only.items()}\n",
    "phenes_adj_only_simple_preprocessing = {i:\" \".join(simple_preprocess(d)) for i,d in phenes_adj_only.items()}\n",
    "phenes_noun_adj = {i:\"{} {}\".format(phenes_noun_only[i],phenes_adj_only[i]) for i in phenes.keys()}\n",
    "phenes_noun_adj_full_preprocessing = {i:\"{} {}\".format(phenes_noun_only_full_preprocessing[i],phenes_adj_only_full_preprocessing[i]) for i in phenes.keys()}\n",
    "phenes_noun_adj_simple_preprocessing = {i:\"{} {}\".format(phenes_noun_only_simple_preprocessing[i],phenes_adj_only_simple_preprocessing[i]) for i in phenes.keys()}\n",
    "\n",
    "# Repeating the vocbulary reduction step using the individual phenes instead of full phenotype descriptions.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in phenes_simple_preprocessing.values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.pairwise_square_word2vec(word2vec_model, tokens_dict, \"cosine\")\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "threshold = 0.2\n",
    "phenes_linares_pontes, reduce_lp, unreduce_lp = reduce_vocab_linares_pontes(phenes_simple_preprocessing, tokens, distance_matrix, n)\n",
    "phenes_connected_components, reduce_cc, unreduce_cc = reduce_vocab_connected_components(phenes_simple_preprocessing, tokens, distance_matrix, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_4\"></a>\n",
    "# Part 4. Generating vector representations and pairwise distances matrices\n",
    "This section uses the text descriptions, preprocessed text descriptions, or ontology term annotations created or read in the previous sections to generate a vector representation for each gene and build a pairwise distance matrix for the whole dataset. Each method specified is a unique combination of a method of vectorization (bag-of-words, n-grams, document embedding model, etc) and distance metric (Euclidean, Jaccard, cosine, etc) applied to those vectors in constructing the pairwise matrix. The method of vectorization here is equivalent to feature selection, so the task is to figure out which type of vectors will encode features that are useful (n-grams, full words, only words from a certain vocabulary, etc).\n",
    "\n",
    "<a id=\"methods\"></a>\n",
    "### Specifying a list of NLP methods to use\n",
    "Something here if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_methods = [\n",
    "\n",
    "\n",
    "    # Full phenotype descriptions\n",
    "\n",
    "\n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    Method(\"Doc2Vec\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Doc2Vec\", \"PubMed,Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Max,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean,Linares Pontes\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Max,Linares Pontes\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "\n",
    "    #Method(\"BERT\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \" Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "\n",
    "\n",
    "    # Methods that use topic models to generate embeddings.\n",
    "    Method(\"Topic Models\", \"LDA,Simple,Topics=20\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":20, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Simple,Topics=50\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Simple,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Simple,Topics=200\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":200, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Full,Topics=20\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":20, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Full,Topics=50\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Full,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Full,Topics=200\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":200, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "\n",
    "    Method(\"Topic Models\", \"NMF,Simple,Topics=20\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":20, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Simple,Topics=50\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Simple,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Simple,Topics=200\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"num_topics\":200, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Full,Topics=20\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":20, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Full,Topics=50\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Full,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Full,Topics=200\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":200, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach selecting for specific parts-of-speech (includes stemming).\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Nouns,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use variations on the n-grams approach with a reduced vocabulary size and simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "\n",
    "\n",
    "    # Methods that use terms inferred from automated annotation of the text.\n",
    "    Method(\"NOBLE Coder\", \"Precise\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"NOBLE Coder\", \"Partial\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"NOBLE Coder\", \"Precise,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "\n",
    "    # Methods that use terms assigned by humans that are present in the dataset.\n",
    "    Method(\"GO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":go_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"PO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":po_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Individual phenes from those larger phenotypes.\n",
    "\n",
    "\n",
    "    # Approaches were the phenotype descriptions were split into individual phenes first (computationally expensive).\n",
    "    Method(\"Doc2Vec (Phenes)\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Doc2Vec (Phenes)\", \"PubMed,Size=100\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_pubmed_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Mean\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Max\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Mean,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Max,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Mean,Linares Pontes\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Max,Linares Pontes\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=2,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=3,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BERT (Phenes)\", \"Base:Layers=4,Summed\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=2,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pmc, \"tokenizer\":bert_tokenizer_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PubMed,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed, \"tokenizer\":bert_tokenizer_pubmed, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"BioBERT (Phenes)\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with full preprocessing (includes stemming).\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,2-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_simple_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach selecting for specific parts-of-speech (includes stemming).\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_adj_only_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,Adjectives,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,Adjectives,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,Adjectives,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Full,Nouns,Adjectives,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_noun_adj_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "    # Methods that use variations on the N-Grams (Phenes) approach with a reduced vocabulary size and simple preprocessing (no stemming).\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,Linares Pontes,1-grams\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,Linares Pontes,1-grams,Binary\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":False}, spatial.distance.jaccard, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,Linares Pontes,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    Method(\"N-Grams (Phenes)\", \"Simple,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_methods = [\n",
    "    # Curation-based methods\n",
    "    #Method(\"GO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":go_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #Method(\"PO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":po_annotations, \"ontology\":ontology, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    # Phenotype descriptions\n",
    "    Method(\"Doc2Vec\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=3,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    # Phene descriptions\n",
    "    #Method(\"Doc2Vec (Phenes)\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"Word2Vec (Phenes)\", \"Wikipedia,Size=300,Mean,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":phenes_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "    #Method(\"N-Grams (Phenes)\", \"Simple,Words,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":phenes_full_preprocessing, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine, tag=\"phenes\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    # Curation-based methods\n",
    "    #Method(\"GO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":go_annotations, \"ontology\":go, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    Method(\"PO\", \"None\", pw.pairwise_square_annotations, {\"ids_to_annotations\":curated_po_annotations, \"ontology\":po, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    # Phenotype descriptions\n",
    "    Method(\"Doc2Vec\", \"Wikipedia,Size=300\", pw.pairwise_square_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    Method(\"Word2Vec\", \"Wikipedia,Size=300,Mean,Nouns,Adjectives\", pw.pairwise_square_word2vec, {\"model\":word2vec_model, \"ids_to_texts\":descriptions_noun_adj_simple_preprocessing, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    #Method(\"BERT\", \"Base:Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\", pw.pairwise_square_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Simple,Words,Linares Pontes,1-grams,Binary,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_linares_pontes, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"N-Grams\", \"Full,Words,1-grams,TFIDF\", pw.pairwise_square_ngrams, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #Method(\"NOBLE Coder\", \"Partial,TFIDF\", pw.pairwise_square_annotations, {\"ids_to_annotations\":annotations_noblecoder_partial, \"ontology\":po, \"binary\":True, \"metric\":\"cosine\", \"tfidf\":True}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"LDA,Full,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"lda\"}, spatial.distance.cosine),\n",
    "    Method(\"Topic Models\", \"NMF,Full,Topics=100\", pw.pairwise_square_topic_model, {\"ids_to_texts\":descriptions_full_preprocessing, \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"nmf\"}, spatial.distance.cosine),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running\"></a>\n",
    "### Running all of the methods to generate distance matrices\n",
    "Notes- Instead of passing in similarity function like cosine distance that will get evaluated for every possible i,j pair of vetors that are created (this is very big when splitting by phenes), don't use a specific similarity function, but instead let the object use a KNN classifier. pass in some limit for k like 100. then the object uses some more efficient (not brute force) algorithm to set the similarity of some vector v to its 100 nearest neighbors as those 100 probabilities, and sets everything else to 0. This would need to be implemented as a matching but separate function from the get_square_matrix_from_vectors thing. And then this would need to be noted in the similarity function that was used for these in the big table of methods. This won't work because the faster (not brute force algorithms) are not for sparse vectors like n-grams, and the non-sparse embeddings aren't really the problem here because those vectors are relatively much short, even when concatenating BERT encoder layers thats only up to around length of ~1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO:None                                                                00:00:00\n",
      "Doc2Vec:Wikipedia,Size=300                                             00:00:00\n",
      "Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives                      00:00:00\n",
      "N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF               00:00:00\n",
      "N-Grams:Full,Words,1-grams,TFIDF                                       00:00:00\n",
      "Topic Models:LDA,Full,Topics=100                                       00:00:00\n",
      "Topic Models:NMF,Full,Topics=100                                       00:00:04\n"
     ]
    }
   ],
   "source": [
    "# Generate all the pairwise distance matrices (not in parallel).\n",
    "graphs = {}\n",
    "names = []\n",
    "durations = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    names.append(method.name_with_hyperparameters)\n",
    "    durations.append(to_hms(duration))\n",
    "    print(\"{:70} {}\".format(method.name_with_hyperparameters,to_hms(duration)))\n",
    "durations_df = pd.DataFrame({\"method\":names,\"duration\":durations})\n",
    "durations_df.to_csv(os.path.join(OUTPUT_DIR,\"part_4_durations.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "### Merging all of the distance matrices into a single dataframe specifying edges\n",
    "This section also handles replacing IDs from the individual methods that are references individual phenes that are part of a larger phenotype, and replacing those IDs with IDs referencing the full phenotypes (one-to-one relationship between phenotypes and genes). In this case, the minimum distance found between any two phenes from those two phenotypes represents the distance between that pair of phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100\n",
       "1    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999\n",
       "2    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969\n",
       "3    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465\n",
       "4    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325\n",
       "5    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572\n",
       "6      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649\n",
       "7    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000\n",
       "8    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466\n",
       "9    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665\n",
       "10   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476\n",
       "11   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113\n",
       "12   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642\n",
       "13   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553\n",
       "14   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000\n",
       "15   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197\n",
       "16   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000\n",
       "17   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999\n",
       "18   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000\n",
       "19   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933\n",
       "20   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging all the edgelists together.\n",
    "metric_dict = {method.name_with_hyperparameters:method.metric for method in methods}\n",
    "tags_dict = {method.name_with_hyperparameters:method.tag for method in methods}\n",
    "names = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "\n",
    "# Modify the edgelists for the methods that were using a phene split.\n",
    "for name,edgelist in edgelists.items():\n",
    "    # Converting phene IDs back to phenotype (gene) IDs where applicable.\n",
    "    if \"phene\" in tags_dict[name]:\n",
    "        edgelist[\"from\"] = edgelist[\"from\"].map(lambda x: phene_id_to_id[x])\n",
    "        edgelist[\"to\"] = edgelist[\"to\"].map(lambda x: phene_id_to_id[x])\n",
    "        edgelist = edgelist.groupby([\"from\",\"to\"], as_index=False).min()\n",
    "    # Making sure the edges are listed with the nodes sorted consistently.\n",
    "    cond = edgelist[\"from\"] > edgelist[\"to\"]\n",
    "    edgelist.loc[cond, ['from', 'to']] = edgelist.loc[cond, ['to', 'from']].values\n",
    "    edgelists[name] = edgelist\n",
    "\n",
    "# Do the merge step and remove self edges from the full dataframe.\n",
    "df = merge_edgelists(edgelists, default_value=1.000)\n",
    "df = remove_self_loops(df)\n",
    "df[\"from\"] = df[\"from\"].astype(\"int64\")\n",
    "df[\"to\"] = df[\"to\"].astype(\"int64\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "### Combining multiple distances measurements into summarizing distance values\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating indicating how likely is it that two genes share atleast one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean\n",
       "1    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148\n",
       "2    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365\n",
       "3    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136\n",
       "4    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103\n",
       "5    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568\n",
       "6      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997\n",
       "7    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726\n",
       "8    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625\n",
       "9    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046\n",
       "10   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312\n",
       "11   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237\n",
       "12   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227\n",
       "13   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490\n",
       "14   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523\n",
       "15   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417\n",
       "16   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274\n",
       "17   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932\n",
       "18   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823\n",
       "19   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674\n",
       "20   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average distance percentile as a means of combining multiple scores.\n",
    "name = \"Mean\"\n",
    "names_to_use_for_mean = [name for name in names if not name in [\"GO:None\",\"PO:None\"]]\n",
    "df[name] = df[names_to_use_for_mean].rank(pct=True).mean(axis=1)\n",
    "names.append(name)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO:None\n",
      "Doc2Vec:Wikipedia,Size=300\n",
      "Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives\n",
      "N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF\n",
      "N-Grams:Full,Words,1-grams,TFIDF\n",
      "Topic Models:LDA,Full,Topics=100\n",
      "Topic Models:NMF,Full,Topics=100\n",
      "Mean\n"
     ]
    }
   ],
   "source": [
    "# Normalizing all of the array representations of the graphs so they can be combined. Then this version of the arrays\n",
    "# should be used by any other cells that need all of the arrays, rather than the arrays accessed from the graph\n",
    "# objects. This is necessary for this analysis because some of the graph objects refer to phene datasets not\n",
    "# phenotype datasets.\n",
    "name_to_array = {}\n",
    "ids = list(descriptions.keys())\n",
    "n = len(descriptions)\n",
    "id_to_array_index = {i:idx for idx,i in enumerate(ids)}\n",
    "array_index_to_id = {idx:i for i,idx in id_to_array_index.items()}\n",
    "for name in names:\n",
    "    print(name)\n",
    "    idx = list(df.columns).index(name)+1\n",
    "    arr = np.ones((n, n))\n",
    "    for row in df.itertuples():\n",
    "        arr[id_to_array_index[row[1]]][id_to_array_index[row[2]]] = row[idx]\n",
    "        arr[id_to_array_index[row[2]]][id_to_array_index[row[1]]] = row[idx]\n",
    "    np.fill_diagonal(arr, 0.000) \n",
    "    name_to_array[name] = arr    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding correlations between human and computational approaches for hand-picked phenotype pairs\n",
    "This is only meant to be run in the context of the notebook, and should never be used in the script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE = 1\n",
    "if OBJECTIVE == 2:\n",
    "#if OBJECTIVE not in (\"phenotypes\", \"pathways\", \"interactions\", \"orthologs\"):\n",
    "    small_table = defaultdict(dict)\n",
    "    for name in names:\n",
    "        values = []\n",
    "        scores = []\n",
    "        for tup,score in pair_to_score.items():\n",
    "            i = id_to_array_index[tup[0]]\n",
    "            j = id_to_array_index[tup[1]]\n",
    "            value = 1 - name_to_array[name][i,j]\n",
    "            values.append(value)\n",
    "            scores.append(score)\n",
    "        rho,pval = spearmanr(values,scores)\n",
    "        small_table[name] = {\"rho\":rho,\"pval\":pval}\n",
    "    pd.DataFrame(small_table).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_6\"></a>\n",
    "# Part 5. Supervised Tasks\n",
    "\n",
    "\n",
    "# todo, read in all the groupings stuff here, or use it from previous cells, but be sure about which ones are which ie whats actually being re-used here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "### Option 1: Merging in the previously curated similarity values from the Oellrich, Walls et al. (2015) dataset\n",
    "This section reads in a file that contains the previously calculated distance values from the Oellrich, Walls et al. (2015) dataset, and merges it with the values which are obtained here for all of the applicable natural language processing or machine learning methods used, so that the graphs which are specified by these sets of distances values can be evaluated side by side in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>216</td>\n",
       "      <td>763</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.517209</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.642482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>216</td>\n",
       "      <td>2481</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.408551</td>\n",
       "      <td>0.265242</td>\n",
       "      <td>0.883737</td>\n",
       "      <td>0.969690</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>0.875735</td>\n",
       "      <td>0.180045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>216</td>\n",
       "      <td>1575</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.401557</td>\n",
       "      <td>0.301706</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.966512</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.993995</td>\n",
       "      <td>0.261654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>216</td>\n",
       "      <td>4819</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.528835</td>\n",
       "      <td>0.688482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979254</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.693337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>216</td>\n",
       "      <td>4964</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.502349</td>\n",
       "      <td>0.290875</td>\n",
       "      <td>0.913450</td>\n",
       "      <td>0.860322</td>\n",
       "      <td>0.937727</td>\n",
       "      <td>0.889797</td>\n",
       "      <td>0.248636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>216</td>\n",
       "      <td>2652</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.459515</td>\n",
       "      <td>0.144683</td>\n",
       "      <td>0.846894</td>\n",
       "      <td>0.933842</td>\n",
       "      <td>0.832370</td>\n",
       "      <td>0.946273</td>\n",
       "      <td>0.186499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>216</td>\n",
       "      <td>831</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.517418</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.642605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>216</td>\n",
       "      <td>2641</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.498490</td>\n",
       "      <td>0.428773</td>\n",
       "      <td>0.988391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986764</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.593865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>216</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.350501</td>\n",
       "      <td>0.252314</td>\n",
       "      <td>0.813934</td>\n",
       "      <td>0.897907</td>\n",
       "      <td>0.763645</td>\n",
       "      <td>0.933441</td>\n",
       "      <td>0.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>216</td>\n",
       "      <td>2258</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.460769</td>\n",
       "      <td>0.317187</td>\n",
       "      <td>0.869624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean\n",
       "1    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148\n",
       "2    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365\n",
       "3    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136\n",
       "4    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103\n",
       "5    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568\n",
       "6      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997\n",
       "7    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726\n",
       "8    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625\n",
       "9    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046\n",
       "10   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312\n",
       "11   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237\n",
       "12   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227\n",
       "13   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490\n",
       "14   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523\n",
       "15   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417\n",
       "16   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274\n",
       "17   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932\n",
       "18   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823\n",
       "19   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674\n",
       "20   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477\n",
       "21   216   763  0.945652                    0.517209                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.642482\n",
       "22   216  2481  0.099010                    0.408551                                           0.265242                                           0.883737                                 0.969690                          0.708483                          0.875735  0.180045\n",
       "23   216  1575  0.367347                    0.401557                                           0.301706                                           0.938601                                 0.966512                          0.807560                          0.993995  0.261654\n",
       "24   216  4819  0.042105                    0.528835                                           0.688482                                           1.000000                                 1.000000                          0.979254                          0.999998  0.693337\n",
       "25   216  4964  0.061224                    0.502349                                           0.290875                                           0.913450                                 0.860322                          0.937727                          0.889797  0.248636\n",
       "26   216  2652  0.042105                    0.459515                                           0.144683                                           0.846894                                 0.933842                          0.832370                          0.946273  0.186499\n",
       "27   216   831  0.086957                    0.517418                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.642605\n",
       "28   216  2641  0.666667                    0.498490                                           0.428773                                           0.988391                                 1.000000                          0.986764                          0.999308  0.593865\n",
       "29   216  2402  0.010870                    0.350501                                           0.252314                                           0.813934                                 0.897907                          0.763645                          0.933441  0.103200\n",
       "30   216  2258  0.031915                    0.460769                                           0.317187                                           0.869624                                 1.000000                          0.984053                          1.000000  0.466052"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathways (both KEGG and PlantCyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y\n",
       "1    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1\n",
       "2    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0\n",
       "3    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1\n",
       "4    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0\n",
       "5    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1\n",
       "6      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1\n",
       "7    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0\n",
       "8    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0\n",
       "9    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1\n",
       "10   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1\n",
       "11   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0\n",
       "12   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1\n",
       "13   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0\n",
       "14   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1\n",
       "15   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1\n",
       "16   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1\n",
       "17   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1\n",
       "18   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1\n",
       "19   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1\n",
       "20   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair mapped to a pathway resource.\n",
    "pathway_mapped_ids = set(kegg_mapped_ids+pmn_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"pwy\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"pwy_y\"] = -1\n",
    "id_to_kegg_group_ids, kegg_group_id_to_ids = kegg_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_pmn_group_ids, pmn_group_id_to_ids = pmn_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_group_ids = {i:flatten([id_to_kegg_group_ids[i],id_to_pmn_group_ids[i]]) for i in dataset.get_ids()}\n",
    "df.loc[(df[\"pwy\"]==True),\"pwy_y\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype Subsets from the Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y\n",
       "1    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0\n",
       "2    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0\n",
       "3    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1\n",
       "4    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0\n",
       "5    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1\n",
       "6      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1\n",
       "7    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0\n",
       "8    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1\n",
       "9    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1\n",
       "10   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0\n",
       "11   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0\n",
       "12   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1\n",
       "13   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0\n",
       "14   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1\n",
       "15   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0\n",
       "16   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1\n",
       "17   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0\n",
       "18   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0\n",
       "19   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0\n",
       "20   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(subsets_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"phe\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"phe_y\"] = -1\n",
    "id_to_group_ids,_ = phe_subsets_groups.get_groupings_for_dataset(dataset)\n",
    "df.loc[(df[\"phe\"]==True),\"phe_y\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y\n",
       "0    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0\n",
       "1    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0\n",
       "2    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0\n",
       "3    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0\n",
       "4    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0\n",
       "5      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0\n",
       "6    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0\n",
       "7    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0\n",
       "8    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0\n",
       "9    216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0\n",
       "10   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0   True    0.0\n",
       "11   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1   True    0.0\n",
       "12   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0   True    0.0\n",
       "13   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1   True    0.0\n",
       "14   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0   True    0.0\n",
       "15   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1   True    0.0\n",
       "16   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0  False   -1.0\n",
       "17   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0  False   -1.0\n",
       "18   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0  False   -1.0\n",
       "19   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1   True    0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(string_edgelist.ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"ppi\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"ppi_y\"] = -1\n",
    "df = df.merge(right=string_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df[\"combined_score\"].fillna(value=0, inplace=True)\n",
    "df.loc[(df[\"ppi\"]==True),\"ppi_y\"] = df[\"combined_score\"]\n",
    "\n",
    "# Convert all the positive values from string on range 0 to arbitrary n to be equal to 1.\n",
    "df.loc[df[\"ppi_y\"] >= 1, \"ppi_y\"] = 1 \n",
    "df.drop(labels=[\"combined_score\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "      <th>ort</th>\n",
       "      <th>ort_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y    ort  ort_y\n",
       "0    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0  False   -1.0\n",
       "1    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0  False   -1.0\n",
       "2    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0  False   -1.0\n",
       "3    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0  False   -1.0\n",
       "4    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0  False   -1.0\n",
       "5      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0  False   -1.0\n",
       "6    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0  False   -1.0\n",
       "7    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0  False   -1.0\n",
       "8    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0  False   -1.0\n",
       "9    216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0  False   -1.0\n",
       "10   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0   True    0.0  False   -1.0\n",
       "11   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1   True    0.0  False   -1.0\n",
       "12   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0   True    0.0  False   -1.0\n",
       "13   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1   True    0.0  False   -1.0\n",
       "14   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0   True    0.0  False   -1.0\n",
       "15   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1   True    0.0  False   -1.0\n",
       "16   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0  False   -1.0  False   -1.0\n",
       "17   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0  False   -1.0  False   -1.0\n",
       "18   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0  False   -1.0  False   -1.0\n",
       "19   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1   True    0.0  False   -1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(panther_edgelist.ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"ort\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"ort_y\"] = -1\n",
    "df = df.merge(right=panther_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df[\"value\"].fillna(value=0, inplace=True)\n",
    "df.loc[(df[\"ort\"]==True),\"ort_y\"] = df[\"value\"]\n",
    "df.drop(labels=[\"value\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "df.head(20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "      <th>ort</th>\n",
       "      <th>ort_y</th>\n",
       "      <th>eqs</th>\n",
       "      <th>eqs_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y    ort  ort_y    eqs  eqs_y\n",
       "0    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0  False   -1.0   True    1.0\n",
       "1    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0  False   -1.0   True    1.0\n",
       "2    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "3    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0  False   -1.0   True    1.0\n",
       "4    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "5      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0  False   -1.0   True    1.0\n",
       "6    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0  False   -1.0   True    1.0\n",
       "7    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "8    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "9    216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0  False   -1.0   True    1.0\n",
       "10   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0   True    0.0  False   -1.0   True    1.0\n",
       "11   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "12   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0   True    0.0  False   -1.0   True    1.0\n",
       "13   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "14   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0   True    0.0  False   -1.0   True    1.0\n",
       "15   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0\n",
       "16   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0\n",
       "17   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0\n",
       "18   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0  False   -1.0  False   -1.0   True    0.9\n",
       "19   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1   True    0.0  False   -1.0  False   -1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to all the curation types.\n",
    "relevant_ids = set(ow_edgelist.ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"eqs\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"eqs_y\"] = -1\n",
    "df = df.merge(right=ow_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df[\"value\"].fillna(value=0, inplace=True)\n",
    "df.loc[(df[\"eqs\"]==True),\"eqs_y\"] = 1-df[\"value\"]\n",
    "df.drop(labels=[\"value\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "df.head(20)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "      <th>ort</th>\n",
       "      <th>ort_y</th>\n",
       "      <th>eqs</th>\n",
       "      <th>eqs_y</th>\n",
       "      <th>curated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y    ort  ort_y    eqs  eqs_y  curated\n",
       "0    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0  False   -1.0   True    1.0     True\n",
       "1    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0  False   -1.0   True    1.0     True\n",
       "2    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "3    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True\n",
       "4    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "5      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0  False   -1.0   True    1.0     True\n",
       "6    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True\n",
       "7    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "8    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "9    216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True\n",
       "10   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0   True    0.0  False   -1.0   True    1.0     True\n",
       "11   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "12   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0   True    0.0  False   -1.0   True    1.0     True\n",
       "13   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "14   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True\n",
       "15   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False\n",
       "16   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0    False\n",
       "17   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0    False\n",
       "18   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0  False   -1.0  False   -1.0   True    0.9     True\n",
       "19   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1   True    0.0  False   -1.0  False   -1.0    False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to all the curation types.\n",
    "relevant_ids = set(ids_with_all_annotations)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"curated\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "df.head(20)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "      <th>ort</th>\n",
       "      <th>ort_y</th>\n",
       "      <th>eqs</th>\n",
       "      <th>eqs_y</th>\n",
       "      <th>curated</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y    ort  ort_y    eqs  eqs_y  curated   same\n",
       "0   216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "1   216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "2   216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "3   216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "4   216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "5     4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0  False   -1.0   True    1.0     True  False\n",
       "6   216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "7   216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "8   216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "9   216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>PO:None</th>\n",
       "      <th>Doc2Vec:Wikipedia,Size=300</th>\n",
       "      <th>Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives</th>\n",
       "      <th>N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</th>\n",
       "      <th>N-Grams:Full,Words,1-grams,TFIDF</th>\n",
       "      <th>Topic Models:LDA,Full,Topics=100</th>\n",
       "      <th>Topic Models:NMF,Full,Topics=100</th>\n",
       "      <th>Mean</th>\n",
       "      <th>pwy</th>\n",
       "      <th>pwy_y</th>\n",
       "      <th>phe</th>\n",
       "      <th>phe_y</th>\n",
       "      <th>ppi</th>\n",
       "      <th>ppi_y</th>\n",
       "      <th>ort</th>\n",
       "      <th>ort_y</th>\n",
       "      <th>eqs</th>\n",
       "      <th>eqs_y</th>\n",
       "      <th>curated</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>796</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.535259</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651148</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>1422</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.433346</td>\n",
       "      <td>0.392821</td>\n",
       "      <td>0.968872</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.989451</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.468365</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>4105</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.458357</td>\n",
       "      <td>0.266936</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>0.971496</td>\n",
       "      <td>0.938106</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591327</td>\n",
       "      <td>0.467373</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.929544</td>\n",
       "      <td>0.979133</td>\n",
       "      <td>0.823325</td>\n",
       "      <td>0.373103</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.912660</td>\n",
       "      <td>0.968155</td>\n",
       "      <td>0.959890</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.202568</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.455354</td>\n",
       "      <td>0.246194</td>\n",
       "      <td>0.946631</td>\n",
       "      <td>0.982502</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>216</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.977752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>5758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348207</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.835393</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.177625</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216</td>\n",
       "      <td>4488</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.482233</td>\n",
       "      <td>0.535111</td>\n",
       "      <td>0.952181</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.983954</td>\n",
       "      <td>0.814665</td>\n",
       "      <td>0.406046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>1613</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.454569</td>\n",
       "      <td>0.446007</td>\n",
       "      <td>0.951457</td>\n",
       "      <td>0.899383</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.946476</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>216</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.428026</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.900719</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.786652</td>\n",
       "      <td>0.897113</td>\n",
       "      <td>0.162237</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.187550</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.907215</td>\n",
       "      <td>0.895581</td>\n",
       "      <td>0.961642</td>\n",
       "      <td>0.169227</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>216</td>\n",
       "      <td>5610</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.369726</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.813427</td>\n",
       "      <td>0.901553</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>216</td>\n",
       "      <td>6171</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.447940</td>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.983178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587523</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>216</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.597025</td>\n",
       "      <td>0.296666</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>0.868236</td>\n",
       "      <td>0.992197</td>\n",
       "      <td>0.399417</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>216</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.592393</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647274</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>216</td>\n",
       "      <td>805</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.537106</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.651932</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>216</td>\n",
       "      <td>1217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439339</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.978254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569823</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>2416</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.321237</td>\n",
       "      <td>0.297715</td>\n",
       "      <td>0.849823</td>\n",
       "      <td>0.863490</td>\n",
       "      <td>0.361316</td>\n",
       "      <td>0.430933</td>\n",
       "      <td>0.076674</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>216</td>\n",
       "      <td>4270</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373014</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.446477</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216</td>\n",
       "      <td>763</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.517209</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.642482</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>216</td>\n",
       "      <td>2481</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.408551</td>\n",
       "      <td>0.265242</td>\n",
       "      <td>0.883737</td>\n",
       "      <td>0.969690</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>0.875735</td>\n",
       "      <td>0.180045</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>216</td>\n",
       "      <td>1575</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.401557</td>\n",
       "      <td>0.301706</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.966512</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.993995</td>\n",
       "      <td>0.261654</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>216</td>\n",
       "      <td>4819</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.528835</td>\n",
       "      <td>0.688482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979254</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.693337</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>216</td>\n",
       "      <td>4964</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.502349</td>\n",
       "      <td>0.290875</td>\n",
       "      <td>0.913450</td>\n",
       "      <td>0.860322</td>\n",
       "      <td>0.937727</td>\n",
       "      <td>0.889797</td>\n",
       "      <td>0.248636</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>216</td>\n",
       "      <td>2652</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.459515</td>\n",
       "      <td>0.144683</td>\n",
       "      <td>0.846894</td>\n",
       "      <td>0.933842</td>\n",
       "      <td>0.832370</td>\n",
       "      <td>0.946273</td>\n",
       "      <td>0.186499</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>216</td>\n",
       "      <td>831</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.517418</td>\n",
       "      <td>0.477813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982037</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.642605</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>216</td>\n",
       "      <td>2641</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.498490</td>\n",
       "      <td>0.428773</td>\n",
       "      <td>0.988391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986764</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>216</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.350501</td>\n",
       "      <td>0.252314</td>\n",
       "      <td>0.813934</td>\n",
       "      <td>0.897907</td>\n",
       "      <td>0.763645</td>\n",
       "      <td>0.933441</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>216</td>\n",
       "      <td>2258</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.460769</td>\n",
       "      <td>0.317187</td>\n",
       "      <td>0.869624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466052</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to   PO:None  Doc2Vec:Wikipedia,Size=300  Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives  N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF  N-Grams:Full,Words,1-grams,TFIDF  Topic Models:LDA,Full,Topics=100  Topic Models:NMF,Full,Topics=100      Mean    pwy  pwy_y    phe  phe_y    ppi  ppi_y    ort  ort_y    eqs  eqs_y  curated   same\n",
       "0    216   796  0.945652                    0.535259                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651148  False     -1   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "1    216  1422  0.869565                    0.433346                                           0.392821                                           0.968872                                 0.987247                          0.989451                          0.989969  0.468365   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "2    216  4105  0.869565                    0.458357                                           0.266936                                           0.937271                                 0.971496                          0.938106                          0.999465  0.333136  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "3    216   402  1.000000                    0.591327                                           0.467373                                           0.912742                                 0.929544                          0.979133                          0.823325  0.373103   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "4    216  4713  0.803922                    0.410462                                           0.224285                                           0.912660                                 0.968155                          0.959890                          0.866572  0.202568  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "5      4   216  0.757576                    0.455354                                           0.246194                                           0.946631                                 0.982502                          0.975380                          0.983649  0.318997  False     -1  False     -1   True    0.0  False   -1.0   True    1.0     True  False\n",
       "6    216  1836  0.956522                    0.479755                                           0.395813                                           0.977752                                 1.000000                          0.986228                          1.000000  0.602726   True      0   True      0  False   -1.0  False   -1.0   True    1.0     True   True\n",
       "7    216  5758  1.000000                    0.348207                                           0.183972                                           0.857923                                 0.835393                          0.989474                          0.694466  0.177625   True      0  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "8    216  4488  0.141509                    0.482233                                           0.535111                                           0.952181                                 0.944082                          0.983954                          0.814665  0.406046  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "9    216  1613  0.823529                    0.454569                                           0.446007                                           0.951457                                 0.899383                          0.941911                          0.946476  0.320312  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "10   216  2024  0.010870                    0.428026                                           0.182588                                           0.900719                                 0.917348                          0.786652                          0.897113  0.162237   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "11   216  5052  0.927835                    0.424005                                           0.187550                                           0.875390                                 0.907215                          0.895581                          0.961642  0.169227  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "12   216  5610  0.054348                    0.369726                                           0.300957                                           0.944029                                 0.962778                          0.813427                          0.901553  0.202490   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "13   216  6171  0.728261                    0.447940                                           0.458443                                           0.983178                                 1.000000                          0.983992                          1.000000  0.587523  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "14   201   216  0.042105                    0.597025                                           0.296666                                           0.957158                                 0.980503                          0.868236                          0.992197  0.399417  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "15   216  4451  0.880435                    0.592393                                           0.369549                                           1.000000                                 1.000000                          0.979263                          1.000000  0.647274  False     -1  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "16   216   805  0.945652                    0.537106                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.651932  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0    False   True\n",
       "17   216  1217  1.000000                    0.439339                                           0.398305                                           0.978254                                 1.000000                          0.986058                          1.000000  0.569823  False     -1   True      0  False   -1.0  False   -1.0  False   -1.0    False   True\n",
       "18   216  2416  0.880435                    0.321237                                           0.297715                                           0.849823                                 0.863490                          0.361316                          0.430933  0.076674  False     -1   True      0  False   -1.0  False   -1.0   True    0.9     True   True\n",
       "19   216  4270  0.090000                    0.469262                                           0.456249                                           0.912177                                 1.000000                          0.373014                          0.999720  0.446477   True      0  False     -1   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "20   216   763  0.945652                    0.517209                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.642482   True      0   True      0  False   -1.0  False   -1.0  False   -1.0    False   True\n",
       "21   216  2481  0.099010                    0.408551                                           0.265242                                           0.883737                                 0.969690                          0.708483                          0.875735  0.180045  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "22   216  1575  0.367347                    0.401557                                           0.301706                                           0.938601                                 0.966512                          0.807560                          0.993995  0.261654   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "23   216  4819  0.042105                    0.528835                                           0.688482                                           1.000000                                 1.000000                          0.979254                          0.999998  0.693337  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "24   216  4964  0.061224                    0.502349                                           0.290875                                           0.913450                                 0.860322                          0.937727                          0.889797  0.248636  False     -1   True      1   True    1.0  False   -1.0   True    1.0     True   True\n",
       "25   216  2652  0.042105                    0.459515                                           0.144683                                           0.846894                                 0.933842                          0.832370                          0.946273  0.186499  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "26   216   831  0.086957                    0.517418                                           0.477813                                           1.000000                                 1.000000                          0.982037                          0.999999  0.642605   True      0   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "27   216  2641  0.666667                    0.498490                                           0.428773                                           0.988391                                 1.000000                          0.986764                          0.999308  0.593865  False     -1   True      0   True    0.0  False   -1.0  False   -1.0    False   True\n",
       "28   216  2402  0.010870                    0.350501                                           0.252314                                           0.813934                                 0.897907                          0.763645                          0.933441  0.103200  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True\n",
       "29   216  2258  0.031915                    0.460769                                           0.317187                                           0.869624                                 1.000000                          0.984053                          1.000000  0.466052  False     -1   True      0   True    0.0  False   -1.0   True    1.0     True   True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to make sure that the number of genes and pairs matches what is expected at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for the number of genes and positive and negative pairs that are being used here. Matches table?\n",
    "\n",
    "# How many genes are being used and how many pairs are being used as instances?\n",
    "#num_genes = len(set(df[\"from\"].values).union(set(df[\"to\"].values)))\n",
    "#num_pairs_calc = int(((num_genes**2)-num_genes)/2)\n",
    "#num_pairs_used = df.shape[0]\n",
    "#assert num_pairs_calc == num_pairs_used\n",
    "\n",
    "# What is the class representation?\n",
    "#shared = Counter(df[\"shared\"].values)\n",
    "#num_positives = shared[1]\n",
    "#num_negatives = shared[0]\n",
    "#assert num_pairs_used == num_positives+num_negatives\n",
    "\n",
    "# Write the values that were tested to a file so they could be read later.\n",
    "#tested_values = {\"genes\":num_genes,\n",
    "#                 \"pairs_calc\":num_pairs_calc,\n",
    "#                 \"pairs_used\":num_pairs_used,\n",
    "#                 \"pos\":num_positives,\n",
    "#                 \"neg\":num_negatives,\n",
    "#                }\n",
    "#with open(os.path.join(OUTPUT_DIR,\"part_6_asserts.txt\"), \"w\") as f:\n",
    "#    f.write(str(tested_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a nested dictionary with shape dict[curated][question][species][approach][metric] --> value.\n",
    "curated = [True,False]\n",
    "species = [\"intra\",\"inter\",\"both\"]\n",
    "question = [\"pwy_y\",\"phe_y\",\"ppi_y\",\"ort_y\"]\n",
    "tables = defaultdict(dict)\n",
    "for c,q in itertools.product(curated,question): \n",
    "    tables[c][q] = defaultdict(dict)\n",
    "for c,q,s in itertools.product(curated,question,species): \n",
    "    tables[c][q][s] = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the tables now\n"
     ]
    }
   ],
   "source": [
    "print(\"Making the tables now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44850, 22)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the value of *n* for each type of iteration through a subset of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>curated</th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>num_pairs</th>\n",
       "      <th>positive_fraction</th>\n",
       "      <th>negative_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>67</td>\n",
       "      <td>43</td>\n",
       "      <td>2168</td>\n",
       "      <td>2211</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.980552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>198</td>\n",
       "      <td>204</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>70</td>\n",
       "      <td>49</td>\n",
       "      <td>2366</td>\n",
       "      <td>2415</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.979710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>132</td>\n",
       "      <td>925</td>\n",
       "      <td>7721</td>\n",
       "      <td>8646</td>\n",
       "      <td>0.106986</td>\n",
       "      <td>0.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>132</td>\n",
       "      <td>925</td>\n",
       "      <td>7721</td>\n",
       "      <td>8646</td>\n",
       "      <td>0.106986</td>\n",
       "      <td>0.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>120</td>\n",
       "      <td>402</td>\n",
       "      <td>5826</td>\n",
       "      <td>6228</td>\n",
       "      <td>0.064547</td>\n",
       "      <td>0.935453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>120</td>\n",
       "      <td>402</td>\n",
       "      <td>6738</td>\n",
       "      <td>7140</td>\n",
       "      <td>0.056303</td>\n",
       "      <td>0.943697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>125</td>\n",
       "      <td>191</td>\n",
       "      <td>6845</td>\n",
       "      <td>7036</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>0.972854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>127</td>\n",
       "      <td>17</td>\n",
       "      <td>948</td>\n",
       "      <td>965</td>\n",
       "      <td>0.017617</td>\n",
       "      <td>0.982383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>127</td>\n",
       "      <td>208</td>\n",
       "      <td>7793</td>\n",
       "      <td>8001</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>0.974003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>171</td>\n",
       "      <td>1527</td>\n",
       "      <td>13008</td>\n",
       "      <td>14535</td>\n",
       "      <td>0.105057</td>\n",
       "      <td>0.894943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>171</td>\n",
       "      <td>1527</td>\n",
       "      <td>13008</td>\n",
       "      <td>14535</td>\n",
       "      <td>0.105057</td>\n",
       "      <td>0.894943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>226</td>\n",
       "      <td>928</td>\n",
       "      <td>21897</td>\n",
       "      <td>22825</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.959343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>2826</td>\n",
       "      <td>2826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>227</td>\n",
       "      <td>928</td>\n",
       "      <td>24723</td>\n",
       "      <td>25651</td>\n",
       "      <td>0.036178</td>\n",
       "      <td>0.963822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>584</td>\n",
       "      <td>584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question curated species  num_genes  positive  negative  num_pairs  positive_fraction  negative_fraction\n",
       "0     pwy_y    true   intra         67        43      2168       2211           0.019448           0.980552\n",
       "1     pwy_y    true   inter         70         6       198        204           0.029412           0.970588\n",
       "2     pwy_y    true    both         70        49      2366       2415           0.020290           0.979710\n",
       "3     phe_y    true   intra        132       925      7721       8646           0.106986           0.893014\n",
       "4     phe_y    true   inter          0         0         0          0                NaN                NaN\n",
       "5     phe_y    true    both        132       925      7721       8646           0.106986           0.893014\n",
       "6     ppi_y    true   intra        120       402      5826       6228           0.064547           0.935453\n",
       "7     ppi_y    true   inter        120         0       912        912           0.000000           1.000000\n",
       "8     ppi_y    true    both        120       402      6738       7140           0.056303           0.943697\n",
       "9     ort_y    true   intra         18         0        48         48           0.000000           1.000000\n",
       "10    ort_y    true   inter         19         0       123        123           0.000000           1.000000\n",
       "11    ort_y    true    both         19         0       171        171           0.000000           1.000000\n",
       "12    pwy_y   false   intra        125       191      6845       7036           0.027146           0.972854\n",
       "13    pwy_y   false   inter        127        17       948        965           0.017617           0.982383\n",
       "14    pwy_y   false    both        127       208      7793       8001           0.025997           0.974003\n",
       "15    phe_y   false   intra        171      1527     13008      14535           0.105057           0.894943\n",
       "16    phe_y   false   inter          0         0         0          0                NaN                NaN\n",
       "17    phe_y   false    both        171      1527     13008      14535           0.105057           0.894943\n",
       "18    ppi_y   false   intra        226       928     21897      22825           0.040657           0.959343\n",
       "19    ppi_y   false   inter        227         0      2826       2826           0.000000           1.000000\n",
       "20    ppi_y   false    both        227       928     24723      25651           0.036178           0.963822\n",
       "21    ort_y   false   intra         41         0       277        277           0.000000           1.000000\n",
       "22    ort_y   false   inter         42         0       584        584           0.000000           1.000000\n",
       "23    ort_y   false    both         42         0       861        861           0.000000           1.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_lists = defaultdict(list)\n",
    "for c,q,s in itertools.product(curated,question,species):\n",
    "    \n",
    "    # Subsetting the dataframe to the rows (gene pairs) that are relevant for this particular biological question.\n",
    "    subset = df[df[q] != -1]\n",
    "    if c:\n",
    "        subset = subset[subset[\"curated\"] == True]\n",
    "        \n",
    "        \n",
    "    # Subsetting the dataframe to the rows (gene pairs) where both genes are from the same or different species.\n",
    "    if s == \"intra\":\n",
    "        subset = subset[subset[\"same\"] == True]\n",
    "    elif s == \"inter\":\n",
    "        subset = subset[subset[\"same\"] == False]\n",
    "        \n",
    "        \n",
    "    # Adding values to the table that are specific to this biological question.\n",
    "    counts = Counter(subset[q].values)\n",
    "    \n",
    "    table_lists[\"question\"].append(q.lower())\n",
    "    table_lists[\"curated\"].append(str(c).lower())\n",
    "    table_lists[\"species\"].append(s.lower())\n",
    "    table_lists[\"num_genes\"].append(len(set(subset[\"to\"].values).union(set(subset[\"from\"].values))))\n",
    "    table_lists[\"positive\"].append(counts[1])\n",
    "    table_lists[\"negative\"].append(counts[0])\n",
    "    #table_lists[\"class_ratio\"].append(\"{:0.4f}\".format(counts[1]/counts[0]))\n",
    "\n",
    "pairs_table = pd.DataFrame(table_lists)  \n",
    "pairs_table[\"num_pairs\"] = pairs_table[\"positive\"]+pairs_table[\"negative\"]\n",
    "pairs_table[\"positive_fraction\"] = pairs_table[\"positive\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table[\"negative_fraction\"] = pairs_table[\"negative\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table.to_csv(os.path.join(OUTPUT_DIR,\"pairs_table.csv\"), index=False)\n",
    "pairs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>curated</th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>num_pairs</th>\n",
       "      <th>positive_fraction</th>\n",
       "      <th>negative_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>67</td>\n",
       "      <td>43</td>\n",
       "      <td>2168</td>\n",
       "      <td>2211</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.980552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>198</td>\n",
       "      <td>204</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>70</td>\n",
       "      <td>49</td>\n",
       "      <td>2366</td>\n",
       "      <td>2415</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.979710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>132</td>\n",
       "      <td>925</td>\n",
       "      <td>7721</td>\n",
       "      <td>8646</td>\n",
       "      <td>0.106986</td>\n",
       "      <td>0.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>132</td>\n",
       "      <td>925</td>\n",
       "      <td>7721</td>\n",
       "      <td>8646</td>\n",
       "      <td>0.106986</td>\n",
       "      <td>0.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>120</td>\n",
       "      <td>402</td>\n",
       "      <td>5826</td>\n",
       "      <td>6228</td>\n",
       "      <td>0.064547</td>\n",
       "      <td>0.935453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>120</td>\n",
       "      <td>402</td>\n",
       "      <td>6738</td>\n",
       "      <td>7140</td>\n",
       "      <td>0.056303</td>\n",
       "      <td>0.943697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>125</td>\n",
       "      <td>191</td>\n",
       "      <td>6845</td>\n",
       "      <td>7036</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>0.972854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>127</td>\n",
       "      <td>17</td>\n",
       "      <td>948</td>\n",
       "      <td>965</td>\n",
       "      <td>0.017617</td>\n",
       "      <td>0.982383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>127</td>\n",
       "      <td>208</td>\n",
       "      <td>7793</td>\n",
       "      <td>8001</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>0.974003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>171</td>\n",
       "      <td>1527</td>\n",
       "      <td>13008</td>\n",
       "      <td>14535</td>\n",
       "      <td>0.105057</td>\n",
       "      <td>0.894943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>171</td>\n",
       "      <td>1527</td>\n",
       "      <td>13008</td>\n",
       "      <td>14535</td>\n",
       "      <td>0.105057</td>\n",
       "      <td>0.894943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>226</td>\n",
       "      <td>928</td>\n",
       "      <td>21897</td>\n",
       "      <td>22825</td>\n",
       "      <td>0.040657</td>\n",
       "      <td>0.959343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>2826</td>\n",
       "      <td>2826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>227</td>\n",
       "      <td>928</td>\n",
       "      <td>24723</td>\n",
       "      <td>25651</td>\n",
       "      <td>0.036178</td>\n",
       "      <td>0.963822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>584</td>\n",
       "      <td>584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ort_y</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question curated species  num_genes  positive  negative  num_pairs  positive_fraction  negative_fraction\n",
       "0     pwy_y    true   intra         67        43      2168       2211           0.019448           0.980552\n",
       "1     pwy_y    true   inter         70         6       198        204           0.029412           0.970588\n",
       "2     pwy_y    true    both         70        49      2366       2415           0.020290           0.979710\n",
       "3     phe_y    true   intra        132       925      7721       8646           0.106986           0.893014\n",
       "4     phe_y    true   inter          0         0         0          0                NaN                NaN\n",
       "5     phe_y    true    both        132       925      7721       8646           0.106986           0.893014\n",
       "6     ppi_y    true   intra        120       402      5826       6228           0.064547           0.935453\n",
       "7     ppi_y    true   inter        120         0       912        912           0.000000           1.000000\n",
       "8     ppi_y    true    both        120       402      6738       7140           0.056303           0.943697\n",
       "9     ort_y    true   intra         18         0        48         48           0.000000           1.000000\n",
       "10    ort_y    true   inter         19         0       123        123           0.000000           1.000000\n",
       "11    ort_y    true    both         19         0       171        171           0.000000           1.000000\n",
       "12    pwy_y   false   intra        125       191      6845       7036           0.027146           0.972854\n",
       "13    pwy_y   false   inter        127        17       948        965           0.017617           0.982383\n",
       "14    pwy_y   false    both        127       208      7793       8001           0.025997           0.974003\n",
       "15    phe_y   false   intra        171      1527     13008      14535           0.105057           0.894943\n",
       "16    phe_y   false   inter          0         0         0          0                NaN                NaN\n",
       "17    phe_y   false    both        171      1527     13008      14535           0.105057           0.894943\n",
       "18    ppi_y   false   intra        226       928     21897      22825           0.040657           0.959343\n",
       "19    ppi_y   false   inter        227         0      2826       2826           0.000000           1.000000\n",
       "20    ppi_y   false    both        227       928     24723      25651           0.036178           0.963822\n",
       "21    ort_y   false   intra         41         0       277        277           0.000000           1.000000\n",
       "22    ort_y   false   inter         42         0       584        584           0.000000           1.000000\n",
       "23    ort_y   false    both         42         0       861        861           0.000000           1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_idx_lists = []\n",
    "subset_properties = []\n",
    "\n",
    "\n",
    "\n",
    "table_lists = defaultdict(list)\n",
    "for c,q,s in itertools.product(curated,question,species):\n",
    "    \n",
    "    \n",
    "    subset_properties.append((c,q,s))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Subsetting the dataframe to the rows (gene pairs) that are relevant for this particular biological question.\n",
    "    subset = df[df[q] != -1]\n",
    "    if c:\n",
    "        subset = subset[subset[\"curated\"] == True]\n",
    "        \n",
    "        \n",
    "    # Subsetting the dataframe to the rows (gene pairs) where both genes are from the same or different species.\n",
    "    if s == \"intra\":\n",
    "        subset = subset[subset[\"same\"] == True]\n",
    "    elif s == \"inter\":\n",
    "        subset = subset[subset[\"same\"] == False]\n",
    "        \n",
    "    subset_idx_lists.append(subset.index.to_list())\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Adding values to the table that are specific to this biological question.\n",
    "    counts = Counter(subset[q].values)\n",
    "    \n",
    "    table_lists[\"question\"].append(q.lower())\n",
    "    table_lists[\"curated\"].append(str(c).lower())\n",
    "    table_lists[\"species\"].append(s.lower())\n",
    "    table_lists[\"num_genes\"].append(len(set(subset[\"to\"].values).union(set(subset[\"from\"].values))))\n",
    "    table_lists[\"positive\"].append(counts[1])\n",
    "    table_lists[\"negative\"].append(counts[0])\n",
    "    #table_lists[\"class_ratio\"].append(\"{:0.4f}\".format(counts[1]/counts[0]))\n",
    "\n",
    "pairs_table = pd.DataFrame(table_lists)  \n",
    "pairs_table[\"num_pairs\"] = pairs_table[\"positive\"]+pairs_table[\"negative\"]\n",
    "pairs_table[\"positive_fraction\"] = pairs_table[\"positive\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table[\"negative_fraction\"] = pairs_table[\"negative\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table.to_csv(os.path.join(OUTPUT_DIR,\"pairs_table.csv\"), index=False)\n",
    "pairs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking more at the distributions of target values for each of the biological questions.\n",
    "from scipy.spatial.distance import jaccard\n",
    "row_tuples = []\n",
    "for q1,q2 in itertools.combinations(question, 2):\n",
    "    q1_subset = df[df[q1] != -1]\n",
    "    q2_subset = df[df[q2] != -1]\n",
    "    overlap_subset  = q1_subset[q1_subset[q2] != -1]\n",
    "    q1_num_pairs = q1_subset.shape[0]\n",
    "    q2_num_pairs = q2_subset.shape[0]\n",
    "    overlap_size = overlap_subset.shape[0]\n",
    "    overlap_sim = 1-jaccard(overlap_subset[q1].values, overlap_subset[q2].values)\n",
    "    row_tuples.append((q1, q2, q1_num_pairs, q2_num_pairs, overlap_size, overlap_sim))\n",
    "question_overlaps_table = pd.DataFrame(row_tuples)\n",
    "question_overlaps_table.columns = [\"question_1\", \"question_2\", \"num_pairs_1\", \"num_pairs_2\", \"num_overlap\", \"sim_overlap\"]\n",
    "question_overlaps_table.sort_values(by=\"sim_overlap\", ascending=False, inplace=True)\n",
    "question_overlaps_table.reset_index(inplace=True, drop=True)\n",
    "question_overlaps_table.to_csv(os.path.join(OUTPUT_DIR,\"question_similarity.csv\"), index=False)\n",
    "question_overlaps_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ks\"></a>\n",
    "### Do the edges joining genes that share a group, pathway, or interaction come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation betwene the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ks\n",
      "done with ks\n"
     ]
    }
   ],
   "source": [
    "print(\"ks\")\n",
    "\n",
    "\n",
    "for properties,idxs in zip(subset_properties, subset_idx_lists):\n",
    "    \n",
    "    # Remember the properties for this subset being looked at, and subset the dataframe accordingly.\n",
    "    c,q,s = properties\n",
    "    subset = df.loc[idxs]\n",
    "        \n",
    "    # Check that this subsetting leaves a valid dataset with both positive and negatives samples.\n",
    "    class_values = pd.unique(subset[q].values)\n",
    "    if not (len(class_values)==2 and 0 in class_values and 1 in class_values):\n",
    "        continue\n",
    "    \n",
    "    # Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "    ppi_pos_dict = {name:(subset[subset[q] > 0.00][name].values) for name in names}\n",
    "    ppi_neg_dict = {name:(subset[subset[q] == 0.00][name].values) for name in names}\n",
    "    for name in names:\n",
    "        stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "        pos_mean = np.average(ppi_pos_dict[name])\n",
    "        neg_mean = np.average(ppi_neg_dict[name])\n",
    "        pos_n = len(ppi_pos_dict[name])\n",
    "        neg_n = len(ppi_neg_dict[name])\n",
    "        \n",
    "        tables[c][q][s][name].update({\"mean_1\":pos_mean, \"mean_0\":neg_mean, \"n_1\":pos_n, \"n_0\":neg_n})\n",
    "        tables[c][q][s][name].update({\"ks\":stat, \"ks_pval\":p})\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Show the kernel estimates for each distribution of weights for each method.\n",
    "    #num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "    #fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "    #for name,ax in zip(names,axs.flatten()):\n",
    "    #    ax.set_title(name)\n",
    "    #    ax.set_xlabel(\"value\")\n",
    "    #    ax.set_ylabel(\"density\")\n",
    "    #    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "    #    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "    #fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "    #fig.tight_layout()\n",
    "    #fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_kernel_density.png\"),dpi=400)\n",
    "    #plt.close()\n",
    "print(\"done with ks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"within\"></a>\n",
    "### Looking at within-group or within-pathway distances in each graph\n",
    "The purpose of this section is to determine which methods generated graphs which tightly group genes which share common pathways or group membership with one another. In order to compare across different methods where the distance value distributions are different, the mean distance values for each group for each method are convereted to percentile scores. Lower percentile scores indicate that the average distance value between any two genes that belong to that group is lower than most of the distance values in the entire distribution for that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OBJECTIVE = \"ASDF\"\n",
    "if OBJECTIVE in (\"pathways\", \"phenotypes\"):\n",
    "    \n",
    "    # Get all the average within-pathway phenotype distance values for each method for each particular pathway.\n",
    "    group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "    group_ids = list(group_id_to_ids.keys())\n",
    "    graph = IndexedGraph(df)\n",
    "    within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "    within_percentiles_dict = defaultdict(lambda: defaultdict(list))\n",
    "    all_weights_dict = {}\n",
    "    for name in names:\n",
    "        all_weights_dict[name] = df[name].values\n",
    "        for group in group_ids:\n",
    "            within_ids = group_id_to_ids[group]\n",
    "            within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "            mean_weight = np.mean((graph.get_values(within_pairs, kind=name)))\n",
    "            within_weights_dict[name][group] = mean_weight\n",
    "            within_percentiles_dict[name][group] = stats.percentileofscore(df[name].values, mean_weight, kind=\"rank\")\n",
    "\n",
    "    # Generating a dataframe of percentiles of the mean in-group distance scores.\n",
    "    within_dist_data = pd.DataFrame(within_percentiles_dict)\n",
    "    within_dist_data = within_dist_data.dropna(axis=0, inplace=False)\n",
    "    within_dist_data = within_dist_data.round(4)\n",
    "\n",
    "    # Adding relevant information to this dataframe and saving.\n",
    "    within_dist_data[\"mean_rank\"] = within_dist_data.rank().mean(axis=1)\n",
    "    within_dist_data[\"mean_percentile\"] = within_dist_data.mean(axis=1)\n",
    "    within_dist_data.sort_values(by=\"mean_percentile\", inplace=True)\n",
    "    within_dist_data.reset_index(inplace=True)\n",
    "    within_dist_data[\"group_id\"] = within_dist_data[\"index\"]\n",
    "    within_dist_data[\"full_name\"] = within_dist_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "    within_dist_data[\"n\"] = within_dist_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "    within_dist_data = within_dist_data[flatten([\"group_id\",\"full_name\",\"n\",\"mean_percentile\",\"mean_rank\",names])]\n",
    "    within_dist_data.to_csv(os.path.join(OUTPUT_DIR,\"part_6_within_distances.csv\"), index=False)\n",
    "    within_dist_data.head(5)\n",
    "print(\"done with / skipping the within pathway similarity thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check a version that doesn't use the Indexing thing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auc\"></a>\n",
    "### Predicting whether two genes belong to the same group, pathway, or share an interaction\n",
    "The purpose of this section is to see if whether or not two genes share atleast one common pathway can be predicted from the distance scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(fraction, num_iterations, y_true, y_prob):\n",
    "    # Run the desired number of bootstrap iterations over the full population of predictions and return st devs.\n",
    "    scores = pd.DataFrame([bootstrap_iteration(fraction, y_true, y_prob) for i in range(num_iterations)])\n",
    "    standard_deviations = {\n",
    "        \"f_1_max_std\": np.std(scores[\"f_1_max\"].values),\n",
    "        \"f_2_max_std\": np.std(scores[\"f_2_max\"].values),\n",
    "        \"f_point5_max_std\": np.std(scores[\"f_point5_max\"].values)}\n",
    "    return(standard_deviations)\n",
    "\n",
    "\n",
    "def bootstrap_iteration(fraction, y_true, y_prob):\n",
    "    assert len(y_true) == len(y_prob)\n",
    "    # Subset the total population of predictions using the provided fraction.\n",
    "    num_predictions = len(y_true)\n",
    "    bootstrapping_fraction = fraction\n",
    "    num_to_retain = int(np.ceil(num_predictions*bootstrapping_fraction))\n",
    "    idx = np.random.choice(np.arange(num_predictions), num_to_retain, replace=False)\n",
    "    y_true_sample = y_true[idx]\n",
    "    y_prob_sample = y_prob[idx]\n",
    "    \n",
    "    # Calculate any desired metrics using just that subset.\n",
    "    n_pos, n_neg = Counter(y_true_sample)[1], Counter(y_true_sample)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true_sample, y_prob_sample)\n",
    "    baseline = Counter(y_true_sample)[1]/len(y_true_sample) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    \n",
    "    # Find the maximum F score for different values of .  \n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f_1_scores = f_beta(precision,recall,beta=1)\n",
    "    f_2_scores = f_beta(precision,recall,beta=2)\n",
    "    f_point5_scores = f_beta(precision,recall,beta=0.5)\n",
    "    \n",
    "    # Create a dictionary of those metric values to return.\n",
    "    scores={\"f_1_max\":np.nanmax(f_1_scores),\"f_2_max\":np.nanmax(f_2_scores),\"f_point5_max\":np.nanmax(f_point5_scores)}\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "for properties,idxs in zip(subset_properties, subset_idx_lists):\n",
    "    \n",
    "    # Remember the properties for this subset being looked at, and subset the dataframe accordingly.\n",
    "    c,q,s = properties\n",
    "    subset = df.loc[idxs]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check that this subsetting leaves a valid dataset with both positive and negatives samples.\n",
    "    class_values = pd.unique(subset[q].values)\n",
    "    if not (len(class_values)==2 and 0 in class_values and 1 in class_values):\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    y_true_dict = {name:subset[q].values for name in names}       #just added .values here...\n",
    "    y_prob_dict = {name:(1 - subset[name].values) for name in names}\n",
    "    #num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "    #fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "    for name,ax in zip(names, axs.flatten()):\n",
    "\n",
    "        # Obtaining the values and metrics.\n",
    "        y_true, y_prob = y_true_dict[name], y_prob_dict[name]\n",
    "        n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "        baseline_auc = Counter(y_true)[1]/len(y_true) \n",
    "        area = auc(recall, precision)\n",
    "        auc_to_baseline_auc_ratio = area/baseline_auc\n",
    "        # The baseline F1 max has a precision of the ratio of positives to all samples and a recall of 1.\n",
    "        # This is because a random classifier achieves that precision at all recall values, so recall is maximized to\n",
    "        # find the maximum F1 value that can be expected due to random chance.\n",
    "        baseline_f1_max = (2*baseline_auc*1)/(baseline_auc+1)\n",
    "\n",
    "        #TABLE[name].update({\"auc\":area,\"ratio\":auc_to_baseline_auc_ratio, \"baseline\":baseline_f1_max, })\n",
    "        tables[c][q][s][name].update({\"auc\":area,\"ratio\":auc_to_baseline_auc_ratio, \"baseline\":baseline_f1_max, })\n",
    "\n",
    "\n",
    "        # Find the maximum F score for different values of .  \n",
    "        f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "        f_1_scores = f_beta(precision,recall,beta=1)\n",
    "        f_2_scores = f_beta(precision,recall,beta=2)\n",
    "        f_point5_scores = f_beta(precision,recall,beta=0.5)\n",
    "        f_1_max, f_1_std = np.nanmax(f_1_scores), np.std(f_1_scores)\n",
    "        f_2_max, f_2_std = np.nanmax(f_2_scores), np.std(f_2_scores)\n",
    "        f_point5_max, f_point5_std = np.nanmax(f_point5_scores), np.std(f_point5_scores)\n",
    "\n",
    "        # Find the standard deviation of each metric when subsampling the dataset of predictions for each method.\n",
    "        bootstrap_fraction = 0.5\n",
    "        bootstrap_iterations = 2\n",
    "        bootstrapped_std_dict = bootstrap(bootstrap_fraction, bootstrap_iterations, y_true, y_prob)\n",
    "\n",
    "        tables[c][q][s][name].update({\"f1_max\":f_1_max, \"f5_max\":f_point5_max, \"f2_max\":f_2_max})\n",
    "        #TABLE[name].update({\"f1_std\":f_1_std, \"f5_std\":f_point5_std, \"f2_std\":f_2_std})\n",
    "        tables[c][q][s][name].update({\"f1_std\":bootstrapped_std_dict[\"f_1_max_std\"], \n",
    "                            \"f5_std\":bootstrapped_std_dict[\"f_point5_max_std\"], \n",
    "                            \"f2_std\":bootstrapped_std_dict[\"f_2_max_std\"]}) \n",
    "\n",
    "        # Producing the precision recall curve.\n",
    "        #step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "        #ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "        #ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "        #ax.axhline(baseline_auc, linestyle=\"--\", color=\"lightgray\")\n",
    "        #ax.set_xlabel('Recall')\n",
    "        #ax.set_ylabel('Precision')\n",
    "        #ax.set_ylim([0.0, 1.05])\n",
    "        #ax.set_xlim([0.0, 1.0])\n",
    "        #ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name, baseline_auc))\n",
    "\n",
    "    #fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "    #fig.tight_layout()\n",
    "    #fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_prcurve_shared.png\"),dpi=400)\n",
    "    #plt.close()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"y\"></a>\n",
    "### Are genes in the same group or pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "skipfornow = 1\n",
    "if skipfornow == 2:\n",
    "    \n",
    "    # When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "    # graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "    # i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "    # in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "    # connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "    df = make_undirected(df)\n",
    "\n",
    "    # What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "    # each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "    # recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "    k = 10      # The threshold of interest for gene ranks.\n",
    "    n = 100     # Number of Monte Carlo simulation iterations to complete.\n",
    "    df[list(names)] = df.groupby(\"from\")[list(names)].rank()\n",
    "    ys = df[df[\"shared\"]==1][list(names)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "    ymax = sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "    # Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "    # edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "    ysims = [sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "    for name in names:\n",
    "        pvalue = len([ysim for ysim in ysims if ysim>=ys[name]])/float(n)\n",
    "        TABLE[name].update({\"y\":ys[name], \"y_max\":ymax, \"y_ratio\":ys[name]/ymax, \"y_pval\":pvalue})\n",
    "    \n",
    "print(\"done skipping the y thing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean\"></a>\n",
    "### Predicting biochemical pathway or group membership based on mean vectors\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes asociated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "# vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "# names = list(vector_dicts.keys())\n",
    "# group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "# valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "# valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "# pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# for name in names:\n",
    "#     for group in valid_group_ids:\n",
    "#         ids = group_id_to_ids[group]\n",
    "#         for identifier in valid_ids:\n",
    "#             # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "#             vectors = np.array([vector_dicts[name][some_id] for some_id in ids if not some_id==identifier])\n",
    "#             mean_vector = vectors.mean(axis=0)\n",
    "#             this_vector = vector_dicts[name][identifier]\n",
    "#             pred_dict[name][identifier][group] = 1-metric_dict[name](mean_vector, this_vector)\n",
    "#             true_dict[name][identifier][group] = (identifier in group_id_to_ids[group])*1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "# fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "# for name,ax in zip(names, axs.flatten()):\n",
    "#     \n",
    "#     # Obtaining the values and metrics.\n",
    "#     y_true = pd.DataFrame(true_dict[name]).as_matrix().flatten()\n",
    "#     y_prob = pd.DataFrame(pred_dict[name]).as_matrix().flatten()\n",
    "#     n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "#     baseline = Counter(y_true)[1]/len(y_true) \n",
    "#     area = auc(recall, precision)\n",
    "#     auc_to_baseline_auc_ratio = area/baseline\n",
    "#     TABLE[name].update({\"mean_auc\":area, \"mean_baseline\":baseline, \"mean_ratio\":auc_to_baseline_auc_ratio})\n",
    "# \n",
    "#     # Producing the precision recall curve.\n",
    "#     step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "#     ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "#     ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "#     ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "#     ax.set_xlabel('Recall')\n",
    "#     ax.set_ylabel('Precision')\n",
    "#     ax.set_ylim([0.0, 1.05])\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name[:10], baseline))\n",
    "#     \n",
    "# fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_prcurve_mean_classifier.png\"),dpi=400)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway membership based on mean similarity values\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representationt of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership with KNN classifier\n",
    "This section looks at how well the group(s) or biochemical pathway(s) that a particular gene belongs to can be predicted based on a KNN classifier generated using every other gene. For this section, only the groups or pathways which contain more than one gene, and the genes mapped to those groups or pathways, are of interest. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but the KNN classifier could never predict this because when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be members of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {True: {'pwy_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.4834103895965197, 'mean_0': 0.5482100897211964, 'n_1': 49, 'n_0': 2366, 'ks': 0.17334862939258544, 'ks_pval': 0.09873263290966383, 'auc': 0.05106335135698371, 'ratio': 2.5166937454513403, 'baseline': 0.03977272727272727, 'f1_max': 0.09448818897637797, 'f5_max': 0.08583690987124463, 'f2_max': 0.1337295690936107, 'f1_std': 0.010350076103500763, 'f5_std': 0.013392259274139548, 'f2_std': 0.03430437586618549}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.3983264703261412, 'mean_0': 0.44394007938995955, 'n_1': 49, 'n_0': 2366, 'ks': 0.2206859074990943, 'ks_pval': 0.015575124610253677, 'auc': 0.05135461228073793, 'ratio': 2.5310487481220836, 'baseline': 0.03977272727272727, 'f1_max': 0.0851063829787234, 'f5_max': 0.09433962264150941, 'f2_max': 0.12426900584795321, 'f1_std': 0.0008196721311475377, 'f5_std': 0.049735449735449744, 'f2_std': 0.0007425742574257571}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.36178871774872484, 'mean_0': 0.4006116117178865, 'n_1': 49, 'n_0': 2366, 'ks': 0.17026929114841202, 'ks_pval': 0.1096119135128325, 'auc': 0.036544925982823356, 'ratio': 1.8011427805820084, 'baseline': 0.03977272727272727, 'f1_max': 0.10389610389610389, 'f5_max': 0.12422360248447203, 'f2_max': 0.14285714285714288, 'f1_std': 0.004205214465937759, 'f5_std': 0.015568240788790877, 'f2_std': 0.013246935648621061}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.8855837854139201, 'mean_0': 0.9298441068277663, 'n_1': 49, 'n_0': 2366, 'ks': 0.15783117980920178, 'ks_pval': 0.16399168096500794, 'auc': 0.04539961453670904, 'ratio': 2.237552430737803, 'baseline': 0.03977272727272727, 'f1_max': 0.11235955056179775, 'f5_max': 0.09389671361502347, 'f2_max': 0.15384615384615385, 'f1_std': 0.011198208286674137, 'f5_std': 0.037974350888258186, 'f2_std': 0.004276276984782348}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.8949115081482969, 'mean_0': 0.9500107839229983, 'n_1': 49, 'n_0': 2366, 'ks': 0.2031759449341867, 'ks_pval': 0.03250830151504336, 'auc': 0.04883185636241887, 'ratio': 2.406712920719216, 'baseline': 0.03977272727272727, 'f1_max': 0.125, 'f5_max': 0.10141987829614604, 'f2_max': 0.18258426966292135, 'f1_std': 0.04436948624805397, 'f5_std': 0.01625671472999718, 'f2_std': 0.05844801857071796}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.907424250024561, 'mean_0': 0.9439346578508854, 'n_1': 49, 'n_0': 2366, 'ks': 0.15469146238377007, 'ks_pval': 0.18065335315075148, 'auc': 0.038193477985431525, 'ratio': 1.8823928435676966, 'baseline': 0.03977272727272727, 'f1_max': 0.08333333333333333, 'f5_max': 0.07692307692307691, 'f2_max': 0.11254019292604502, 'f1_std': 0.007300792657488529, 'f5_std': 0.023096821877309673, 'f2_std': 0.0058620689655172475}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.870439602096036, 'mean_0': 0.9463790014084319, 'n_1': 49, 'n_0': 2366, 'ks': 0.13971742543171115, 'ks_pval': 0.2787521796181721, 'auc': 0.057951178631745356, 'ratio': 2.8561652325645928, 'baseline': 0.03977272727272727, 'f1_max': 0.12244897959183673, 'f5_max': 0.13761467889908258, 'f2_max': 0.14563106796116507, 'f1_std': 0.0017421602787456442, 'f5_std': 0.0023024498065941923, 'f2_std': 0.01026392961876832}, 'Mean': {'mean_1': 0.39906828506207387, 'mean_0': 0.4606939108257606, 'n_1': 49, 'n_0': 2366, 'ks': 0.15179326168337157, 'ks_pval': 0.19718450431837797, 'auc': 0.055830301121088735, 'ratio': 2.7516362695393735, 'baseline': 0.03977272727272727, 'f1_max': 0.12048192771084339, 'f5_max': 0.13513513513513514, 'f2_max': 0.12903225806451613, 'f1_std': 0.002685284640171856, 'f5_std': 0.011408083441981756, 'f2_std': 0.01432151808091657}}), 'phe_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.45649680493892647, 'mean_0': 0.49732435192391117, 'n_1': 925, 'n_0': 7721, 'ks': 0.06517220497274895, 'ks_pval': 1.0, 'auc': 0.134028261599426, 'ratio': 1.2527657835552835, 'baseline': 0.19329223696583428, 'f1_max': 0.19909391453410066, 'f5_max': 0.1492977693851056, 'f2_max': 0.3752704276917956, 'f1_std': 0.0016713240053016915, 'f5_std': 0.00041564841152197385, 'f2_std': 0.0009117235004404978}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.3630305012021829, 'mean_0': 0.44321376774509025, 'n_1': 925, 'n_0': 7721, 'ks': 0.32302663497586437, 'ks_pval': 2.7159834825224414e-75, 'auc': 0.31599017978723376, 'ratio': 2.9535687507464035, 'baseline': 0.19329223696583428, 'f1_max': 0.3265132139812447, 'f5_max': 0.3709488481062085, 'f2_max': 0.4349130173965207, 'f1_std': 0.01112919476090829, 'f5_std': 0.0016553962600650562, 'f2_std': 0.014427557898945842}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.2944134066221057, 'mean_0': 0.42129445697217827, 'n_1': 925, 'n_0': 7721, 'ks': 0.3290589021867354, 'ks_pval': 4.089319193593568e-78, 'auc': 0.2641647071870235, 'ratio': 2.4691546576637897, 'baseline': 0.19329223696583428, 'f1_max': 0.333041958041958, 'f5_max': 0.32545130943300277, 'f2_max': 0.4482543640897756, 'f1_std': 0.01679450109943384, 'f5_std': 0.01015094873083694, 'f2_std': 0.010269353889355487}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.8248678253241802, 'mean_0': 0.9436125533000008, 'n_1': 925, 'n_0': 7721, 'ks': 0.46788379883574804, 'ks_pval': 1.7063466665438385e-157, 'auc': 0.36426313362210605, 'ratio': 3.4047773549153826, 'baseline': 0.19329223696583428, 'f1_max': 0.3908045977011494, 'f5_max': 0.3839709578330075, 'f2_max': 0.5278325496738965, 'f1_std': 0.0015287055483938494, 'f5_std': 0.005420587596241033, 'f2_std': 0.0024705891774580158}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.8269408937384302, 'mean_0': 0.962994993610233, 'n_1': 925, 'n_0': 7721, 'ks': 0.5225747960108794, 'ks_pval': 2.324685709828791e-196, 'auc': 0.4243039968870252, 'ratio': 3.965980926578616, 'baseline': 0.19329223696583428, 'f1_max': 0.4619263741805346, 'f5_max': 0.4663402692778458, 'f2_max': 0.5630550621669627, 'f1_std': 0.008076325760246195, 'f5_std': 0.007642193189618063, 'f2_std': 0.006400227978768658}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.8518494242432036, 'mean_0': 0.9499706229113631, 'n_1': 925, 'n_0': 7721, 'ks': 0.21474056364355548, 'ks_pval': 1.641200226785324e-33, 'auc': 0.21354065118352827, 'ratio': 1.9959702379813897, 'baseline': 0.19329223696583428, 'f1_max': 0.27671755725190844, 'f5_max': 0.29653753856702086, 'f2_max': 0.37552776875608956, 'f1_std': 0.01494727218675776, 'f5_std': 0.019978694257117263, 'f2_std': 0.003425012395524063}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.7941794851678182, 'mean_0': 0.9594414246281148, 'n_1': 925, 'n_0': 7721, 'ks': 0.48249848605242984, 'ks_pval': 1.847121915548514e-167, 'auc': 0.38078088806623495, 'ratio': 3.5591692521304514, 'baseline': 0.19329223696583428, 'f1_max': 0.4368794326241135, 'f5_max': 0.4365826119683854, 'f2_max': 0.5369176677409903, 'f1_std': 0.016532306838476507, 'f5_std': 0.002449654528018874, 'f2_std': 0.014935563652259787}, 'Mean': {'mean_1': 0.249977231412014, 'mean_0': 0.47431906115791955, 'n_1': 925, 'n_0': 7721, 'ks': 0.4790334538657295, 'ks_pval': 4.539174229550336e-165, 'auc': 0.34396819613719276, 'ratio': 3.215080025732074, 'baseline': 0.19329223696583428, 'f1_max': 0.37903225806451607, 'f5_max': 0.3677545056015587, 'f2_max': 0.5348240991311779, 'f1_std': 0.0038636944671235074, 'f5_std': 0.024057948275411217, 'f2_std': 0.0024456177663778056}}), 'ppi_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.3329240706102116, 'mean_0': 0.41680490763106104, 'n_1': 402, 'n_0': 6738, 'ks': 0.1325850710826987, 'ks_pval': 3.2253621925502548e-06, 'auc': 0.08878280007555851, 'ratio': 1.5768885386554423, 'baseline': 0.10660302307080351, 'f1_max': 0.13072476656291684, 'f5_max': 0.10538116591928252, 'f2_max': 0.24456154573706254, 'f1_std': 0.00035543765133336236, 'f5_std': 0.01314787276486163, 'f2_std': 0.006710439937447049}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.4186710921001843, 'mean_0': 0.44434644766344245, 'n_1': 402, 'n_0': 6738, 'ks': 0.08574890463089715, 'ks_pval': 0.007074129783022021, 'auc': 0.09064285303022807, 'ratio': 1.6099253000891254, 'baseline': 0.10660302307080351, 'f1_max': 0.1194554238032499, 'f5_max': 0.137524557956778, 'f2_max': 0.24154894046417763, 'f1_std': 0.01030607336844945, 'f5_std': 0.01625671472999715, 'f2_std': 0.010358417428037309}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.365026620330626, 'mean_0': 0.3898728042094291, 'n_1': 402, 'n_0': 6738, 'ks': 0.07111592527123953, 'ks_pval': 0.04092982958892377, 'auc': 0.07582196786278661, 'ratio': 1.3466886829360607, 'baseline': 0.10660302307080351, 'f1_max': 0.11996066863323501, 'f5_max': 0.13064133016627077, 'f2_max': 0.23567187689325095, 'f1_std': 0.011735544441519696, 'f5_std': 0.011629651860744286, 'f2_std': 0.0020501381987888534}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.8999005928057505, 'mean_0': 0.9295876244108903, 'n_1': 402, 'n_0': 6738, 'ks': 0.10028663450335146, 'ks_pval': 0.0009704616363171841, 'auc': 0.0906247376628347, 'ratio': 1.6096035495339298, 'baseline': 0.10660302307080351, 'f1_max': 0.13886210221793638, 'f5_max': 0.16045548654244307, 'f2_max': 0.2297668038408779, 'f1_std': 0.01242271582321959, 'f5_std': 0.01106194690265487, 'f2_std': 0.002515172424185308}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.9151766507265315, 'mean_0': 0.9528051999305666, 'n_1': 402, 'n_0': 6738, 'ks': 0.11845713551565414, 'ks_pval': 4.756270465957831e-05, 'auc': 0.09643426745743353, 'ratio': 1.7127877354379983, 'baseline': 0.10660302307080351, 'f1_max': 0.1441899915182358, 'f5_max': 0.18321513002364065, 'f2_max': 0.2297668038408779, 'f1_std': 0.012232051549360107, 'f5_std': 0.010351643778610073, 'f2_std': 0.015635134241114}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.9173211571946762, 'mean_0': 0.9419431660487966, 'n_1': 402, 'n_0': 6738, 'ks': 0.045757410631614856, 'ks_pval': 0.3925702059796524, 'auc': 0.06981724558560545, 'ratio': 1.240037645475679, 'baseline': 0.10660302307080351, 'f1_max': 0.10842824601366743, 'f5_max': 0.12089810017271158, 'f2_max': 0.2311463927153864, 'f1_std': 0.0022160007884589328, 'f5_std': 0.021956856702619404, 'f2_std': 0.0009667523214599294}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.903761863822339, 'mean_0': 0.9459254087033863, 'n_1': 402, 'n_0': 6738, 'ks': 0.09732947019134071, 'ks_pval': 0.0015119607956634142, 'auc': 0.08433491934378545, 'ratio': 1.4978888659567864, 'baseline': 0.10660302307080351, 'f1_max': 0.1404878048780488, 'f5_max': 0.13073639274279616, 'f2_max': 0.2297668038408779, 'f1_std': 0.010643341392879146, 'f5_std': 0.0012034383954154515, 'f2_std': 0.0027601323677542144}, 'Mean': {'mean_1': 0.39258570580763963, 'mean_0': 0.4285187509810027, 'n_1': 402, 'n_0': 6738, 'ks': 0.08601693225767866, 'ks_pval': 0.006829303016991806, 'auc': 0.08383649493989463, 'ratio': 1.4890362534100687, 'baseline': 0.10660302307080351, 'f1_max': 0.13142857142857145, 'f5_max': 0.1467505241090147, 'f2_max': 0.23399903753609239, 'f1_std': 0.004215704589326866, 'f5_std': 0.00452925992801656, 'f2_std': 0.005535740016847937}}), 'ort_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {})})}, False: {'pwy_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.6551781960502512, 'mean_0': 0.6695736023454875, 'n_1': 208, 'n_0': 7793, 'ks': 0.10214418264912298, 'ks_pval': 0.027075433259181603, 'auc': 0.03538622635404651, 'ratio': 1.3611788320131062, 'baseline': 0.050676087221342425, 'f1_max': 0.05687809554241892, 'f5_max': 0.04875886524822696, 'f2_max': 0.12930408252215606, 'f1_std': 0.00247752089168074, 'f5_std': 0.00028495891842259327, 'f2_std': 0.0029430027187800495}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.4373503535288157, 'mean_0': 0.4523642776923556, 'n_1': 208, 'n_0': 7793, 'ks': 0.10484384408097998, 'ks_pval': 0.02152605849726552, 'auc': 0.03974480976995102, 'ratio': 1.5288376104297023, 'baseline': 0.050676087221342425, 'f1_max': 0.08660351826792963, 'f5_max': 0.0744514106583072, 'f2_max': 0.12911843276936777, 'f1_std': 0.012152227405159782, 'f5_std': 0.011225097568797977, 'f2_std': 0.011147227693026687}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.4235580326807512, 'mean_0': 0.4341057510653357, 'n_1': 208, 'n_0': 7793, 'ks': 0.041883001510231076, 'ks_pval': 0.8539874663004244, 'auc': 0.03713099100470335, 'ratio': 1.428293553022267, 'baseline': 0.050676087221342425, 'f1_max': 0.06756756756756756, 'f5_max': 0.08928571428571429, 'f2_max': 0.12053628980275881, 'f1_std': 0.011806056236481614, 'f5_std': 0.026492851135407905, 'f2_std': 0.001660705160960646}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.9109930953733232, 'mean_0': 0.9387255403981482, 'n_1': 208, 'n_0': 7793, 'ks': 0.09957469227807994, 'ks_pval': 0.03347704434218246, 'auc': 0.04391794565050189, 'ratio': 1.6893628997580077, 'baseline': 0.050676087221342425, 'f1_max': 0.08185840707964602, 'f5_max': 0.07142857142857144, 'f2_max': 0.1259093452714046, 'f1_std': 0.012517447212900538, 'f5_std': 0.005613231242293702, 'f2_std': 0.001541657686136455}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.9280873309181039, 'mean_0': 0.9582989459529421, 'n_1': 208, 'n_0': 7793, 'ks': 0.11649507941051634, 'ks_pval': 0.0074769340639143644, 'auc': 0.04457110684789332, 'ratio': 1.714487624471127, 'baseline': 0.050676087221342425, 'f1_max': 0.0791139240506329, 'f5_max': 0.06651884700665188, 'f2_max': 0.13411567476948869, 'f1_std': 0.008343845737970812, 'f5_std': 0.01061188106203706, 'f2_std': 0.005654936918635722}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.9312939551966993, 'mean_0': 0.950034898914214, 'n_1': 208, 'n_0': 7793, 'ks': 0.08274190841879793, 'ks_pval': 0.11781082222950334, 'auc': 0.0392274223030532, 'ratio': 1.5089356050323492, 'baseline': 0.050676087221342425, 'f1_max': 0.06805293005671077, 'f5_max': 0.06032171581769436, 'f2_max': 0.12663454920853406, 'f1_std': 0.0031499507820190245, 'f5_std': 0.010311928702010968, 'f2_std': 0.00895139268159477}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.9144257675323788, 'mean_0': 0.9507795186312664, 'n_1': 208, 'n_0': 7793, 'ks': 0.09670722739342014, 'ks_pval': 0.04216601213332505, 'auc': 0.04368274791566783, 'ratio': 1.6803157022752802, 'baseline': 0.050676087221342425, 'f1_max': 0.07272727272727272, 'f5_max': 0.07258064516129033, 'f2_max': 0.12694145758661884, 'f1_std': 0.013191489361702134, 'f5_std': 0.03195503195503196, 'f2_std': 0.004157322083519585}, 'Mean': {'mean_1': 0.464833553082926, 'mean_0': 0.5013242057601266, 'n_1': 208, 'n_0': 7793, 'ks': 0.12172721574588635, 'ks_pval': 0.0044860784558728994, 'auc': 0.03803574854218164, 'ratio': 1.4630962696442082, 'baseline': 0.050676087221342425, 'f1_max': 0.06701030927835051, 'f5_max': 0.07075471698113207, 'f2_max': 0.13275956784471707, 'f1_std': 0.003049923751906207, 'f5_std': 0.0006386162099040332, 'f2_std': 0.0027650974107036874}}), 'phe_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.5631011809093629, 'mean_0': 0.6373822483177846, 'n_1': 1527, 'n_0': 13008, 'ks': 0.12558208096815737, 'ks_pval': 3.8115484751742027e-19, 'auc': 0.16999734576743492, 'ratio': 1.6181476232676268, 'baseline': 0.19013821441912587, 'f1_max': 0.21221804511278197, 'f5_max': 0.18813634351482955, 'f2_max': 0.38296485414742787, 'f1_std': 0.0007443733869394037, 'f5_std': 0.010454189225138952, 'f2_std': 0.00044339508179436216}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.3432669622227617, 'mean_0': 0.44241294578421736, 'n_1': 1527, 'n_0': 13008, 'ks': 0.35101959320182596, 'ks_pval': 1.1110921875420914e-146, 'auc': 0.3785712254134047, 'ratio': 3.603492312628577, 'baseline': 0.19013821441912587, 'f1_max': 0.3722438391699093, 'f5_max': 0.4543853469531525, 'f2_max': 0.44818726805595205, 'f1_std': 0.01204793461791584, 'f5_std': 0.00454107760318645, 'f2_std': 0.007407850944621236}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.3129530057431136, 'mean_0': 0.45951755832575436, 'n_1': 1527, 'n_0': 13008, 'ks': 0.37256957785687883, 'ks_pval': 3.439862311491725e-165, 'auc': 0.29120771708283794, 'ratio': 2.77190842684941, 'baseline': 0.19013821441912587, 'f1_max': 0.3464345873104997, 'f5_max': 0.34357156920653675, 'f2_max': 0.4697088906372934, 'f1_std': 0.010126183256548904, 'f5_std': 0.0067672058709190575, 'f2_std': 0.01095277353400334}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.7876492897029601, 'mean_0': 0.9496884354173124, 'n_1': 1527, 'n_0': 13008, 'ks': 0.5366032872018307, 'ks_pval': 0.0, 'auc': 0.44722759963975445, 'ratio': 4.257009273584695, 'baseline': 0.19013821441912587, 'f1_max': 0.4503993816026797, 'f5_max': 0.4574945932457162, 'f2_max': 0.5692617836229241, 'f1_std': 0.004274516164378461, 'f5_std': 0.009341782422179229, 'f2_std': 0.0073643339114461415}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.7901547583159363, 'mean_0': 0.9669560705514263, 'n_1': 1527, 'n_0': 13008, 'ks': 0.5780428003199481, 'ks_pval': 0.0, 'auc': 0.49207386142483583, 'ratio': 4.683885773287484, 'baseline': 0.19013821441912587, 'f1_max': 0.5062140042437101, 'f5_max': 0.5177043128041617, 'f2_max': 0.6002449979583504, 'f1_std': 0.0072504262849446, 'f5_std': 0.0010557955484383208, 'f2_std': 0.003579390860989906}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.804359634130782, 'mean_0': 0.956319278844449, 'n_1': 1527, 'n_0': 13008, 'ks': 0.29550139312788015, 'ks_pval': 4.4809004160676234e-104, 'auc': 0.28024402742578775, 'ratio': 2.667548748286722, 'baseline': 0.19013821441912587, 'f1_max': 0.34722678043977684, 'f5_max': 0.39018836679776453, 'f2_max': 0.3948085941760279, 'f1_std': 0.007678468239670194, 'f5_std': 0.01227879611417676, 'f2_std': 0.005524682543834369}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.7537193888091291, 'mean_0': 0.9636201841724039, 'n_1': 1527, 'n_0': 13008, 'ks': 0.5327428851400497, 'ks_pval': 0.0, 'auc': 0.44851929761094667, 'ratio': 4.2693045126228615, 'baseline': 0.19013821441912587, 'f1_max': 0.4753284671532847, 'f5_max': 0.49478634226129625, 'f2_max': 0.5654236027155213, 'f1_std': 0.004909929252034512, 'f5_std': 0.0012794053491727575, 'f2_std': 0.0009546938617204237}, 'Mean': {'mean_1': 0.2420260226124239, 'mean_0': 0.5081111872155509, 'n_1': 1527, 'n_0': 13008, 'ks': 0.5444005139953167, 'ks_pval': 0.0, 'auc': 0.43346466890334573, 'ratio': 4.1260045596006085, 'baseline': 0.19013821441912587, 'f1_max': 0.43303571428571425, 'f5_max': 0.4635026366916458, 'f2_max': 0.5721096543504172, 'f1_std': 0.0044426685714084035, 'f5_std': 0.0076389322110006486, 'f2_std': 0.007180779040841467}}), 'ppi_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {}), 'PO:None': {'mean_1': 0.402396456776889, 'mean_0': 0.5104130013525128, 'n_1': 928, 'n_0': 24723, 'ks': 0.14669682321501543, 'ks_pval': 3.82300175322747e-17, 'auc': 0.05604913150291714, 'ratio': 1.549263224333327, 'baseline': 0.0698295646939313, 'f1_max': 0.09128377346969929, 'f5_max': 0.06383432963279248, 'f2_max': 0.1800672250973697, 'f1_std': 0.0014962879123396194, 'f5_std': 0.002282030752734192, 'f2_std': 0.0007568684786884994}, 'Doc2Vec:Wikipedia,Size=300': {'mean_1': 0.43233302478896635, 'mean_0': 0.44614054601911285, 'n_1': 928, 'n_0': 24723, 'ks': 0.0573251192174814, 'ks_pval': 0.005598093623395394, 'auc': 0.045528758579236606, 'ratio': 1.2584678731853427, 'baseline': 0.0698295646939313, 'f1_max': 0.07663483621419075, 'f5_max': 0.06960556844547564, 'f2_max': 0.16287982480153296, 'f1_std': 0.0007853103684656235, 'f5_std': 0.004379928582056245, 'f2_std': 0.000698027020323011}, 'Word2Vec:Wikipedia,Size=300,Mean,Nouns,Adjectives': {'mean_1': 0.3777545864951327, 'mean_0': 0.4025335079795518, 'n_1': 928, 'n_0': 24723, 'ks': 0.07880353105512528, 'ks_pval': 2.9960567154647405e-05, 'auc': 0.04851674268281471, 'ratio': 1.3410592312035345, 'baseline': 0.0698295646939313, 'f1_max': 0.08808467053601912, 'f5_max': 0.07588187038556195, 'f2_max': 0.1610255114928012, 'f1_std': 0.003738994705216228, 'f5_std': 0.0002835243048766242, 'f2_std': 0.0029864796818357547}, 'N-Grams:Simple,Words,Linares Pontes,1-grams,Binary,TFIDF': {'mean_1': 0.9110231089958389, 'mean_0': 0.9328154397858982, 'n_1': 928, 'n_0': 24723, 'ks': 0.0927281607800638, 'ks_pval': 4.1777072736662625e-07, 'auc': 0.05143649265902819, 'ratio': 1.4217645185309613, 'baseline': 0.0698295646939313, 'f1_max': 0.09129571874113977, 'f5_max': 0.07883672039243167, 'f2_max': 0.1624973273465897, 'f1_std': 0.0002361272627994307, 'f5_std': 0.0007696601912153822, 'f2_std': 0.0046757713712100996}, 'N-Grams:Full,Words,1-grams,TFIDF': {'mean_1': 0.9280380655895779, 'mean_0': 0.9530544956236975, 'n_1': 928, 'n_0': 24723, 'ks': 0.1013951827629444, 'ks_pval': 2.0598900154707278e-08, 'auc': 0.05323464653513166, 'ratio': 1.4714675843455411, 'baseline': 0.0698295646939313, 'f1_max': 0.09266409266409267, 'f5_max': 0.08217324052590873, 'f2_max': 0.16746706070680947, 'f1_std': 0.0017829404216036168, 'f5_std': 0.0040421231783852735, 'f2_std': 0.00391845090210749}, 'Topic Models:LDA,Full,Topics=100': {'mean_1': 0.9293565117673531, 'mean_0': 0.946610849728435, 'n_1': 928, 'n_0': 24723, 'ks': 0.07723843112723461, 'ks_pval': 4.637489314337543e-05, 'auc': 0.04604745998470419, 'ratio': 1.2728053836935853, 'baseline': 0.0698295646939313, 'f1_max': 0.0797501301405518, 'f5_max': 0.06390977443609024, 'f2_max': 0.1584597914731083, 'f1_std': 0.00261247935664536, 'f5_std': 0.002977655468525464, 'f2_std': 0.005299095791817712}, 'Topic Models:NMF,Full,Topics=100': {'mean_1': 0.9140416285134431, 'mean_0': 0.9420809502909153, 'n_1': 928, 'n_0': 24723, 'ks': 0.06484499112232511, 'ks_pval': 0.0010822040795185745, 'auc': 0.04818701741990587, 'ratio': 1.3319452412047472, 'baseline': 0.0698295646939313, 'f1_max': 0.08262295081967214, 'f5_max': 0.07229965156794425, 'f2_max': 0.1610978520286396, 'f1_std': 0.0028872180451127924, 'f5_std': 0.0038712990076460174, 'f2_std': 0.0027594083852671564}, 'Mean': {'mean_1': 0.42475011893091913, 'mean_0': 0.4671395922262745, 'n_1': 928, 'n_0': 24723, 'ks': 0.10023125192651822, 'ks_pval': 3.1344678231959745e-08, 'auc': 0.04635527869407686, 'ratio': 1.281313851057937, 'baseline': 0.0698295646939313, 'f1_max': 0.08810209962947715, 'f5_max': 0.07489106753812635, 'f2_max': 0.16653663806037328, 'f1_std': 0.00463401524147522, 'f5_std': 0.0020737693741193497, 'f2_std': 0.00022542547510903677}}), 'ort_y': defaultdict(<class 'dict'>, {'intra': defaultdict(<class 'dict'>, {}), 'inter': defaultdict(<class 'dict'>, {}), 'both': defaultdict(<class 'dict'>, {})})}})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"output\"></a>\n",
    "### Summarizing the results for this notebook\n",
    "Write a large table of results to an output file. Columns are generally metrics and rows are generally methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Species</th>\n",
       "      <th>Objective</th>\n",
       "      <th>Curated</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Group</th>\n",
       "      <th>Order</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_0</th>\n",
       "      <th>ks</th>\n",
       "      <th>ks_pval</th>\n",
       "      <th>auc</th>\n",
       "      <th>ratio</th>\n",
       "      <th>baseline</th>\n",
       "      <th>f1_max</th>\n",
       "      <th>f5_max</th>\n",
       "      <th>f2_max</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>f5_std</th>\n",
       "      <th>f2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431464</td>\n",
       "      <td>0.521843</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.207350</td>\n",
       "      <td>4.551976e-02</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>2.839788</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.025649</td>\n",
       "      <td>0.024561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395605</td>\n",
       "      <td>0.438123</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.188535</td>\n",
       "      <td>8.706231e-02</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>2.724235</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.128617</td>\n",
       "      <td>0.018762</td>\n",
       "      <td>0.013471</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.355652</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.172134</td>\n",
       "      <td>1.456672e-01</td>\n",
       "      <td>0.038639</td>\n",
       "      <td>1.986783</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.159744</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.043413</td>\n",
       "      <td>0.012671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.877587</td>\n",
       "      <td>0.928449</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>8.803716e-02</td>\n",
       "      <td>0.047849</td>\n",
       "      <td>2.460329</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>0.096618</td>\n",
       "      <td>0.168919</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.033568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.884761</td>\n",
       "      <td>0.948228</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>1.506733e-02</td>\n",
       "      <td>0.051735</td>\n",
       "      <td>2.660127</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.197568</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.896164</td>\n",
       "      <td>0.941884</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.188642</td>\n",
       "      <td>8.677684e-02</td>\n",
       "      <td>0.040714</td>\n",
       "      <td>2.093456</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.114223</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>0.012277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.853158</td>\n",
       "      <td>0.945058</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.164099</td>\n",
       "      <td>1.841856e-01</td>\n",
       "      <td>0.063421</td>\n",
       "      <td>3.260999</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.036963</td>\n",
       "      <td>0.025008</td>\n",
       "      <td>0.054745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.388355</td>\n",
       "      <td>0.455111</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2168.0</td>\n",
       "      <td>0.178634</td>\n",
       "      <td>1.194929e-01</td>\n",
       "      <td>0.060064</td>\n",
       "      <td>3.088398</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.141343</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.051908</td>\n",
       "      <td>0.018268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456497</td>\n",
       "      <td>0.497324</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.134028</td>\n",
       "      <td>1.252766</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.199094</td>\n",
       "      <td>0.149298</td>\n",
       "      <td>0.375270</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.363031</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.323027</td>\n",
       "      <td>2.715983e-75</td>\n",
       "      <td>0.315990</td>\n",
       "      <td>2.953569</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.326513</td>\n",
       "      <td>0.370949</td>\n",
       "      <td>0.434913</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>0.018140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.294413</td>\n",
       "      <td>0.421294</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.329059</td>\n",
       "      <td>4.089319e-78</td>\n",
       "      <td>0.264165</td>\n",
       "      <td>2.469155</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.325451</td>\n",
       "      <td>0.448254</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.007612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824868</td>\n",
       "      <td>0.943613</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.467884</td>\n",
       "      <td>1.706347e-157</td>\n",
       "      <td>0.364263</td>\n",
       "      <td>3.404777</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.383971</td>\n",
       "      <td>0.527833</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.826941</td>\n",
       "      <td>0.962995</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.522575</td>\n",
       "      <td>2.324686e-196</td>\n",
       "      <td>0.424304</td>\n",
       "      <td>3.965981</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.461926</td>\n",
       "      <td>0.466340</td>\n",
       "      <td>0.563055</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.008128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.851849</td>\n",
       "      <td>0.949971</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.214741</td>\n",
       "      <td>1.641200e-33</td>\n",
       "      <td>0.213541</td>\n",
       "      <td>1.995970</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.296538</td>\n",
       "      <td>0.375528</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.001842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.794179</td>\n",
       "      <td>0.959441</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.482498</td>\n",
       "      <td>1.847122e-167</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>3.559169</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.436879</td>\n",
       "      <td>0.436583</td>\n",
       "      <td>0.536918</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>0.003620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.249977</td>\n",
       "      <td>0.474319</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.479033</td>\n",
       "      <td>4.539174e-165</td>\n",
       "      <td>0.343968</td>\n",
       "      <td>3.215080</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.367755</td>\n",
       "      <td>0.534824</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332924</td>\n",
       "      <td>0.351720</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.073876</td>\n",
       "      <td>3.123891e-02</td>\n",
       "      <td>0.089652</td>\n",
       "      <td>1.388933</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.130725</td>\n",
       "      <td>0.105381</td>\n",
       "      <td>0.258286</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.007427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.418671</td>\n",
       "      <td>0.437196</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.055782</td>\n",
       "      <td>1.849863e-01</td>\n",
       "      <td>0.095075</td>\n",
       "      <td>1.472958</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.126926</td>\n",
       "      <td>0.137525</td>\n",
       "      <td>0.264770</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.365027</td>\n",
       "      <td>0.390007</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.066921</td>\n",
       "      <td>6.564002e-02</td>\n",
       "      <td>0.083925</td>\n",
       "      <td>1.300204</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.127889</td>\n",
       "      <td>0.131894</td>\n",
       "      <td>0.263016</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.007847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.899901</td>\n",
       "      <td>0.928662</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.094435</td>\n",
       "      <td>2.266630e-03</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>1.526102</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.144724</td>\n",
       "      <td>0.160455</td>\n",
       "      <td>0.256508</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.007518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.915177</td>\n",
       "      <td>0.949935</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.104398</td>\n",
       "      <td>5.045929e-04</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>1.590140</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.148043</td>\n",
       "      <td>0.183215</td>\n",
       "      <td>0.256508</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.007787</td>\n",
       "      <td>0.004532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.917321</td>\n",
       "      <td>0.940855</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.044592</td>\n",
       "      <td>4.300630e-01</td>\n",
       "      <td>0.078209</td>\n",
       "      <td>1.211652</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.127505</td>\n",
       "      <td>0.256876</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.903762</td>\n",
       "      <td>0.942864</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.090879</td>\n",
       "      <td>3.735598e-03</td>\n",
       "      <td>0.092286</td>\n",
       "      <td>1.429751</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>0.135960</td>\n",
       "      <td>0.256508</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.015978</td>\n",
       "      <td>0.003006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.392586</td>\n",
       "      <td>0.422885</td>\n",
       "      <td>402.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>0.077409</td>\n",
       "      <td>2.083170e-02</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>1.398938</td>\n",
       "      <td>0.121267</td>\n",
       "      <td>0.135294</td>\n",
       "      <td>0.146751</td>\n",
       "      <td>0.260032</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.638670</td>\n",
       "      <td>0.637591</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.053672</td>\n",
       "      <td>6.376717e-01</td>\n",
       "      <td>0.036731</td>\n",
       "      <td>1.353096</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>0.055724</td>\n",
       "      <td>0.127369</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436422</td>\n",
       "      <td>0.446506</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>4.187175e-02</td>\n",
       "      <td>0.040717</td>\n",
       "      <td>1.499933</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.090523</td>\n",
       "      <td>0.075939</td>\n",
       "      <td>0.130455</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.010412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.420228</td>\n",
       "      <td>0.432504</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.046469</td>\n",
       "      <td>7.989763e-01</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>1.451449</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.071685</td>\n",
       "      <td>0.092402</td>\n",
       "      <td>0.126610</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.026701</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906312</td>\n",
       "      <td>0.936126</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.099053</td>\n",
       "      <td>4.858056e-02</td>\n",
       "      <td>0.046521</td>\n",
       "      <td>1.713726</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.073206</td>\n",
       "      <td>0.130426</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923859</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.116888</td>\n",
       "      <td>1.140524e-02</td>\n",
       "      <td>0.046893</td>\n",
       "      <td>1.727430</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.068847</td>\n",
       "      <td>0.137947</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>0.009467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929261</td>\n",
       "      <td>0.947597</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.072246</td>\n",
       "      <td>2.729650e-01</td>\n",
       "      <td>0.041084</td>\n",
       "      <td>1.513422</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.069246</td>\n",
       "      <td>0.061107</td>\n",
       "      <td>0.129140</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>0.017743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.909082</td>\n",
       "      <td>0.947824</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.108647</td>\n",
       "      <td>2.295480e-02</td>\n",
       "      <td>0.046539</td>\n",
       "      <td>1.714390</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.075227</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.134824</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.001865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.456037</td>\n",
       "      <td>0.492173</td>\n",
       "      <td>191.0</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>0.125605</td>\n",
       "      <td>5.147917e-03</td>\n",
       "      <td>0.039881</td>\n",
       "      <td>1.469109</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>0.072698</td>\n",
       "      <td>0.138356</td>\n",
       "      <td>0.015320</td>\n",
       "      <td>0.021222</td>\n",
       "      <td>0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.563101</td>\n",
       "      <td>0.637382</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.125582</td>\n",
       "      <td>3.811548e-19</td>\n",
       "      <td>0.169997</td>\n",
       "      <td>1.618148</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.212218</td>\n",
       "      <td>0.188136</td>\n",
       "      <td>0.382965</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.006301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343267</td>\n",
       "      <td>0.442413</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.351020</td>\n",
       "      <td>1.111092e-146</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>3.603492</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.372244</td>\n",
       "      <td>0.454385</td>\n",
       "      <td>0.448187</td>\n",
       "      <td>0.011125</td>\n",
       "      <td>0.016249</td>\n",
       "      <td>0.010135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312953</td>\n",
       "      <td>0.459518</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.372570</td>\n",
       "      <td>3.439862e-165</td>\n",
       "      <td>0.291208</td>\n",
       "      <td>2.771908</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.346435</td>\n",
       "      <td>0.343572</td>\n",
       "      <td>0.469709</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.787649</td>\n",
       "      <td>0.949688</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.536603</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.447228</td>\n",
       "      <td>4.257009</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.450399</td>\n",
       "      <td>0.457495</td>\n",
       "      <td>0.569262</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.007375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.790155</td>\n",
       "      <td>0.966956</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.578043</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>4.683886</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.506214</td>\n",
       "      <td>0.517704</td>\n",
       "      <td>0.600245</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.008027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.804360</td>\n",
       "      <td>0.956319</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.295501</td>\n",
       "      <td>4.480900e-104</td>\n",
       "      <td>0.280244</td>\n",
       "      <td>2.667549</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.347227</td>\n",
       "      <td>0.390188</td>\n",
       "      <td>0.394809</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.004973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.753719</td>\n",
       "      <td>0.963620</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.532743</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.448519</td>\n",
       "      <td>4.269305</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.475328</td>\n",
       "      <td>0.494786</td>\n",
       "      <td>0.565424</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.009565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.242026</td>\n",
       "      <td>0.508111</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.544401</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.433465</td>\n",
       "      <td>4.126005</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.433036</td>\n",
       "      <td>0.463503</td>\n",
       "      <td>0.572110</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>PO</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402396</td>\n",
       "      <td>0.462912</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.086010</td>\n",
       "      <td>3.806476e-06</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>1.401056</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.091563</td>\n",
       "      <td>0.064053</td>\n",
       "      <td>0.182691</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.005420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.432333</td>\n",
       "      <td>0.442286</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.043791</td>\n",
       "      <td>6.578137e-02</td>\n",
       "      <td>0.048782</td>\n",
       "      <td>1.199834</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.081638</td>\n",
       "      <td>0.070258</td>\n",
       "      <td>0.178118</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.377755</td>\n",
       "      <td>0.399543</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>2.880137e-04</td>\n",
       "      <td>0.052405</td>\n",
       "      <td>1.288955</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.091554</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.176699</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.010510</td>\n",
       "      <td>0.005275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.911023</td>\n",
       "      <td>0.930406</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.085396</td>\n",
       "      <td>4.590999e-06</td>\n",
       "      <td>0.055304</td>\n",
       "      <td>1.360244</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.095102</td>\n",
       "      <td>0.080074</td>\n",
       "      <td>0.177923</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.007242</td>\n",
       "      <td>0.007826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.928038</td>\n",
       "      <td>0.950041</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.093126</td>\n",
       "      <td>3.934202e-07</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>1.395457</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.095262</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.180806</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.944424</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.065225</td>\n",
       "      <td>1.026290e-03</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>1.224382</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.085874</td>\n",
       "      <td>0.067783</td>\n",
       "      <td>0.174989</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.914042</td>\n",
       "      <td>0.938714</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.058915</td>\n",
       "      <td>4.140151e-03</td>\n",
       "      <td>0.051993</td>\n",
       "      <td>1.278823</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.085540</td>\n",
       "      <td>0.075620</td>\n",
       "      <td>0.176230</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>0.007598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mean</td>\n",
       "      <td>intra</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.424750</td>\n",
       "      <td>0.460189</td>\n",
       "      <td>928.0</td>\n",
       "      <td>21897.0</td>\n",
       "      <td>0.087103</td>\n",
       "      <td>2.717755e-06</td>\n",
       "      <td>0.049649</td>\n",
       "      <td>1.221160</td>\n",
       "      <td>0.078137</td>\n",
       "      <td>0.091717</td>\n",
       "      <td>0.075219</td>\n",
       "      <td>0.179182</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>PO</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.855693</td>\n",
       "      <td>0.836912</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>1.571793e-01</td>\n",
       "      <td>0.026412</td>\n",
       "      <td>0.898022</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.047554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417830</td>\n",
       "      <td>0.507635</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>1.465718e-02</td>\n",
       "      <td>0.075628</td>\n",
       "      <td>2.571354</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.308642</td>\n",
       "      <td>0.065426</td>\n",
       "      <td>0.083742</td>\n",
       "      <td>0.076450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.405766</td>\n",
       "      <td>0.402931</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>6.105562e-01</td>\n",
       "      <td>0.030695</td>\n",
       "      <td>1.043644</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.049242</td>\n",
       "      <td>0.037054</td>\n",
       "      <td>0.087286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.942892</td>\n",
       "      <td>0.945122</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>7.598563e-01</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>1.106114</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.063855</td>\n",
       "      <td>0.066066</td>\n",
       "      <td>0.065537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.969529</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>9.136105e-01</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>1.108705</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.034746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.988121</td>\n",
       "      <td>0.966385</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>4.282710e-01</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.897705</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.031255</td>\n",
       "      <td>0.020914</td>\n",
       "      <td>0.060884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.994289</td>\n",
       "      <td>0.960841</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>6.322689e-01</td>\n",
       "      <td>0.025963</td>\n",
       "      <td>0.882754</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.014881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Mean</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.475849</td>\n",
       "      <td>0.521824</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>6.971042e-01</td>\n",
       "      <td>0.033979</td>\n",
       "      <td>1.155289</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.021053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>PO</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840647</td>\n",
       "      <td>0.900504</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.450112</td>\n",
       "      <td>1.322443e-03</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>1.583938</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.047011</td>\n",
       "      <td>0.160550</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.027597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.447781</td>\n",
       "      <td>0.494663</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.299516</td>\n",
       "      <td>7.972678e-02</td>\n",
       "      <td>0.029271</td>\n",
       "      <td>1.661581</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.140187</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.031840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.460972</td>\n",
       "      <td>0.445671</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.144391</td>\n",
       "      <td>8.288482e-01</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.896732</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.091864</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.005234</td>\n",
       "      <td>0.002943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.963590</td>\n",
       "      <td>0.957494</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.298709</td>\n",
       "      <td>8.111984e-02</td>\n",
       "      <td>0.017129</td>\n",
       "      <td>0.972343</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.013082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.975596</td>\n",
       "      <td>0.979002</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.108960</td>\n",
       "      <td>9.757130e-01</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>1.133342</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.009153</td>\n",
       "      <td>0.025672</td>\n",
       "      <td>0.004814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.954137</td>\n",
       "      <td>0.967640</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.227290</td>\n",
       "      <td>3.051204e-01</td>\n",
       "      <td>0.020746</td>\n",
       "      <td>1.177622</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.106534</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.018836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.974466</td>\n",
       "      <td>0.972121</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.172189</td>\n",
       "      <td>6.426587e-01</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.929281</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.082285</td>\n",
       "      <td>0.048352</td>\n",
       "      <td>0.071629</td>\n",
       "      <td>0.013468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mean</td>\n",
       "      <td>inter</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.563662</td>\n",
       "      <td>0.567397</td>\n",
       "      <td>17.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.104120</td>\n",
       "      <td>9.843916e-01</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>1.048158</td>\n",
       "      <td>0.034623</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.483410</td>\n",
       "      <td>0.548210</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.173349</td>\n",
       "      <td>9.873263e-02</td>\n",
       "      <td>0.051063</td>\n",
       "      <td>2.516694</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.085837</td>\n",
       "      <td>0.133730</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398326</td>\n",
       "      <td>0.443940</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.220686</td>\n",
       "      <td>1.557512e-02</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>2.531049</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.124269</td>\n",
       "      <td>0.040056</td>\n",
       "      <td>0.072720</td>\n",
       "      <td>0.058072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.361789</td>\n",
       "      <td>0.400612</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.170269</td>\n",
       "      <td>1.096119e-01</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>1.801143</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.124224</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>0.025897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>0.929844</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.157831</td>\n",
       "      <td>1.639917e-01</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>2.237552</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.093897</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.020265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.894912</td>\n",
       "      <td>0.950011</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.203176</td>\n",
       "      <td>3.250830e-02</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>2.406713</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.182584</td>\n",
       "      <td>0.064251</td>\n",
       "      <td>0.060214</td>\n",
       "      <td>0.070483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.907424</td>\n",
       "      <td>0.943935</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.154691</td>\n",
       "      <td>1.806534e-01</td>\n",
       "      <td>0.038193</td>\n",
       "      <td>1.882393</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.112540</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>0.015685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.870440</td>\n",
       "      <td>0.946379</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.139717</td>\n",
       "      <td>2.787522e-01</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>2.856165</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.054487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.399068</td>\n",
       "      <td>0.460694</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.151793</td>\n",
       "      <td>1.971845e-01</td>\n",
       "      <td>0.055830</td>\n",
       "      <td>2.751636</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456497</td>\n",
       "      <td>0.497324</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.134028</td>\n",
       "      <td>1.252766</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.199094</td>\n",
       "      <td>0.149298</td>\n",
       "      <td>0.375270</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.005089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.363031</td>\n",
       "      <td>0.443214</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.323027</td>\n",
       "      <td>2.715983e-75</td>\n",
       "      <td>0.315990</td>\n",
       "      <td>2.953569</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.326513</td>\n",
       "      <td>0.370949</td>\n",
       "      <td>0.434913</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.294413</td>\n",
       "      <td>0.421294</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.329059</td>\n",
       "      <td>4.089319e-78</td>\n",
       "      <td>0.264165</td>\n",
       "      <td>2.469155</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.325451</td>\n",
       "      <td>0.448254</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.824868</td>\n",
       "      <td>0.943613</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.467884</td>\n",
       "      <td>1.706347e-157</td>\n",
       "      <td>0.364263</td>\n",
       "      <td>3.404777</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.383971</td>\n",
       "      <td>0.527833</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.013416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.826941</td>\n",
       "      <td>0.962995</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.522575</td>\n",
       "      <td>2.324686e-196</td>\n",
       "      <td>0.424304</td>\n",
       "      <td>3.965981</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.461926</td>\n",
       "      <td>0.466340</td>\n",
       "      <td>0.563055</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.851849</td>\n",
       "      <td>0.949971</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.214741</td>\n",
       "      <td>1.641200e-33</td>\n",
       "      <td>0.213541</td>\n",
       "      <td>1.995970</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.296538</td>\n",
       "      <td>0.375528</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.008798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.794179</td>\n",
       "      <td>0.959441</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.482498</td>\n",
       "      <td>1.847122e-167</td>\n",
       "      <td>0.380781</td>\n",
       "      <td>3.559169</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.436879</td>\n",
       "      <td>0.436583</td>\n",
       "      <td>0.536918</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>0.008031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.249977</td>\n",
       "      <td>0.474319</td>\n",
       "      <td>925.0</td>\n",
       "      <td>7721.0</td>\n",
       "      <td>0.479033</td>\n",
       "      <td>4.539174e-165</td>\n",
       "      <td>0.343968</td>\n",
       "      <td>3.215080</td>\n",
       "      <td>0.193292</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.367755</td>\n",
       "      <td>0.534824</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.010788</td>\n",
       "      <td>0.012550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332924</td>\n",
       "      <td>0.416805</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.132585</td>\n",
       "      <td>3.225362e-06</td>\n",
       "      <td>0.088783</td>\n",
       "      <td>1.576889</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.130725</td>\n",
       "      <td>0.105381</td>\n",
       "      <td>0.244562</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.008010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.418671</td>\n",
       "      <td>0.444346</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>7.074130e-03</td>\n",
       "      <td>0.090643</td>\n",
       "      <td>1.609925</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.119455</td>\n",
       "      <td>0.137525</td>\n",
       "      <td>0.241549</td>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.008080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.365027</td>\n",
       "      <td>0.389873</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.071116</td>\n",
       "      <td>4.092983e-02</td>\n",
       "      <td>0.075822</td>\n",
       "      <td>1.346689</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.119961</td>\n",
       "      <td>0.130641</td>\n",
       "      <td>0.235672</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.002773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.899901</td>\n",
       "      <td>0.929588</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.100287</td>\n",
       "      <td>9.704616e-04</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>1.609604</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.138862</td>\n",
       "      <td>0.160455</td>\n",
       "      <td>0.229767</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.001787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.915177</td>\n",
       "      <td>0.952805</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.118457</td>\n",
       "      <td>4.756270e-05</td>\n",
       "      <td>0.096434</td>\n",
       "      <td>1.712788</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.144190</td>\n",
       "      <td>0.183215</td>\n",
       "      <td>0.229767</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.035682</td>\n",
       "      <td>0.010561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.917321</td>\n",
       "      <td>0.941943</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.045757</td>\n",
       "      <td>3.925702e-01</td>\n",
       "      <td>0.069817</td>\n",
       "      <td>1.240038</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.108428</td>\n",
       "      <td>0.120898</td>\n",
       "      <td>0.231146</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.903762</td>\n",
       "      <td>0.945925</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.097329</td>\n",
       "      <td>1.511961e-03</td>\n",
       "      <td>0.084335</td>\n",
       "      <td>1.497889</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.130736</td>\n",
       "      <td>0.229767</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.004648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.392586</td>\n",
       "      <td>0.428519</td>\n",
       "      <td>402.0</td>\n",
       "      <td>6738.0</td>\n",
       "      <td>0.086017</td>\n",
       "      <td>6.829303e-03</td>\n",
       "      <td>0.083836</td>\n",
       "      <td>1.489036</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.146751</td>\n",
       "      <td>0.233999</td>\n",
       "      <td>0.007125</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.669574</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.102144</td>\n",
       "      <td>2.707543e-02</td>\n",
       "      <td>0.035386</td>\n",
       "      <td>1.361179</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.056878</td>\n",
       "      <td>0.048759</td>\n",
       "      <td>0.129304</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.011935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437350</td>\n",
       "      <td>0.452364</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.104844</td>\n",
       "      <td>2.152606e-02</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>1.528838</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.086604</td>\n",
       "      <td>0.074451</td>\n",
       "      <td>0.129118</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.022457</td>\n",
       "      <td>0.021881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.423558</td>\n",
       "      <td>0.434106</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.041883</td>\n",
       "      <td>8.539875e-01</td>\n",
       "      <td>0.037131</td>\n",
       "      <td>1.428294</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.120536</td>\n",
       "      <td>0.022287</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.003801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910993</td>\n",
       "      <td>0.938726</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.099575</td>\n",
       "      <td>3.347704e-02</td>\n",
       "      <td>0.043918</td>\n",
       "      <td>1.689363</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.081858</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.125909</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.928087</td>\n",
       "      <td>0.958299</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.116495</td>\n",
       "      <td>7.476934e-03</td>\n",
       "      <td>0.044571</td>\n",
       "      <td>1.714488</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.079114</td>\n",
       "      <td>0.066519</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.008303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.931294</td>\n",
       "      <td>0.950035</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.082742</td>\n",
       "      <td>1.178108e-01</td>\n",
       "      <td>0.039227</td>\n",
       "      <td>1.508936</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.068053</td>\n",
       "      <td>0.060322</td>\n",
       "      <td>0.126635</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.016472</td>\n",
       "      <td>0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.914426</td>\n",
       "      <td>0.950780</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.096707</td>\n",
       "      <td>4.216601e-02</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>1.680316</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.126941</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>0.022032</td>\n",
       "      <td>0.011287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>pwy_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.464834</td>\n",
       "      <td>0.501324</td>\n",
       "      <td>208.0</td>\n",
       "      <td>7793.0</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>4.486078e-03</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>1.463096</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.067010</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0.132760</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.563101</td>\n",
       "      <td>0.637382</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.125582</td>\n",
       "      <td>3.811548e-19</td>\n",
       "      <td>0.169997</td>\n",
       "      <td>1.618148</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.212218</td>\n",
       "      <td>0.188136</td>\n",
       "      <td>0.382965</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.007146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343267</td>\n",
       "      <td>0.442413</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.351020</td>\n",
       "      <td>1.111092e-146</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>3.603492</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.372244</td>\n",
       "      <td>0.454385</td>\n",
       "      <td>0.448187</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.013007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312953</td>\n",
       "      <td>0.459518</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.372570</td>\n",
       "      <td>3.439862e-165</td>\n",
       "      <td>0.291208</td>\n",
       "      <td>2.771908</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.346435</td>\n",
       "      <td>0.343572</td>\n",
       "      <td>0.469709</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.006650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.787649</td>\n",
       "      <td>0.949688</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.536603</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.447228</td>\n",
       "      <td>4.257009</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.450399</td>\n",
       "      <td>0.457495</td>\n",
       "      <td>0.569262</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.790155</td>\n",
       "      <td>0.966956</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.578043</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>4.683886</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.506214</td>\n",
       "      <td>0.517704</td>\n",
       "      <td>0.600245</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.804360</td>\n",
       "      <td>0.956319</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.295501</td>\n",
       "      <td>4.480900e-104</td>\n",
       "      <td>0.280244</td>\n",
       "      <td>2.667549</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.347227</td>\n",
       "      <td>0.390188</td>\n",
       "      <td>0.394809</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.753719</td>\n",
       "      <td>0.963620</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.532743</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.448519</td>\n",
       "      <td>4.269305</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.475328</td>\n",
       "      <td>0.494786</td>\n",
       "      <td>0.565424</td>\n",
       "      <td>0.007394</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.000716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>phe_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.242026</td>\n",
       "      <td>0.508111</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>0.544401</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.433465</td>\n",
       "      <td>4.126005</td>\n",
       "      <td>0.190138</td>\n",
       "      <td>0.433036</td>\n",
       "      <td>0.463503</td>\n",
       "      <td>0.572110</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.011559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>PO</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402396</td>\n",
       "      <td>0.510413</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.146697</td>\n",
       "      <td>3.823002e-17</td>\n",
       "      <td>0.056049</td>\n",
       "      <td>1.549263</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.091284</td>\n",
       "      <td>0.063834</td>\n",
       "      <td>0.180067</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.005489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300</td>\n",
       "      <td>NLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.432333</td>\n",
       "      <td>0.446141</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.057325</td>\n",
       "      <td>5.598094e-03</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>1.258468</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.076635</td>\n",
       "      <td>0.069606</td>\n",
       "      <td>0.162880</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.006889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Wikipedia,Size=300,Mean,Nouns,Adjectives</td>\n",
       "      <td>NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.377755</td>\n",
       "      <td>0.402534</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>2.996057e-05</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>1.341059</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>0.075882</td>\n",
       "      <td>0.161026</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.002705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Simple,Words,Linares Pontes,1-grams,Binary,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>0.911023</td>\n",
       "      <td>0.932815</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.092728</td>\n",
       "      <td>4.177707e-07</td>\n",
       "      <td>0.051436</td>\n",
       "      <td>1.421765</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.091296</td>\n",
       "      <td>0.078837</td>\n",
       "      <td>0.162497</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.001274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>N-Grams</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>Full,Words,1-grams,TFIDF</td>\n",
       "      <td>NLP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.928038</td>\n",
       "      <td>0.953054</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.101395</td>\n",
       "      <td>2.059890e-08</td>\n",
       "      <td>0.053235</td>\n",
       "      <td>1.471468</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.092664</td>\n",
       "      <td>0.082173</td>\n",
       "      <td>0.167467</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.003736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>LDA,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929357</td>\n",
       "      <td>0.946611</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.077238</td>\n",
       "      <td>4.637489e-05</td>\n",
       "      <td>0.046047</td>\n",
       "      <td>1.272805</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.079750</td>\n",
       "      <td>0.063910</td>\n",
       "      <td>0.158460</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.011990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Topic Models</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>NMF,Full,Topics=100</td>\n",
       "      <td>NLP</td>\n",
       "      <td>6</td>\n",
       "      <td>0.914042</td>\n",
       "      <td>0.942081</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.064845</td>\n",
       "      <td>1.082204e-03</td>\n",
       "      <td>0.048187</td>\n",
       "      <td>1.331945</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.082623</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.161098</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Mean</td>\n",
       "      <td>both</td>\n",
       "      <td>ppi_y</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>NLP</td>\n",
       "      <td>7</td>\n",
       "      <td>0.424750</td>\n",
       "      <td>0.467140</td>\n",
       "      <td>928.0</td>\n",
       "      <td>24723.0</td>\n",
       "      <td>0.100231</td>\n",
       "      <td>3.134468e-08</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>1.281314</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.088102</td>\n",
       "      <td>0.074891</td>\n",
       "      <td>0.166537</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method Species Objective Curated                                   Hyperparameters Group  Order    mean_1    mean_0     n_1      n_0        ks        ks_pval       auc     ratio  baseline    f1_max    f5_max    f2_max    f1_std    f5_std    f2_std\n",
       "0              PO   intra     pwy_y    true                                              None   NLP      0  0.431464  0.521843    43.0   2168.0  0.207350   4.551976e-02  0.055229  2.839788  0.038154  0.100000  0.089686  0.138889  0.036000  0.025649  0.024561\n",
       "1         Doc2Vec   intra     pwy_y    true                                Wikipedia,Size=300   NLP      1  0.395605  0.438123    43.0   2168.0  0.188535   8.706231e-02  0.052982  2.724235  0.038154  0.087912  0.106383  0.128617  0.018762  0.013471  0.012672\n",
       "2        Word2Vec   intra     pwy_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.355652  0.400400    43.0   2168.0  0.172134   1.456672e-01  0.038639  1.986783  0.038154  0.112676  0.130435  0.159744  0.022812  0.043413  0.012671\n",
       "3         N-Grams   intra     pwy_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.877587  0.928449    43.0   2168.0  0.188203   8.803716e-02  0.047849  2.460329  0.038154  0.119760  0.096618  0.168919  0.028571  0.003846  0.033568\n",
       "4         N-Grams   intra     pwy_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.884761  0.948228    43.0   2168.0  0.235905   1.506733e-02  0.051735  2.660127  0.038154  0.130719  0.103520  0.197568  0.012367  0.022616  0.001608\n",
       "5    Topic Models   intra     pwy_y    true                               LDA,Full,Topics=100   NLP      5  0.896164  0.941884    43.0   2168.0  0.188642   8.677684e-02  0.040714  2.093456  0.038154  0.087719  0.084746  0.114223  0.025377  0.010449  0.012277\n",
       "6    Topic Models   intra     pwy_y    true                               NMF,Full,Topics=100   NLP      6  0.853158  0.945058    43.0   2168.0  0.164099   1.841856e-01  0.063421  3.260999  0.038154  0.130435  0.145631  0.161290  0.036963  0.025008  0.054745\n",
       "7            Mean   intra     pwy_y    true                                              None   NLP      7  0.388355  0.455111    43.0   2168.0  0.178634   1.194929e-01  0.060064  3.088398  0.038154  0.129870  0.139665  0.141343  0.014936  0.051908  0.018268\n",
       "8              PO   intra     phe_y    true                                              None   NLP      0  0.456497  0.497324   925.0   7721.0  0.065172   1.000000e+00  0.134028  1.252766  0.193292  0.199094  0.149298  0.375270  0.000068  0.000473  0.000282\n",
       "9         Doc2Vec   intra     phe_y    true                                Wikipedia,Size=300   NLP      1  0.363031  0.443214   925.0   7721.0  0.323027   2.715983e-75  0.315990  2.953569  0.193292  0.326513  0.370949  0.434913  0.016763  0.013415  0.018140\n",
       "10       Word2Vec   intra     phe_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.294413  0.421294   925.0   7721.0  0.329059   4.089319e-78  0.264165  2.469155  0.193292  0.333042  0.325451  0.448254  0.006888  0.006861  0.007612\n",
       "11        N-Grams   intra     phe_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.824868  0.943613   925.0   7721.0  0.467884  1.706347e-157  0.364263  3.404777  0.193292  0.390805  0.383971  0.527833  0.000929  0.004768  0.003396\n",
       "12        N-Grams   intra     phe_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.826941  0.962995   925.0   7721.0  0.522575  2.324686e-196  0.424304  3.965981  0.193292  0.461926  0.466340  0.563055  0.002787  0.003731  0.008128\n",
       "13   Topic Models   intra     phe_y    true                               LDA,Full,Topics=100   NLP      5  0.851849  0.949971   925.0   7721.0  0.214741   1.641200e-33  0.213541  1.995970  0.193292  0.276718  0.296538  0.375528  0.004786  0.027310  0.001842\n",
       "14   Topic Models   intra     phe_y    true                               NMF,Full,Topics=100   NLP      6  0.794179  0.959441   925.0   7721.0  0.482498  1.847122e-167  0.380781  3.559169  0.193292  0.436879  0.436583  0.536918  0.005051  0.008385  0.003620\n",
       "15           Mean   intra     phe_y    true                                              None   NLP      7  0.249977  0.474319   925.0   7721.0  0.479033  4.539174e-165  0.343968  3.215080  0.193292  0.379032  0.367755  0.534824  0.004963  0.005785  0.001239\n",
       "16             PO   intra     ppi_y    true                                              None   NLP      0  0.332924  0.351720   402.0   5826.0  0.073876   3.123891e-02  0.089652  1.388933  0.121267  0.130725  0.105381  0.258286  0.002825  0.001049  0.007427\n",
       "17        Doc2Vec   intra     ppi_y    true                                Wikipedia,Size=300   NLP      1  0.418671  0.437196   402.0   5826.0  0.055782   1.849863e-01  0.095075  1.472958  0.121267  0.126926  0.137525  0.264770  0.000013  0.006383  0.002321\n",
       "18       Word2Vec   intra     ppi_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.365027  0.390007   402.0   5826.0  0.066921   6.564002e-02  0.083925  1.300204  0.121267  0.127889  0.131894  0.263016  0.012664  0.018895  0.007847\n",
       "19        N-Grams   intra     ppi_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.899901  0.928662   402.0   5826.0  0.094435   2.266630e-03  0.098506  1.526102  0.121267  0.144724  0.160455  0.256508  0.003552  0.013040  0.007518\n",
       "20        N-Grams   intra     ppi_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.915177  0.949935   402.0   5826.0  0.104398   5.045929e-04  0.102639  1.590140  0.121267  0.148043  0.183215  0.256508  0.002748  0.007787  0.004532\n",
       "21   Topic Models   intra     ppi_y    true                               LDA,Full,Topics=100   NLP      5  0.917321  0.940855   402.0   5826.0  0.044592   4.300630e-01  0.078209  1.211652  0.121267  0.121947  0.127505  0.256876  0.004793  0.017687  0.008095\n",
       "22   Topic Models   intra     ppi_y    true                               NMF,Full,Topics=100   NLP      6  0.903762  0.942864   402.0   5826.0  0.090879   3.735598e-03  0.092286  1.429751  0.121267  0.145749  0.135960  0.256508  0.013440  0.015978  0.003006\n",
       "23           Mean   intra     ppi_y    true                                              None   NLP      7  0.392586  0.422885   402.0   5826.0  0.077409   2.083170e-02  0.090298  1.398938  0.121267  0.135294  0.146751  0.260032  0.003654  0.002527  0.001636\n",
       "24             PO   intra     pwy_y   false                                              None   NLP      0  0.638670  0.637591   191.0   6845.0  0.053672   6.376717e-01  0.036731  1.353096  0.052857  0.060498  0.055724  0.127369  0.001736  0.003225  0.000515\n",
       "25        Doc2Vec   intra     pwy_y   false                                Wikipedia,Size=300   NLP      1  0.436422  0.446506   191.0   6845.0  0.101028   4.187175e-02  0.040717  1.499933  0.052857  0.090523  0.075939  0.130455  0.013120  0.012950  0.010412\n",
       "26       Word2Vec   intra     pwy_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.420228  0.432504   191.0   6845.0  0.046469   7.989763e-01  0.039401  1.451449  0.052857  0.071685  0.092402  0.126610  0.001778  0.026701  0.007254\n",
       "27        N-Grams   intra     pwy_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.906312  0.936126   191.0   6845.0  0.099053   4.858056e-02  0.046521  1.713726  0.052857  0.085561  0.073206  0.130426  0.003702  0.004823  0.000795\n",
       "28        N-Grams   intra     pwy_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.923859  0.955432   191.0   6845.0  0.116888   1.140524e-02  0.046893  1.727430  0.052857  0.082902  0.068847  0.137947  0.008604  0.004280  0.009467\n",
       "29   Topic Models   intra     pwy_y   false                               LDA,Full,Topics=100   NLP      5  0.929261  0.947597   191.0   6845.0  0.072246   2.729650e-01  0.041084  1.513422  0.052857  0.069246  0.061107  0.129140  0.004158  0.006139  0.017743\n",
       "30   Topic Models   intra     pwy_y   false                               NMF,Full,Topics=100   NLP      6  0.909082  0.947824   191.0   6845.0  0.108647   2.295480e-02  0.046539  1.714390  0.052857  0.075227  0.075630  0.134824  0.004841  0.008078  0.001865\n",
       "31           Mean   intra     pwy_y   false                                              None   NLP      7  0.456037  0.492173   191.0   6845.0  0.125605   5.147917e-03  0.039881  1.469109  0.052857  0.070652  0.072698  0.138356  0.015320  0.021222  0.001602\n",
       "32             PO   intra     phe_y   false                                              None   NLP      0  0.563101  0.637382  1527.0  13008.0  0.125582   3.811548e-19  0.169997  1.618148  0.190138  0.212218  0.188136  0.382965  0.004591  0.007188  0.006301\n",
       "33        Doc2Vec   intra     phe_y   false                                Wikipedia,Size=300   NLP      1  0.343267  0.442413  1527.0  13008.0  0.351020  1.111092e-146  0.378571  3.603492  0.190138  0.372244  0.454385  0.448187  0.011125  0.016249  0.010135\n",
       "34       Word2Vec   intra     phe_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.312953  0.459518  1527.0  13008.0  0.372570  3.439862e-165  0.291208  2.771908  0.190138  0.346435  0.343572  0.469709  0.001055  0.002313  0.002488\n",
       "35        N-Grams   intra     phe_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.787649  0.949688  1527.0  13008.0  0.536603   0.000000e+00  0.447228  4.257009  0.190138  0.450399  0.457495  0.569262  0.005304  0.002000  0.007375\n",
       "36        N-Grams   intra     phe_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.790155  0.966956  1527.0  13008.0  0.578043   0.000000e+00  0.492074  4.683886  0.190138  0.506214  0.517704  0.600245  0.005892  0.007600  0.008027\n",
       "37   Topic Models   intra     phe_y   false                               LDA,Full,Topics=100   NLP      5  0.804360  0.956319  1527.0  13008.0  0.295501  4.480900e-104  0.280244  2.667549  0.190138  0.347227  0.390188  0.394809  0.003484  0.004991  0.004973\n",
       "38   Topic Models   intra     phe_y   false                               NMF,Full,Topics=100   NLP      6  0.753719  0.963620  1527.0  13008.0  0.532743   0.000000e+00  0.448519  4.269305  0.190138  0.475328  0.494786  0.565424  0.000641  0.002146  0.009565\n",
       "39           Mean   intra     phe_y   false                                              None   NLP      7  0.242026  0.508111  1527.0  13008.0  0.544401   0.000000e+00  0.433465  4.126005  0.190138  0.433036  0.463503  0.572110  0.003855  0.008761  0.004119\n",
       "40             PO   intra     ppi_y   false                                              None   NLP      0  0.402396  0.462912   928.0  21897.0  0.086010   3.806476e-06  0.056963  1.401056  0.078137  0.091563  0.064053  0.182691  0.004185  0.000907  0.005420\n",
       "41        Doc2Vec   intra     ppi_y   false                                Wikipedia,Size=300   NLP      1  0.432333  0.442286   928.0  21897.0  0.043791   6.578137e-02  0.048782  1.199834  0.078137  0.081638  0.070258  0.178118  0.002048  0.000174  0.004265\n",
       "42       Word2Vec   intra     ppi_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.377755  0.399543   928.0  21897.0  0.070484   2.880137e-04  0.052405  1.288955  0.078137  0.091554  0.078125  0.176699  0.003292  0.010510  0.005275\n",
       "43        N-Grams   intra     ppi_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.911023  0.930406   928.0  21897.0  0.085396   4.590999e-06  0.055304  1.360244  0.078137  0.095102  0.080074  0.177923  0.003620  0.007242  0.007826\n",
       "44        N-Grams   intra     ppi_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.928038  0.950041   928.0  21897.0  0.093126   3.934202e-07  0.056735  1.395457  0.078137  0.095262  0.083008  0.180806  0.004720  0.004004  0.002612\n",
       "45   Topic Models   intra     ppi_y   false                               LDA,Full,Topics=100   NLP      5  0.929357  0.944424   928.0  21897.0  0.065225   1.026290e-03  0.049780  1.224382  0.078137  0.085874  0.067783  0.174989  0.000316  0.008336  0.000505\n",
       "46   Topic Models   intra     ppi_y   false                               NMF,Full,Topics=100   NLP      6  0.914042  0.938714   928.0  21897.0  0.058915   4.140151e-03  0.051993  1.278823  0.078137  0.085540  0.075620  0.176230  0.004932  0.007128  0.007598\n",
       "47           Mean   intra     ppi_y   false                                              None   NLP      7  0.424750  0.460189   928.0  21897.0  0.087103   2.717755e-06  0.049649  1.221160  0.078137  0.091717  0.075219  0.179182  0.003563  0.000506  0.002073\n",
       "48             PO   inter     pwy_y    true                                              None   NLP      0  0.855693  0.836912     6.0    198.0  0.439394   1.571793e-01  0.026412  0.898022  0.057143  0.100000  0.080645  0.136986  0.028571  0.020338  0.047554\n",
       "49        Doc2Vec   inter     pwy_y    true                                Wikipedia,Size=300   NLP      1  0.417830  0.507635     6.0    198.0  0.606061   1.465718e-02  0.075628  2.571354  0.057143  0.200000  0.227273  0.308642  0.065426  0.083742  0.076450\n",
       "50       Word2Vec   inter     pwy_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.405766  0.402931     6.0    198.0  0.292929   6.105562e-01  0.030695  1.043644  0.057143  0.095238  0.062893  0.196078  0.049242  0.037054  0.087286\n",
       "51        N-Grams   inter     pwy_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.942892  0.945122     6.0    198.0  0.257576   7.598563e-01  0.032533  1.106114  0.057143  0.100000  0.080645  0.170455  0.063855  0.066066  0.065537\n",
       "52        N-Grams   inter     pwy_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.967655  0.969529     6.0    198.0  0.212121   9.136105e-01  0.032609  1.108705  0.057143  0.125000  0.090909  0.200000  0.015009  0.009566  0.034746\n",
       "53   Topic Models   inter     pwy_y    true                               LDA,Full,Topics=100   NLP      5  0.988121  0.966385     6.0    198.0  0.338384   4.282710e-01  0.026403  0.897705  0.057143  0.083333  0.053763  0.185185  0.031255  0.020914  0.060884\n",
       "54   Topic Models   inter     pwy_y    true                               NMF,Full,Topics=100   NLP      6  0.994289  0.960841     6.0    198.0  0.287879   6.322689e-01  0.025963  0.882754  0.057143  0.066667  0.043290  0.144928  0.010000  0.007122  0.014881\n",
       "55           Mean   inter     pwy_y    true                                              None   NLP      7  0.475849  0.521824     6.0    198.0  0.272727   6.971042e-01  0.033979  1.155289  0.057143  0.111111  0.075758  0.208333  0.010571  0.007024  0.021053\n",
       "56             PO   inter     pwy_y   false                                              None   NLP      0  0.840647  0.900504    17.0    948.0  0.450112   1.322443e-03  0.027904  1.583938  0.034623  0.072727  0.047011  0.160550  0.010784  0.001744  0.027597\n",
       "57        Doc2Vec   inter     pwy_y   false                                Wikipedia,Size=300   NLP      1  0.447781  0.494663    17.0    948.0  0.299516   7.972678e-02  0.029271  1.661581  0.034623  0.083333  0.065359  0.140187  0.007177  0.006303  0.031840\n",
       "58       Word2Vec   inter     pwy_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.460972  0.445671    17.0    948.0  0.144391   8.288482e-01  0.015797  0.896732  0.034623  0.042424  0.027650  0.091864  0.005947  0.005234  0.002943\n",
       "59        N-Grams   inter     pwy_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.963590  0.957494    17.0    948.0  0.298709   8.111984e-02  0.017129  0.972343  0.034623  0.078431  0.065359  0.098039  0.003788  0.010119  0.013082\n",
       "60        N-Grams   inter     pwy_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.975596  0.979002    17.0    948.0  0.108960   9.757130e-01  0.019966  1.133342  0.034623  0.071429  0.072464  0.111111  0.009153  0.025672  0.004814\n",
       "61   Topic Models   inter     pwy_y   false                               LDA,Full,Topics=100   NLP      5  0.954137  0.967640    17.0    948.0  0.227290   3.051204e-01  0.020746  1.177622  0.034623  0.080000  0.102041  0.106534  0.019481  0.012542  0.018836\n",
       "62   Topic Models   inter     pwy_y   false                               NMF,Full,Topics=100   NLP      6  0.974466  0.972121    17.0    948.0  0.172189   6.426587e-01  0.016371  0.929281  0.034623  0.071429  0.081967  0.082285  0.048352  0.071629  0.013468\n",
       "63           Mean   inter     pwy_y   false                                              None   NLP      7  0.563662  0.567397    17.0    948.0  0.104120   9.843916e-01  0.018465  1.048158  0.034623  0.049587  0.034642  0.094697  0.007147  0.004762  0.014286\n",
       "64             PO    both     pwy_y    true                                              None   NLP      0  0.483410  0.548210    49.0   2366.0  0.173349   9.873263e-02  0.051063  2.516694  0.039773  0.094488  0.085837  0.133730  0.003501  0.006360  0.011144\n",
       "65        Doc2Vec    both     pwy_y    true                                Wikipedia,Size=300   NLP      1  0.398326  0.443940    49.0   2366.0  0.220686   1.557512e-02  0.051355  2.531049  0.039773  0.085106  0.094340  0.124269  0.040056  0.072720  0.058072\n",
       "66       Word2Vec    both     pwy_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.361789  0.400612    49.0   2366.0  0.170269   1.096119e-01  0.036545  1.801143  0.039773  0.103896  0.124224  0.142857  0.020185  0.048243  0.025897\n",
       "67        N-Grams    both     pwy_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.885584  0.929844    49.0   2366.0  0.157831   1.639917e-01  0.045400  2.237552  0.039773  0.112360  0.093897  0.153846  0.008929  0.021930  0.020265\n",
       "68        N-Grams    both     pwy_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.894912  0.950011    49.0   2366.0  0.203176   3.250830e-02  0.048832  2.406713  0.039773  0.125000  0.101420  0.182584  0.064251  0.060214  0.070483\n",
       "69   Topic Models    both     pwy_y    true                               LDA,Full,Topics=100   NLP      5  0.907424  0.943935    49.0   2366.0  0.154691   1.806534e-01  0.038193  1.882393  0.039773  0.083333  0.076923  0.112540  0.015574  0.015390  0.015685\n",
       "70   Topic Models    both     pwy_y    true                               NMF,Full,Topics=100   NLP      6  0.870440  0.946379    49.0   2366.0  0.139717   2.787522e-01  0.057951  2.856165  0.039773  0.122449  0.137615  0.145631  0.042593  0.044444  0.054487\n",
       "71           Mean    both     pwy_y    true                                              None   NLP      7  0.399068  0.460694    49.0   2366.0  0.151793   1.971845e-01  0.055830  2.751636  0.039773  0.120482  0.135135  0.129032  0.019504  0.014706  0.005076\n",
       "72             PO    both     phe_y    true                                              None   NLP      0  0.456497  0.497324   925.0   7721.0  0.065172   1.000000e+00  0.134028  1.252766  0.193292  0.199094  0.149298  0.375270  0.000933  0.006741  0.005089\n",
       "73        Doc2Vec    both     phe_y    true                                Wikipedia,Size=300   NLP      1  0.363031  0.443214   925.0   7721.0  0.323027   2.715983e-75  0.315990  2.953569  0.193292  0.326513  0.370949  0.434913  0.000850  0.003628  0.000726\n",
       "74       Word2Vec    both     phe_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.294413  0.421294   925.0   7721.0  0.329059   4.089319e-78  0.264165  2.469155  0.193292  0.333042  0.325451  0.448254  0.004079  0.004260  0.000220\n",
       "75        N-Grams    both     phe_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.824868  0.943613   925.0   7721.0  0.467884  1.706347e-157  0.364263  3.404777  0.193292  0.390805  0.383971  0.527833  0.005152  0.000214  0.013416\n",
       "76        N-Grams    both     phe_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.826941  0.962995   925.0   7721.0  0.522575  2.324686e-196  0.424304  3.965981  0.193292  0.461926  0.466340  0.563055  0.002204  0.004853  0.001554\n",
       "77   Topic Models    both     phe_y    true                               LDA,Full,Topics=100   NLP      5  0.851849  0.949971   925.0   7721.0  0.214741   1.641200e-33  0.213541  1.995970  0.193292  0.276718  0.296538  0.375528  0.011897  0.020665  0.008798\n",
       "78   Topic Models    both     phe_y    true                               NMF,Full,Topics=100   NLP      6  0.794179  0.959441   925.0   7721.0  0.482498  1.847122e-167  0.380781  3.559169  0.193292  0.436879  0.436583  0.536918  0.010394  0.011069  0.008031\n",
       "79           Mean    both     phe_y    true                                              None   NLP      7  0.249977  0.474319   925.0   7721.0  0.479033  4.539174e-165  0.343968  3.215080  0.193292  0.379032  0.367755  0.534824  0.001787  0.010788  0.012550\n",
       "80             PO    both     ppi_y    true                                              None   NLP      0  0.332924  0.416805   402.0   6738.0  0.132585   3.225362e-06  0.088783  1.576889  0.106603  0.130725  0.105381  0.244562  0.004131  0.006684  0.008010\n",
       "81        Doc2Vec    both     ppi_y    true                                Wikipedia,Size=300   NLP      1  0.418671  0.444346   402.0   6738.0  0.085749   7.074130e-03  0.090643  1.609925  0.106603  0.119455  0.137525  0.241549  0.010847  0.006245  0.008080\n",
       "82       Word2Vec    both     ppi_y    true          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.365027  0.389873   402.0   6738.0  0.071116   4.092983e-02  0.075822  1.346689  0.106603  0.119961  0.130641  0.235672  0.002478  0.005308  0.002773\n",
       "83        N-Grams    both     ppi_y    true  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.899901  0.929588   402.0   6738.0  0.100287   9.704616e-04  0.090625  1.609604  0.106603  0.138862  0.160455  0.229767  0.005427  0.017804  0.001787\n",
       "84        N-Grams    both     ppi_y    true                          Full,Words,1-grams,TFIDF   NLP      4  0.915177  0.952805   402.0   6738.0  0.118457   4.756270e-05  0.096434  1.712788  0.106603  0.144190  0.183215  0.229767  0.006397  0.035682  0.010561\n",
       "85   Topic Models    both     ppi_y    true                               LDA,Full,Topics=100   NLP      5  0.917321  0.941943   402.0   6738.0  0.045757   3.925702e-01  0.069817  1.240038  0.106603  0.108428  0.120898  0.231146  0.006160  0.000905  0.000746\n",
       "86   Topic Models    both     ppi_y    true                               NMF,Full,Topics=100   NLP      6  0.903762  0.945925   402.0   6738.0  0.097329   1.511961e-03  0.084335  1.497889  0.106603  0.140488  0.130736  0.229767  0.001064  0.006414  0.004648\n",
       "87           Mean    both     ppi_y    true                                              None   NLP      7  0.392586  0.428519   402.0   6738.0  0.086017   6.829303e-03  0.083836  1.489036  0.106603  0.131429  0.146751  0.233999  0.007125  0.006248  0.003639\n",
       "88             PO    both     pwy_y   false                                              None   NLP      0  0.655178  0.669574   208.0   7793.0  0.102144   2.707543e-02  0.035386  1.361179  0.050676  0.056878  0.048759  0.129304  0.005594  0.005055  0.011935\n",
       "89        Doc2Vec    both     pwy_y   false                                Wikipedia,Size=300   NLP      1  0.437350  0.452364   208.0   7793.0  0.104844   2.152606e-02  0.039745  1.528838  0.050676  0.086604  0.074451  0.129118  0.022119  0.022457  0.021881\n",
       "90       Word2Vec    both     pwy_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.423558  0.434106   208.0   7793.0  0.041883   8.539875e-01  0.037131  1.428294  0.050676  0.067568  0.089286  0.120536  0.022287  0.030303  0.003801\n",
       "91        N-Grams    both     pwy_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.910993  0.938726   208.0   7793.0  0.099575   3.347704e-02  0.043918  1.689363  0.050676  0.081858  0.071429  0.125909  0.009722  0.000278  0.009602\n",
       "92        N-Grams    both     pwy_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.928087  0.958299   208.0   7793.0  0.116495   7.476934e-03  0.044571  1.714488  0.050676  0.079114  0.066519  0.134116  0.009167  0.009429  0.008303\n",
       "93   Topic Models    both     pwy_y   false                               LDA,Full,Topics=100   NLP      5  0.931294  0.950035   208.0   7793.0  0.082742   1.178108e-01  0.039227  1.508936  0.050676  0.068053  0.060322  0.126635  0.015572  0.016472  0.000536\n",
       "94   Topic Models    both     pwy_y   false                               NMF,Full,Topics=100   NLP      6  0.914426  0.950780   208.0   7793.0  0.096707   4.216601e-02  0.043683  1.680316  0.050676  0.072727  0.072581  0.126941  0.010165  0.022032  0.011287\n",
       "95           Mean    both     pwy_y   false                                              None   NLP      7  0.464834  0.501324   208.0   7793.0  0.121727   4.486078e-03  0.038036  1.463096  0.050676  0.067010  0.070755  0.132760  0.004400  0.013249  0.004072\n",
       "96             PO    both     phe_y   false                                              None   NLP      0  0.563101  0.637382  1527.0  13008.0  0.125582   3.811548e-19  0.169997  1.618148  0.190138  0.212218  0.188136  0.382965  0.005890  0.016225  0.007146\n",
       "97        Doc2Vec    both     phe_y   false                                Wikipedia,Size=300   NLP      1  0.343267  0.442413  1527.0  13008.0  0.351020  1.111092e-146  0.378571  3.603492  0.190138  0.372244  0.454385  0.448187  0.003427  0.002096  0.013007\n",
       "98       Word2Vec    both     phe_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.312953  0.459518  1527.0  13008.0  0.372570  3.439862e-165  0.291208  2.771908  0.190138  0.346435  0.343572  0.469709  0.010813  0.004224  0.006650\n",
       "99        N-Grams    both     phe_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.787649  0.949688  1527.0  13008.0  0.536603   0.000000e+00  0.447228  4.257009  0.190138  0.450399  0.457495  0.569262  0.000009  0.009337  0.001444\n",
       "100       N-Grams    both     phe_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.790155  0.966956  1527.0  13008.0  0.578043   0.000000e+00  0.492074  4.683886  0.190138  0.506214  0.517704  0.600245  0.003469  0.001650  0.001580\n",
       "101  Topic Models    both     phe_y   false                               LDA,Full,Topics=100   NLP      5  0.804360  0.956319  1527.0  13008.0  0.295501  4.480900e-104  0.280244  2.667549  0.190138  0.347227  0.390188  0.394809  0.000852  0.002972  0.002463\n",
       "102  Topic Models    both     phe_y   false                               NMF,Full,Topics=100   NLP      6  0.753719  0.963620  1527.0  13008.0  0.532743   0.000000e+00  0.448519  4.269305  0.190138  0.475328  0.494786  0.565424  0.007394  0.009393  0.000716\n",
       "103          Mean    both     phe_y   false                                              None   NLP      7  0.242026  0.508111  1527.0  13008.0  0.544401   0.000000e+00  0.433465  4.126005  0.190138  0.433036  0.463503  0.572110  0.008409  0.005964  0.011559\n",
       "104            PO    both     ppi_y   false                                              None   NLP      0  0.402396  0.510413   928.0  24723.0  0.146697   3.823002e-17  0.056049  1.549263  0.069830  0.091284  0.063834  0.180067  0.003471  0.002514  0.005489\n",
       "105       Doc2Vec    both     ppi_y   false                                Wikipedia,Size=300   NLP      1  0.432333  0.446141   928.0  24723.0  0.057325   5.598094e-03  0.045529  1.258468  0.069830  0.076635  0.069606  0.162880  0.003221  0.001991  0.006889\n",
       "106      Word2Vec    both     ppi_y   false          Wikipedia,Size=300,Mean,Nouns,Adjectives   NLP      2  0.377755  0.402534   928.0  24723.0  0.078804   2.996057e-05  0.048517  1.341059  0.069830  0.088085  0.075882  0.161026  0.002005  0.001175  0.002705\n",
       "107       N-Grams    both     ppi_y   false  Simple,Words,Linares Pontes,1-grams,Binary,TFIDF   NLP      3  0.911023  0.932815   928.0  24723.0  0.092728   4.177707e-07  0.051436  1.421765  0.069830  0.091296  0.078837  0.162497  0.005376  0.002131  0.001274\n",
       "108       N-Grams    both     ppi_y   false                          Full,Words,1-grams,TFIDF   NLP      4  0.928038  0.953054   928.0  24723.0  0.101395   2.059890e-08  0.053235  1.471468  0.069830  0.092664  0.082173  0.167467  0.003345  0.000564  0.003736\n",
       "109  Topic Models    both     ppi_y   false                               LDA,Full,Topics=100   NLP      5  0.929357  0.946611   928.0  24723.0  0.077238   4.637489e-05  0.046047  1.272805  0.069830  0.079750  0.063910  0.158460  0.007641  0.007921  0.011990\n",
       "110  Topic Models    both     ppi_y   false                               NMF,Full,Topics=100   NLP      6  0.914042  0.942081   928.0  24723.0  0.064845   1.082204e-03  0.048187  1.331945  0.069830  0.082623  0.072300  0.161098  0.000696  0.004524  0.001576\n",
       "111          Mean    both     ppi_y   false                                              None   NLP      7  0.424750  0.467140   928.0  24723.0  0.100231   3.134468e-08  0.046355  1.281314  0.069830  0.088102  0.074891  0.166537  0.000834  0.008600  0.004926"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dfs = []\n",
    "for s,c,q in itertools.product(species,curated,question):\n",
    "    TABLE = tables[c][q][s]\n",
    "    results = pd.DataFrame(TABLE).transpose()\n",
    "    columns = flatten([\"Species\", \"Objective\",\"Curated\",\"Hyperparameters\",\"Group\",\"Order\",results.columns])\n",
    "    results[\"Hyperparameters\"] = \"\"\n",
    "    results[\"Group\"] = \"NLP\"\n",
    "    results[\"Order\"] = np.arange(results.shape[0])\n",
    "    results[\"Species\"] = s.lower()\n",
    "    results[\"Objective\"] = q.lower()\n",
    "    results[\"Curated\"] = str(c).lower()\n",
    "    results = results[columns]\n",
    "    results.reset_index(inplace=True)\n",
    "    results = results.rename({\"index\":\"Method\"}, axis=\"columns\")\n",
    "    hyperparam_sep = \":\"\n",
    "    results[\"Hyperparameters\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[1] if hyperparam_sep in x else \"None\")\n",
    "    results[\"Method\"] = results[\"Method\"].map(lambda x: x.split(hyperparam_sep)[0])\n",
    "    result_dfs.append(results)\n",
    "\n",
    "results = pd.concat(result_dfs)\n",
    "results.reset_index(inplace=True, drop=True)\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"part_6_full_table.csv\"), index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stophere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_5\"></a>\n",
    "# Part 5. Cluster Analysis\n",
    "The purpose of this section is to look at different ways that the embeddings obtained for the dataset of phenotype descriptions can be used to cluster or organize the genes to which those phenotypes are mapped into subgroups or representations. These approaches include generating topic models from the data, and doing agglomerative clustering to find clusters to which each gene belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this whole section doesn't really need to be run for the interactions or orthologs objectives, it only applies\n",
    "# to the pathways and phenotypes ones.\n",
    "\n",
    "# Things used by this section\n",
    "if OBJECTIVE == \"phenotypes\":\n",
    "    groups = phe_subsets_groups\n",
    "else:\n",
    "    groups = kegg_groups\n",
    "id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic_modeling\"></a>\n",
    "### Approach 1: Topic modeling based on n-grams with a reduced vocabulary\n",
    "Topic modelling learns a set of word probability distributions from the dataset of text descriptions, which represent distinct topics which are present in the dataset. Each text description can then be represented as a discrete probability distribution over the learned topics based on the probability that a given piece of text belongs to each particular topics. This is a form of data reduction because a high dimensionsal bag-of-words can be represented as a vector of *k* probabilities where *k* is the number of topics. The main advantages of topic modelling over clustering is that topic modelling provides soft classifications that can be additionally interpreted, rather than hard classifications into a single cluster. Topic models are also explainable, because the word probability distributions for that topic can be used to determine which words are most representative of any given topic. One problem with topic modelling is that is uses the n-grams embeddings to semantic similarity between different words is not accounted for. To help alleviate this, this section uses implementations of some existing algorithms to compress the vocabulary as a preprocessing step based on word distance matrices generated using word embeddings.\n",
    "\n",
    "Topic models define topics present in a dataset of texts as word or n-gram probability distributions. These models represent each instance of text then as being composed of or generated as as mixture of these topics. The vector for each text that indicates which fraction of that text is generated by a each topic is of length *n* where *n* is the number of topics, and can be used as a reduced dimensionality of the text, with a much smaller vector length than the n-grams embedding itself. Therefore we can build a topic model of the data with 100 topics for example in order to then represent each description in the dataset as a a vector of length 100. This section constructs topic models from the n-gram representations of the dataset and selects different values for the number of topics in order to find a value that works well during the grid search over the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a list of texts to create a topic model from, from one of the processed description dictionaries above. \n",
    "texts = [description for i,description in descriptions_linares_pontes.items()]\n",
    "\n",
    "# Creating and fitting the topic model, either NFM or LDA.\n",
    "number_of_topics = 42\n",
    "seed = 0\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", max_df=0.95, min_df=2, lowercase=False)\n",
    "features = vectorizer.fit_transform(texts)\n",
    "cls = NMF(n_components=number_of_topics, random_state=seed)\n",
    "cls.fit(features)\n",
    "\n",
    "# Function for retrieving the topic vectors for a list of text descriptions.\n",
    "def get_topic_embeddings(texts, model, vectorizer):\n",
    "    ngrams_vectors = vectorizer.transform(texts).toarray()\n",
    "    topic_vectors = model.transform(ngrams_vectors)\n",
    "    return(topic_vectors)\n",
    "    \n",
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_topic_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    texts = [descriptions_linares_pontes[i] for i in ids]\n",
    "    topic_vectors = get_topic_embeddings(texts, cls, vectorizer)\n",
    "    mean_topic_vector = np.mean(topic_vectors, axis=0)\n",
    "    group_to_topic_vector[group_id] = mean_topic_vector\n",
    "    \n",
    "tm_df = pd.DataFrame(group_to_topic_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "if OBJECTIVE == \"phenotypes\":\n",
    "    filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "    lmtm_df = pd.read_csv(filename)    \n",
    "    columns_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "    tm_df = tm_df[columns_in_order]\n",
    "    \n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "tm_df[\"idxmax\"] = tm_df.idxmax(axis = 1)\n",
    "tm_df[\"idxmax\"] = tm_df[\"idxmax\"].apply(lambda x: tm_df.columns.get_loc(x))\n",
    "tm_df = tm_df.sort_values(by=\"idxmax\")\n",
    "tm_df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "tm_df = tm_df.reset_index(drop=False).rename({\"index\":\"topic\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "tm_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_topic_modeling.csv\"), index=False)\n",
    "tm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Describing what the most representative tokens for each topic in the model are.\n",
    "num_top_words = 2\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    print(i,end=\": \")\n",
    "    for fid in topic_vec.argsort()[-1:-num_top_words-1:-1]:\n",
    "        word = feature_names[fid]\n",
    "        word = \" \".join(unreduce_lp[word])\n",
    "        print(word, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "### Approach 2: Agglomerative clustering and comparison to predefined groups\n",
    "This clustering approach uses agglomerative clustering to cluster the genes into a fixed number of clusters based off the distances between their embedding representations using all of the above methods. Clustering into a fixed number of clusters allows for clustering into a similar number of groups as a present in some existing grouping of the data, such as phenotype categories or biochemical pathways, and then determining if the clusters obtained are at all similar to the groupings that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate the numpy array where values are mean distance percentiles between all the methods.\n",
    "mean_pct_array = name_to_array[\"Mean\"]\n",
    "to_id = array_index_to_id\n",
    "\n",
    "# Do agglomerative clustering based on that distance matrix.\n",
    "number_of_clusters = 42\n",
    "ac = AgglomerativeClustering(n_clusters=number_of_clusters, linkage=\"complete\", affinity=\"precomputed\")\n",
    "clustering = ac.fit(mean_pct_array)\n",
    "id_to_cluster = {}\n",
    "cluster_to_ids = defaultdict(list)\n",
    "for idx,c in enumerate(clustering.labels_):\n",
    "    id_to_cluster[to_id[idx]] = c\n",
    "    cluster_to_ids[c].append(to_id[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_cluster_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    \n",
    "    mean_cluster_vector = np.zeros(number_of_clusters)\n",
    "    for i in ids:\n",
    "        cluster = id_to_cluster[i]\n",
    "        mean_cluster_vector[cluster] = mean_cluster_vector[cluster]+1\n",
    "    mean_cluster_vector = mean_cluster_vector/mean_cluster_vector.sum(axis=0,keepdims=1)\n",
    "    group_to_cluster_vector[group_id] = mean_cluster_vector\n",
    "    \n",
    "ac_df = pd.DataFrame(group_to_cluster_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "if OBJECTIVE == \"phenotypes\":\n",
    "    filename = \"../data/group_related_files/lloyd/lloyd_function_hierarchy_irb_cleaned.csv\"\n",
    "    lmtm_df = pd.read_csv(filename)    \n",
    "    columns_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "    tm_df = tm_df[columns_in_order]\n",
    "\n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "ac_df[\"idxmax\"] = ac_df.idxmax(axis = 1)\n",
    "ac_df[\"idxmax\"] = ac_df[\"idxmax\"].apply(lambda x: ac_df.columns.get_loc(x))\n",
    "ac_df = ac_df.sort_values(by=\"idxmax\")\n",
    "ac_df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "ac_df = ac_df.reset_index(drop=False).rename({\"index\":\"cluster\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "ac_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_agglomerative_clustering.csv\"), index=False)\n",
    "ac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phenologs\"></a>\n",
    "### Approach 3: Looking for phenolog relationships between clusters and OMIM disease phenotypes\n",
    "This section produces a table of values that provides a score for the a particular pair of a cluster found for this dataset of plant genes and a disease phenotype. Currently the value indicates the fraction of the plant genes in that cluster that have orthologs associated with that disease phenotype. This should be replaced or supplemented with a p-value for evaluating the significance of this value given the distribution of genes and their mappings to all of the disease phenotypes. All the rows from the input dataframe containing the PantherDB and OMIM information where the ID from this dataset is not known or the mapping to a phenotype was unsuccessful are removed at this step, fix this if the metric for evaluating cluster to phenotype phenolog mappings need this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the dataframe mapping plant genes --> human orthologs --> disease phenotypes.\n",
    "omim_df = pd.read_csv(panther_to_omim_filename)\n",
    "# Add a column that indicates which ID in the dataset those plant genes refer to, for mapping to phenotypes.\n",
    "name_to_id = dataset.get_name_to_id_dictionary()\n",
    "omim_df[\"id\"] = omim_df[\"gene_identifier\"].map(lambda x: name_to_id.get(x,None))\n",
    "omim_df = omim_df.dropna(subset=[\"id\",\"phenotype_mim_name\"], inplace=False)\n",
    "omim_df[\"phenotype_mim_name\"] = omim_df[\"phenotype_mim_name\"].astype(str)\n",
    "omim_df[\"compressed_phenotype_mim_name\"] = omim_df[\"phenotype_mim_name\"].map(lambda x: x.split(\",\")[0])\n",
    "omim_df[\"id\"] = omim_df[\"id\"].astype(\"int64\")\n",
    "omim_df[\"phenotype_mim_number\"] = omim_df[\"phenotype_mim_number\"].astype(\"int64\")\n",
    "# Generate mappings between the IDs in this dataset and disease phenotypes or orthologous genes.\n",
    "id_to_mim_phenotype_names = defaultdict(list)\n",
    "for i,p in zip(omim_df[\"id\"].values,omim_df[\"compressed_phenotype_mim_name\"].values):\n",
    "    id_to_mim_phenotype_names[i].append(p)\n",
    "id_to_human_gene_symbols = defaultdict(list)\n",
    "for i,s in zip(omim_df[\"id\"].values,omim_df[\"human_ortholog_gene_symbol\"].values):\n",
    "    id_to_human_gene_symbols[i].append(s)\n",
    "omim_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many genes in our dataset map to orthologs that map to the same OMIM phenotype?\n",
    "omim_df.groupby(\"compressed_phenotype_mim_name\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phenolog_x_dict = defaultdict(dict)\n",
    "phenolog_p_dict = defaultdict(dict)\n",
    "candidate_genes_dict = defaultdict(dict)\n",
    "phenotypes = pd.unique(omim_df[\"compressed_phenotype_mim_name\"].values)\n",
    "clusters = list(cluster_to_ids.keys())\n",
    "for cluster,phenotype in itertools.product(clusters,phenotypes):\n",
    "    \n",
    "    # What are the candidate genes predicted if this phenolog pairing is real?\n",
    "    ids = cluster_to_ids[cluster]\n",
    "    candidate_genes_dict[cluster][phenotype] = list(set(flatten([id_to_human_gene_symbols[i] for i in ids if phenotype not in id_to_mim_phenotype_names.get(i,[])])))\n",
    "\n",
    "    # What is the p-value for this phenolog pairing?\n",
    "    # The size of the population (genes in the dataset).\n",
    "    M = len(id_to_cluster.keys())\n",
    "    # The number of elements we draw without replacement (genes in the cluster).\n",
    "    N = len(cluster_to_ids[cluster])     \n",
    "    # The number of available successes in the population (genes that map to orthologs that map to this phenotype).\n",
    "    n = len([i for i in id_to_cluster.keys() if phenotype in id_to_mim_phenotype_names.get(i,[])])\n",
    "    # The number of successes drawn (genes in this cluster that map to orthologs that map to this phenotype).\n",
    "    x = list(set(flatten([id_to_mim_phenotype_names.get(i,[]) for i in ids]))).count(phenotype)\n",
    "    prob = 1-hypergeom.cdf(x-1, M, n, N) # Equivalent to prob = 1-sum([hypergeom.pmf(x_i, M, n, N) for x_i in range(0,x)])\n",
    "    phenolog_x_dict[cluster][phenotype] = x\n",
    "    phenolog_p_dict[cluster][phenotype] = prob\n",
    "    \n",
    "\n",
    "# Convert the dictionary to a table of values with cluster and phenotype as the rows and columns.\n",
    "phenolog_matrix = pd.DataFrame(phenolog_x_dict)        \n",
    "phenolog_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Produce a melted version of the phenolog matrix sorted by value and including predicted candidate genes.\n",
    "phenolog_matrix_reset = phenolog_matrix.reset_index(drop=False).rename({\"index\":\"omim_phenotype_name\"}, axis=\"columns\")\n",
    "phenolog_df = pd.melt(phenolog_matrix_reset, id_vars=[\"omim_phenotype_name\"], value_vars=phenolog_matrix.columns[1:], var_name=\"cluster\", value_name=\"x\")\n",
    "# What other information should be present in this melted phenologs matrix?\n",
    "phenolog_df[\"size\"] = phenolog_df[\"cluster\"].map(lambda x: len(cluster_to_ids[x]))\n",
    "phenolog_df[\"candidate_gene_symbols\"] = np.vectorize(lambda x,y: concatenate_with_bar_delim(*candidate_genes_dict[x][y]))(phenolog_df[\"cluster\"], phenolog_df[\"omim_phenotype_name\"])\n",
    "phenolog_df[\"p_value\"] = np.vectorize(lambda x,y: phenolog_p_dict[x][y])(phenolog_df[\"cluster\"], phenolog_df[\"omim_phenotype_name\"])\n",
    "phenolog_df[\"p_adjusted\"] = multipletests(phenolog_df[\"p_value\"].values, method='bonferroni')[1]\n",
    "phenolog_df.sort_values(by=[\"p_value\"], inplace=True, ascending=True)\n",
    "phenolog_df = phenolog_df[[\"omim_phenotype_name\", \"cluster\", \"size\", \"x\", \"p_value\", \"p_adjusted\", \"candidate_gene_symbols\"]]\n",
    "phenolog_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_phenologs.csv\"), index=False)\n",
    "phenolog_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: Agglomerative clustering and sillhouette scores for each NLP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "# Note that homogeneity scores don't fit for evaluating how close the clustering is to pathway membership, etc.\n",
    "# This is because genes can be assigned to more than one pathway, metric would have to be changed to account for this.\n",
    "# So all this section does is determines which values of n_clusters provide good clustering results for each matrix.\n",
    "n_clusters_silhouette_scores = defaultdict(dict)\n",
    "min_n_clusters = 20\n",
    "max_n_clusters = 80\n",
    "step_size = 4\n",
    "number_of_clusters = np.arange(min_n_clusters, max_n_clusters, step_size)\n",
    "for n in number_of_clusters:\n",
    "    for name in names:\n",
    "        distance_matrix = name_to_array[name]\n",
    "        #to_id = array_index_to_id\n",
    "        ac = AgglomerativeClustering(n_clusters=n, linkage=\"complete\", affinity=\"precomputed\")\n",
    "        clustering = ac.fit(distance_matrix)\n",
    "        sil_score = silhouette_score(distance_matrix, clustering.labels_, metric=\"precomputed\")\n",
    "        n_clusters_silhouette_scores[name][n] = sil_score\n",
    "sil_df = pd.DataFrame(n_clusters_silhouette_scores).reset_index(drop=False).rename({\"index\":\"n\"},axis=\"columns\")\n",
    "sil_df.to_csv(os.path.join(OUTPUT_DIR,\"part_5_silhouette_scores_by_n.csv\"), index=False)\n",
    "sil_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
