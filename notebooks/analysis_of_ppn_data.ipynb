{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text mining analysis and network analysis of the Oellrich, Walls et al., (2015) dataset\n",
    "The purpose of this notebook is to evaluate the phenotype similarity network based off of hand-curated EQ statement annotations with GO, PO, PATO, and ChEBI terms with regards to how well this network clusters phenotypes related to genes which have known protein-protein interactions, have biochemical pathways in common, or are involved in common phenotype groupings. These properties of this network are compared against these properties in networks constructed from the same data but using natural language processing and machine learning approaches to evaluating text similarity, as well as combinations of these approaches using higher level models. Output from this notebook is saved with naming conventions according to when the notebook was run, and all outputs are saved as images or tables in CSV format.\n",
    "\n",
    "The purpose of this notebook is to answer the question of how networks genereated using phenotypic-text similarity based approaches through either embedding, vocabulary presence, or ontology annotation compare to or relate to networks that specify known protein-protein interactions. The hypothesis that these networks are potentially related is based on the idea that if two proteins interact, they are likely to be acting in a common pathway with a common biological function. If the phenotypic outcome of this pathway is observable and documented, then similarites between text describing the mutant phenotype for these genes may coincide with direct protein-protein interactions. The different sections in this notebook correspond to different ways of determining if the graphs based on similarity between text descriptions, encodings of text descriptions, or annotations derived from text descriptions at all correspond to known protein-protein interactions in this dataset. The knowledge source about the protein-protein interactions for genes in this dataset is the STRING database. The available entries in the whole dataset are subset to include only the genes that correspond to proteins that are atleast mentioned in the STRING database. This ways if a protein-protein interaction is not specified between two of the remaining genes, it is not because no interactions at all are documented either of those genes. The following cells focus on setting up a dataframe which specifies edge lists specific to each similarity method, and also a protein-protein interaction score for the genes which correspond to those two given nodes in the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/irbraun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/irbraun/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.datasets.known import Known\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.graphs import pairwise as pw\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.graphs.models import train_logistic_regression_model, apply_logistic_regression_model\n",
    "from oats.graphs.models import train_random_forest_model, apply_random_forest_model\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, build_vocabulary_from_tokens\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining the output and input file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summarizing output dictionary has the shape TABLE[method][(tag,metric)] --> value.\n",
    "TAG = \"Biochemical Pathways from KEGG\"\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = \"../data/pickles/ppn_dataset.pickle\"                          # The full dataset pickle.\n",
    "groupings_filename = \"../data/pickles/lloyd_subsets.pickle\"                      # The groupings pickle.\n",
    "background_corpus_filename = \"../data/corpus_related_files/background.txt\"       # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/phenotypes_large.txt\" # Text file with specific content.\n",
    "doc2vec_pubmed_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                    # File holding saved Doc2Vec model.\n",
    "doc2vec_wikipedia_filename = \"../gensim/pubmed_dbow/doc2vec.bin\"                 # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                       # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                       # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                            # Jar for NOBLE Coder tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the dataset of genes, phenotypes, and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 2683\n",
      "Number of unique IDs:            2683\n",
      "Number of unique descriptions:   1794\n",
      "Number of unique gene name sets: 2683\n",
      "Number of species represented:   6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>description</th>\n",
       "      <th>term_ids</th>\n",
       "      <th>pmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ath</td>\n",
       "      <td>At5g37020|ARF8</td>\n",
       "      <td>Long hypocotyl. Reduced fertility. Short hypoc...</td>\n",
       "      <td>PO:0020100|PATO:0000573|PO:0000003|PATO:000183...</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ath</td>\n",
       "      <td>At3g25100|CDC45</td>\n",
       "      <td>Complete sterility. Defects in meiosis. Partia...</td>\n",
       "      <td>PO:0009046|PATO:0000274|PATO:0000462|GO:000712...</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g76420|CUC3</td>\n",
       "      <td>Partially fused cotyledons. Partially fused co...</td>\n",
       "      <td>PO:0020030|PATO:0000642</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g20110|PDE330</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g71720|PDE338</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g76620|PDE339</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ath</td>\n",
       "      <td>At2g01140|PDE345</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ath</td>\n",
       "      <td>At3g55250|PDE329</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ath</td>\n",
       "      <td>At4g30720|PDE327</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ath</td>\n",
       "      <td>At4g34830|PDE346</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>ath</td>\n",
       "      <td>At5g02120|PDE335</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>ath</td>\n",
       "      <td>At5g08610|PDE340</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>ath</td>\n",
       "      <td>At5g09790|PDE336</td>\n",
       "      <td>Pigment defective embryo. Pigment defective em...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g79850|ORE4</td>\n",
       "      <td>Delayed leaf senescence. Pigment defective emb...</td>\n",
       "      <td>GO:0010150|PATO:0000502|PO:0009009|PATO:000224...</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>ath</td>\n",
       "      <td>At2g30950|VAR2</td>\n",
       "      <td>Pigment defective embryo. Variegated leaves. P...</td>\n",
       "      <td>PO:0009009|PATO:0002247|PATO:0000460|PO:000902...</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g01460|PIPK11</td>\n",
       "      <td>Pollen tube growth sensitive to latrunculin B ...</td>\n",
       "      <td>PO:0025195|PATO:0001549|CHEBI:49703|EO:0007189</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>ath</td>\n",
       "      <td>At1g44446|CH1</td>\n",
       "      <td>Pale. Yellow-green. Pale. Yellow-green.</td>\n",
       "      <td>PO:0000003|PATO:0002251|PATO:0001941</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>ath</td>\n",
       "      <td>At4g01190|PIPK10</td>\n",
       "      <td>Pollen tube growth sensitive to latrunculin B ...</td>\n",
       "      <td>PO:0025195|PATO:0001549|CHEBI:49703|EO:0007189</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>ath</td>\n",
       "      <td>At2g02950|PKS1</td>\n",
       "      <td>Pale under hourly far red pulses. Pale under h...</td>\n",
       "      <td>PO:0000003|PATO:0002251|EO:0007203</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>ath</td>\n",
       "      <td>At5g17220|TT19</td>\n",
       "      <td>Pale seed coat. Pale seed coat.</td>\n",
       "      <td>PO:0009088|PATO:0002251</td>\n",
       "      <td>0         1         2         3         4     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id species        gene_names                                        description                                           term_ids                                               pmid\n",
       "0    0     ath    At5g37020|ARF8  Long hypocotyl. Reduced fertility. Short hypoc...  PO:0020100|PATO:0000573|PO:0000003|PATO:000183...  0         1         2         3         4     ...\n",
       "1    1     ath   At3g25100|CDC45  Complete sterility. Defects in meiosis. Partia...  PO:0009046|PATO:0000274|PATO:0000462|GO:000712...  0         1         2         3         4     ...\n",
       "2    2     ath    At1g76420|CUC3  Partially fused cotyledons. Partially fused co...                            PO:0020030|PATO:0000642  0         1         2         3         4     ...\n",
       "3    3     ath  At1g20110|PDE330  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "4    4     ath  At1g71720|PDE338  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "5    5     ath  At1g76620|PDE339  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "6    6     ath  At2g01140|PDE345  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "7    7     ath  At3g55250|PDE329  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "8    8     ath  At4g30720|PDE327  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "9    9     ath  At4g34830|PDE346  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "10  10     ath  At5g02120|PDE335  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "11  11     ath  At5g08610|PDE340  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "12  12     ath  At5g09790|PDE336  Pigment defective embryo. Pigment defective em...               PO:0009009|PATO:0002247|PATO:0000460  0         1         2         3         4     ...\n",
       "13  13     ath    At1g79850|ORE4  Delayed leaf senescence. Pigment defective emb...  GO:0010150|PATO:0000502|PO:0009009|PATO:000224...  0         1         2         3         4     ...\n",
       "14  14     ath    At2g30950|VAR2  Pigment defective embryo. Variegated leaves. P...  PO:0009009|PATO:0002247|PATO:0000460|PO:000902...  0         1         2         3         4     ...\n",
       "15  15     ath  At1g01460|PIPK11  Pollen tube growth sensitive to latrunculin B ...     PO:0025195|PATO:0001549|CHEBI:49703|EO:0007189  0         1         2         3         4     ...\n",
       "16  16     ath     At1g44446|CH1            Pale. Yellow-green. Pale. Yellow-green.               PO:0000003|PATO:0002251|PATO:0001941  0         1         2         3         4     ...\n",
       "17  17     ath  At4g01190|PIPK10  Pollen tube growth sensitive to latrunculin B ...     PO:0025195|PATO:0001549|CHEBI:49703|EO:0007189  0         1         2         3         4     ...\n",
       "18  18     ath    At2g02950|PKS1  Pale under hourly far red pulses. Pale under h...                 PO:0000003|PATO:0002251|EO:0007203  0         1         2         3         4     ...\n",
       "19  19     ath    At5g17220|TT19                    Pale seed coat. Pale seed coat.                            PO:0009088|PATO:0002251  0         1         2         3         4     ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_pickle(dataset_filename)\n",
    "dataset.describe()\n",
    "dataset.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in pathway or grouping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups present for each species\n",
      "  ath: 42\n",
      "Number of genes names mapped to any group for each species\n",
      "  ath: 7620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_id</th>\n",
       "      <th>gene_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>FSM</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB|FSM|OVP|SRF</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>CDR|LIT</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|WAT</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP|SRF</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>CHS</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>NLS|GRS|IST</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>LEF|FSM</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species                         group_id                                         gene_names\n",
       "0     ath                              FSM                         at1g01030|nga3|top1|ngatha\n",
       "1     ath                  EMB|FSM|OVP|SRF    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2     ath                          CDR|LIT             at1g01060|lhy|late elongated hypocotyl\n",
       "3     ath                          IST|WAT   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4     ath                          OVP|SRF                 at1g01280|cyp703a2|cytochrome p450\n",
       "5     ath                              EMB        at1g01370|cenh3|centromere-specific histone\n",
       "6     ath                              CHS  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7     ath                      NLS|GRS|IST  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8     ath                          LEF|FSM                          at1g01510|an|angustifolia\n",
       "9     ath  SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP                              at1g01550|bps1|bypass"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = load_from_pickle(groupings_filename)\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "groups.describe()\n",
    "groups.to_csv(os.path.join(OUTPUT_DIR,\"groupings.csv\"))\n",
    "groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relating the dataset of genes to the pathway or grouping data\n",
    "This section generates tables that indicate how the genes present in the dataset were mapped to the defined pathways or groups. This includes a summary table that indicates how many genes by species were succcessfully mapped to atleast one pathway or group, as well as a more detailed table describing how many genes from each species were mapped to each particular pathway or group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "summary = defaultdict(dict)\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "for species in dataset.get_species():\n",
    "    summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "    summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table.loc[\"total\"]= table.sum()\n",
    "table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"species\"}, axis=\"columns\")\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_summary.csv\"), index=False)\n",
    "\n",
    "# Generate a table describing how many genes from each species map to which particular group.\n",
    "summary = defaultdict(dict)\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "    summary[group_id][\"total\"] = len([x for x in ids])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table = table.sort_values(by=\"total\", ascending=False)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "table.loc[\"total\"] = table.sum()\n",
    "table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_by_group.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the protein-protein interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce size of the dataset by removing genes not mentioned in the STRING.\n",
    "naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis thaliana\n",
    "    \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\", # maize\n",
    "    \"../data/group_related_files/string/4530.protein.links.detailed.v11.0.txt\", # tomato \n",
    "    \"../data/group_related_files/string/4081.protein.links.detailed.v11.0.txt\", # medicago\n",
    "    \"../data/group_related_files/string/3880.protein.links.detailed.v11.0.txt\", # rice \n",
    "    \"../data/group_related_files/string/3847.protein.links.detailed.v11.0.txt\", # soybean\n",
    "]\n",
    "#genes = dataset.get_gene_dictionary()\n",
    "#string_data = String(genes, naming_file, *interaction_files)\n",
    "\n",
    "# Filter the dataset based on whether or not the genes were successfully mapped to an interaction.\n",
    "#dataset.filter_with_ids(string_data.ids)\n",
    "#dataset.describe()\n",
    "\n",
    "# Merging information from the protein-protein interaction database with this dataset.\n",
    "#df = df.merge(right=string_data.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "#df.fillna(value=0,inplace=True)\n",
    "#df[\"interaction\"] = (df[\"combined_score\"] != 0.00)*1\n",
    "#df.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mapped to atleast one pathway in whatever the source of pathway membership we are using is (KEGG, Plant Metabolic Network, etc). This is because for these genes, it will be impossible to correctly predict their pathway membership, and we have no evidence that they belong or do not belong in certain pathways so they can not be identified as being true or false negatives in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff that is specific to using STRING.\n",
    "# Reduce size of the dataset by removing genes not mentioned in the STRING.\n",
    "#naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "#interactions_file_1 = \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\"\n",
    "#interactions_file_2 = \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\"\n",
    "#genes = dataset.get_gene_dictionary()\n",
    "#string_data = String(genes, naming_file, interactions_file_1, interactions_file_2)\n",
    "#group_mapped_ids = string_data.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 2000\n",
      "Number of unique IDs:            2000\n",
      "Number of unique descriptions:   1315\n",
      "Number of unique gene name sets: 2000\n",
      "Number of species represented:   1\n"
     ]
    }
   ],
   "source": [
    "# Filter based on succcessful mappings to groups or pathways.\n",
    "dataset.filter_with_ids(group_mapped_ids)\n",
    "dataset.filter_random_k(2000)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the mappings in each direction again now that the dataset has been subset.\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading NLP models and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and models related to the machine learning text embedding methods.\n",
    "doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_filename)\n",
    "doc2vec_pubmed_model = gensim.models.Doc2Vec.load(doc2vec_pubmed_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Constructing a vocabulary by looking at what words are overrepresented in domain specific text.\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "tokens = get_overrepresented_tokens(phenotypes_corpus, background_corpus, max_features=5000)\n",
    "vocabulary_from_text = build_vocabulary_from_tokens(tokens)\n",
    "\n",
    "# Constructing a vocabulary by assuming all words present in a given ontology are important.\n",
    "ontology = Ontology(ontology_filename)\n",
    "vocabulary_from_ontology = build_vocabulary_from_tokens(ontology.get_tokens())\n",
    "\n",
    "# Loading other necessary objects or models and annotating text with ontology terms.\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "annotations_from_dataset = dataset.get_annotations_dictionary()\n",
    "annotations_noblecoder_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=1)\n",
    "annotations_noblecoder_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=0)\n",
    "\n",
    "# Generating random descriptions that are drawn from same token population and retain original lengths.\n",
    "#tokens = [w for w in itertools.chain.from_iterable(word_tokenize(desc) for desc in descriptions.values())]\n",
    "#scrambled_descriptions = {k:\" \".join(np.random.choice(tokens,len(word_tokenize(v)))) for k,v in descriptions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text descriptions\n",
    "Normalizing case, lemmatization, stemming, removing stopwords, removing punctuation, handling numerics, creating parse trees, part-of-speech tagging, and anything else necessary for a particular dataset of descripitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vector representations and pairwise distances matrices\n",
    "This section uses the text descriptions, preprocessed text descriptions, or ontology term annotations created or read in the previous sections to generate a vector representation for each gene and build a pairwise distance matrix for the whole dataset. Each method specified is a unique combination of a method of vectorization (bag-of-words, n-grams, document embedding model, etc) and distance metric (Euclidean, Jaccard, etc) applied to those vectors in constructing the pairwise matrix. The method of vectorization here is equivalent to feature selection, so the task is to figure out which type of vectors will encode features that are useful (n-grams, full words, only words from a certain vocabulary, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of tuples, each tuple will be used to build to find a matrix of pairwise distances.\n",
    "# The required items in each tuple are:\n",
    "# Index 0: name of the method\n",
    "# Index 1: function to call for running this method\n",
    "# Index 2: arguments to pass to that function as dictionary of keyword args\n",
    "# Index 3: distance metric to apply to vectors generated with that method\n",
    "name_function_args_tuples = [\n",
    "    # Methods that use neural networks to generate embeddings.\n",
    "    (\"doc2vec_wiki\", pw.pairwise_doc2vec_onegroup, {\"model\":doc2vec_wiki_model, \"object_dict\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    (\"doc2vec_pubmed\", pw.pairwise_doc2vec_onegroup, {\"model\":doc2vec_pubmed_model, \"object_dict\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine),\n",
    "    #(\"bert_base\", pw.pairwise_bert_onegroup, {\"model\":bert_model, \"tokenizer\":bert_tokenizer, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine),\n",
    "    #(\"word2vec_avg\", pw.pairwise_word2vec_onegroup, {\"model\":word2vec_model, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine),\n",
    "    #(\"word2vec_max\", pw.pairwise_word2vec_onegroup, {\"model\":word2vec_model, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine),\n",
    "    # Methods that use variations on the ngram approach.\n",
    "    #(\"ngrams_w12\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":5000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w12b\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":5000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w1\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":5000, \"tfidf\":False}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w1b\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":5000, \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #(\"ngrams_w12_t\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2),\"max_features\":5000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w12b_t\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":5000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w1_t\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":5000, \"tfidf\":True}, spatial.distance.cosine),\n",
    "    #(\"ngrams_w1b_t\", pw.pairwise_ngrams_onegroup, {\"object_dict\":descriptions, \"metric\":\"jaccard\", \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":5000, \"tfidf\":True}, spatial.distance.jaccard),\n",
    "    # Methods that use terms inferred from automated annotation of the text.\n",
    "    #(\"nc_precise\", pw.pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #(\"nc_partial\", pw.pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":False}, spatial.distance.jaccard),\n",
    "    #(\"nc_precise_t\", pw.pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":True}, spatial.distance.jaccard),\n",
    "    #(\"nc_partial_t\", pw.pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\", \"tfidf\":True}, spatial.distance.jaccard),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all of the similarity matrices in parallel.\n",
    "#start_time_mp = time.perf_counter()\n",
    "#pool = mp.Pool(mp.cpu_count())\n",
    "#results = [pool.apply_async(function_wrapper_with_duration, args=(func, args)) for (name,func,args,metric) in name_function_args_tuples]\n",
    "#results = [result.get() for result in results]\n",
    "#graphs = {tup[0]:result[0] for tup,result in zip(name_function_args_tuples,results)}\n",
    "#metric_dict = {tup[0]:tup[3] for tup in name_function_args_tuples}\n",
    "#durations = {tup[0]:result[1] for tup,result in zip(name_function_args_tuples,results)}\n",
    "#pool.close()\n",
    "#pool.join()    \n",
    "#total_time_mp = time.perf_counter()-start_time_mp\n",
    "\n",
    "# Reporting how long each matrix took to build and how much time parallel processing saved.\n",
    "#print(\"Durations of generating each pairwise similarity matrix (hh:mm:ss)\")\n",
    "#print(\"-----------------------------------------------------------------\")\n",
    "#savings = total_time_mp/sum(durations.values())\n",
    "#for (name,duration) in durations.items():\n",
    "#    print(\"{:15} {}\".format(name, to_hms(duration)))\n",
    "#print(\"-----------------------------------------------------------------\")\n",
    "#print(\"{:15} {}\".format(\"total\", to_hms(sum(durations.values()))))\n",
    "#print(\"{:15} {} ({:.2%} of single thread time)\".format(\"multiprocess\", to_hms(total_time_mp), savings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the pairwise distance matrices.\n",
    "graphs = {tup[0]:tup[1](**tup[2]) for tup in name_function_args_tuples}\n",
    "metric_dict = {tup[0]:tup[3] for tup in name_function_args_tuples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>doc2vec_wiki</th>\n",
       "      <th>doc2vec_pubmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000986</th>\n",
       "      <td>1959</td>\n",
       "      <td>291</td>\n",
       "      <td>0.166998</td>\n",
       "      <td>0.412131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000987</th>\n",
       "      <td>1959</td>\n",
       "      <td>1779</td>\n",
       "      <td>0.177247</td>\n",
       "      <td>0.420312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000988</th>\n",
       "      <td>1959</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.194602</td>\n",
       "      <td>0.054726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000989</th>\n",
       "      <td>1959</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.140026</td>\n",
       "      <td>0.187836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000991</th>\n",
       "      <td>291</td>\n",
       "      <td>1779</td>\n",
       "      <td>0.253233</td>\n",
       "      <td>0.396681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000992</th>\n",
       "      <td>291</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.212883</td>\n",
       "      <td>0.423898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000993</th>\n",
       "      <td>291</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.478551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000995</th>\n",
       "      <td>1779</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.285101</td>\n",
       "      <td>0.446728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000996</th>\n",
       "      <td>1779</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.239120</td>\n",
       "      <td>0.428067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000998</th>\n",
       "      <td>2340</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>0.212068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         from    to  doc2vec_wiki  doc2vec_pubmed\n",
       "2000986  1959   291      0.166998        0.412131\n",
       "2000987  1959  1779      0.177247        0.420312\n",
       "2000988  1959  2340      0.194602        0.054726\n",
       "2000989  1959  1915      0.140026        0.187836\n",
       "2000991   291  1779      0.253233        0.396681\n",
       "2000992   291  2340      0.212883        0.423898\n",
       "2000993   291  1915      0.206655        0.478551\n",
       "2000995  1779  2340      0.285101        0.446728\n",
       "2000996  1779  1915      0.239120        0.428067\n",
       "2000998  2340  1915      0.195073        0.212068"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging all of the edgelist dataframes together.\n",
    "methods = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "df = pw.merge_edgelists(edgelists, default_value=0.000)\n",
    "df = pw.remove_self_loops(df)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging in the previously curated similarity values from the publication\n",
    "This section reads in a file that contains the previously calculated distance values from the Oellrich, Walls et al. (2015) dataset, and merges it with the values which are obtained here for all of the applicable natural language processing or machine learning methods used, so that the graphs which are specified by these sets of distances values can be evaluated side by side in the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>doc2vec_wiki</th>\n",
       "      <th>doc2vec_pubmed</th>\n",
       "      <th>pppn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999210</th>\n",
       "      <td>1959</td>\n",
       "      <td>291</td>\n",
       "      <td>0.166998</td>\n",
       "      <td>0.412131</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999211</th>\n",
       "      <td>1959</td>\n",
       "      <td>1779</td>\n",
       "      <td>0.177247</td>\n",
       "      <td>0.420312</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999212</th>\n",
       "      <td>1959</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.194602</td>\n",
       "      <td>0.054726</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999213</th>\n",
       "      <td>1959</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.140026</td>\n",
       "      <td>0.187836</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999214</th>\n",
       "      <td>291</td>\n",
       "      <td>1779</td>\n",
       "      <td>0.253233</td>\n",
       "      <td>0.396681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999215</th>\n",
       "      <td>291</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.212883</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999216</th>\n",
       "      <td>291</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.478551</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999217</th>\n",
       "      <td>1779</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.285101</td>\n",
       "      <td>0.446728</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999218</th>\n",
       "      <td>1779</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.239120</td>\n",
       "      <td>0.428067</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999219</th>\n",
       "      <td>2340</td>\n",
       "      <td>1915</td>\n",
       "      <td>0.195073</td>\n",
       "      <td>0.212068</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         from    to  doc2vec_wiki  doc2vec_pubmed      pppn\n",
       "1999210  1959   291      0.166998        0.412131  1.000000\n",
       "1999211  1959  1779      0.177247        0.420312  1.000000\n",
       "1999212  1959  2340      0.194602        0.054726  0.000000\n",
       "1999213  1959  1915      0.140026        0.187836  0.333333\n",
       "1999214   291  1779      0.253233        0.396681  1.000000\n",
       "1999215   291  2340      0.212883        0.423898  1.000000\n",
       "1999216   291  1915      0.206655        0.478551  1.000000\n",
       "1999217  1779  2340      0.285101        0.446728  1.000000\n",
       "1999218  1779  1915      0.239120        0.428067  1.000000\n",
       "1999219  2340  1915      0.195073        0.212068  0.333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_edgelist_path = \"../data/supplemental_files_oellrich_walls/13007_2015_53_MOESM9_ESM.txt\"\n",
    "known = Known(dataset.get_name_to_id_dictionary(), known_edgelist_path)\n",
    "df = df.merge(right=known.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df.fillna(value=0,inplace=True)\n",
    "df[\"value\"] = 1-df[\"value\"]\n",
    "df.rename(columns={\"value\":\"pppn\"}, inplace=True)\n",
    "methods.append(\"pppn\")\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding information specific to each edge\n",
    "The relevant information for each edge includes questions like whether or not the two genes that edge connects share a group or biochemical pathway in common, or if those genes are from the same species. This information can then later be used as the target values for predictive models, or for filtering the graphs represented by these edge lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a4b3cd31a88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate a column indicating whether or not the two genes have atleast one pathway in common.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shared\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_group_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_group_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shared\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generate a column indicating whether or not the two genes are from the same species.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6926\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6927\u001b[0m         )\n\u001b[0;32m-> 6928\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 result = reduction.reduce(\n\u001b[0;32m--> 285\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                 )\n\u001b[1;32m    287\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels, fastpath)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_subtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_all_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate a column indicating whether or not the two genes have atleast one pathway in common.\n",
    "df[\"shared\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "print(Counter(df[\"shared\"].values))\n",
    "\n",
    "# Generate a column indicating whether or not the two genes are from the same species.\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)*1\n",
    "print(Counter(df[\"same\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple distance methods with machine learning models\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating indicating how likely is it that two genes share atleast one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "method = \"lr\"\n",
    "splits = 12\n",
    "kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "df[method] = pd.Series()\n",
    "for train,test in kf.split(df):\n",
    "    lr_model = train_logistic_regression_model(df=df.iloc[train], predictor_columns=methods, target_column=\"shared\")\n",
    "    df[method].iloc[test] = apply_logistic_regression_model(df=df.iloc[test], predictor_columns=methods, model=lr_model)\n",
    "df[method] = 1-df[method]\n",
    "methods.append(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "#method = \"rf\"\n",
    "#splits = 2\n",
    "#kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "#df[method] = pd.Series()\n",
    "#for train,test in kf.split(df):\n",
    "#    rf_model = train_random_forest_model(df=df.iloc[train], predictor_columns=methods, target_column=\"shared\")\n",
    "#    df[method].iloc[test] = apply_random_forest_model(df=df.iloc[test],predictor_columns=methods, model=rf_model)\n",
    "#df[method] = 1-df[method]\n",
    "#methods.append(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the edges joining genes that share atleast one pathway come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation betwene the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "ppi_pos_dict = {name:(df[df[\"shared\"] > 0.00][name].values) for name in methods}\n",
    "ppi_neg_dict = {name:(df[df[\"shared\"] == 0.00][name].values) for name in methods}\n",
    "for name in methods:\n",
    "    stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "    pos_mean = np.average(ppi_pos_dict[name])\n",
    "    neg_mean = np.average(ppi_neg_dict[name])\n",
    "    pos_n = len(ppi_pos_dict[name])\n",
    "    neg_n = len(ppi_neg_dict[name])\n",
    "    TABLE[name].update({(TAG,\"mean_1\"):pos_mean, (TAG,\"mean_0\"):neg_mean, (TAG,\"n_1\"):pos_n, (TAG,\"n_0\"):neg_n})\n",
    "    TABLE[name].update({(TAG,\"ks\"):stat, (TAG,\"ks_pval\"):p})\n",
    "\n",
    "# Show the kernel estimates for each distribution of weights for each method.\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for name,ax in zip(methods,axs.flatten()):\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"kernel_density.png\"),dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at within-group or within-pathway distances in each graph\n",
    "The purpose of this section is to determine which methods generated graphs which tightly group genes which share common pathways or group membership with one another. In order to compare across different methods where the distance value distributions are different, the mean distance values for each group for each method are convereted to percentile scores. Lower percentile scores indicate that the average distance value between any two genes that belong to that group is lower than most of the distance values in the entire distribution for that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the average within-pathway phenotype distance values for each method for each particular pathway.\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "within_percentiles_dict = defaultdict(lambda: defaultdict(list))\n",
    "all_weights_dict = {}\n",
    "for method in methods:\n",
    "    all_weights_dict[method] = df[method].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        mean_weight = np.mean((graph.get_values(within_pairs, kind=method)))\n",
    "        within_weights_dict[method][group] = mean_weight\n",
    "        within_percentiles_dict[method][group] = stats.percentileofscore(df[method].values, mean_weight, kind=\"rank\")\n",
    "\n",
    "# Generating a dataframe of percentiles of the mean in-group distance scores.\n",
    "heatmap_data = pd.DataFrame(within_percentiles_dict)\n",
    "heatmap_data = heatmap_data.dropna(axis=0, inplace=False)\n",
    "heatmap_data = heatmap_data.round(4)\n",
    "\n",
    "# Adding relevant information to this dataframe and saving.\n",
    "heatmap_data[\"avg_rank\"] = heatmap_data.rank().mean(axis=1)\n",
    "heatmap_data.sort_values(by=\"avg_rank\", inplace=True)\n",
    "heatmap_data.reset_index(inplace=True)\n",
    "heatmap_data[\"group_id\"] = heatmap_data[\"index\"]\n",
    "heatmap_data[\"full_name\"] = heatmap_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "heatmap_data[\"n\"] = heatmap_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "heatmap_data = heatmap_data[flatten([\"group_id\",\"full_name\",\"n\",\"avg_rank\",methods])]\n",
    "heatmap_data.to_csv(os.path.join(OUTPUT_DIR,\"within_distances.csv\"), index=False)\n",
    "heatmap_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting whether two genes belong to a common biochemical pathway\n",
    "The purpose of this section is to see if whether or not two genes share atleast one common pathway can be predicted from the distance scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_dict = {name:df[\"shared\"] for name in methods}\n",
    "y_prob_dict = {name:(1 - df[name].values) for name in methods}\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true, y_prob = y_true_dict[method], y_prob_dict[method]\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"auc\"):area, (TAG,\"baseline\"):baseline, (TAG,\"ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_shared.png\"),dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are genes in the same biochemical pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "# graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "# i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "# in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "# connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "df = pw.make_undirected(df)\n",
    "\n",
    "# What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "# each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "# recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "k = 10      # The threshold of interest for gene ranks.\n",
    "n = 100     # Number of Monte Carlo simulation iterations to complete.\n",
    "df[list(methods)] = df.groupby(\"from\")[list(methods)].rank()\n",
    "ys = df[df[\"shared\"]==1][list(methods)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "ymax = sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "# Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "# edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "ysims = [sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "for method in methods:\n",
    "    pvalue = len([ysim for ysim in ysims if ysim>=ys[method]])/float(n)\n",
    "    TABLE[method][(TAG,\"y\")] = ys[method]\n",
    "    TABLE[method][(TAG,\"y_max\")] = ymax\n",
    "    TABLE[method][(TAG,\"y_ratio\")] = ys[method]/ymax\n",
    "    TABLE[method][(TAG,\"y_pval\")] = pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership based on mean vectors\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes asociated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "methods = list(vector_dicts.keys())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for method in methods:\n",
    "    for group in valid_group_ids:\n",
    "        ids = group_id_to_ids[group]\n",
    "        for identifier in valid_ids:\n",
    "            # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "            vectors = np.array([vector_dicts[method][some_id] for some_id in ids if not some_id==identifier])\n",
    "            mean_vector = vectors.mean(axis=0)\n",
    "            this_vector = vector_dicts[method][identifier]\n",
    "            pred_dict[method][identifier][group] = 1-metric_dict[method](mean_vector, this_vector)\n",
    "            true_dict[method][identifier][group] = (identifier in group_id_to_ids[group])*1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true = pd.DataFrame(true_dict[method]).as_matrix().flatten()\n",
    "    y_prob = pd.DataFrame(pred_dict[method]).as_matrix().flatten()\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"mean_auc\"):area, (TAG,\"mean_baseline\"):baseline, (TAG,\"mean_ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_mean_classifier.png\"),dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway membership based on mean similarity values\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representationt of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership with KNN classifier\n",
    "This section looks at how well the group(s) or biochemical pathway(s) that a particular gene belongs to can be predicted based on a KNN classifier generated using every other gene. For this section, only the groups or pathways which contain more than one gene, and the genes mapped to those groups or pathways, are of interest. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but the KNN classifier could never predict this because when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be memebers of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "\n",
    "# Leave one out predictions.\n",
    "pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for method in methods:\n",
    "    for i in valid_ids:\n",
    "        ids = [identifier for identifier in valid_ids if identifier!=i]\n",
    "        # Make the labels just the ID's for that gene instead of pathways or groups, transform votes later.\n",
    "        X = [vector_dicts[method][identifier] for identifier in ids]\n",
    "        y = [identifier for identifier in ids]\n",
    "        neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "        neigh.fit(X,y)\n",
    "        probs = neigh.predict_proba([vector_dicts[method][i]])\n",
    "        # The classes are given in lexicographic order from the predict_proba method.\n",
    "        # 1. Create a 2D binary array with a row for each group and a column for each gene.\n",
    "        # 2. Multiply each row of that 2D binary array with the 1D array that has the probability for each gene.\n",
    "        # 3. Find the sum of each row to get the score specific to each group for this particular gene.\n",
    "        # 4. Append to the arrays for the true binary classifications and the predictions for each group. \n",
    "        ids.sort()\n",
    "        binary_arr = np.array([[(group_id in id_to_group_ids[x])*1 for x in ids] for group_id in valid_group_ids]) \n",
    "        group_probs_by_gene = probs*binary_arr\n",
    "        group_probs = np.sum(group_probs_by_gene, axis=1)\n",
    "        for group_index,group_id in enumerate(valid_group_ids):\n",
    "            pred_dict[method][i][group_id] = group_probs[group_index]\n",
    "            true_dict[method][i][group_id] = (i in group_id_to_ids[group_id])*1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true = pd.DataFrame(true_dict[method]).as_matrix().flatten()\n",
    "    y_prob = pd.DataFrame(pred_dict[method]).as_matrix().flatten()\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"knn_auc\"):area, (TAG,\"knn_baseline\"):baseline, (TAG,\"knn_ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_knn_classifier.png\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the results for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"full_table.csv\"), index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can machine learning approaches learn relationships between concepts that are in ontologies?\n",
    "If the neural network document encoding models (Doc2Vec) are being successfully trained, then they should be able to recapture some of the domain-specific information that is written into relationships present in biological ontologies. Specifically, two concepts which have a parent-child relationship in PATO or PO can be considered to be highly similar in this context. We compare the distances between the labels for these pairs of terms as inferred by both the general Doc2Vec model trained on the English Wikipedia corpus, as well as our own models trained specifically on abstracts from PubMed that are specific to plant phenotypes. Here we generate figures to compare the results for a specific set of handpicked phrase or term pairs, as well as a second figure over all pairs parsed from the hierarchies in each ontology to check whether the result generalizes to the ontologies as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Convert the distance values in the dataframe to be percentiles.\n",
    "df_pct = df.copy()\n",
    "df_pct[methods] = df[methods].rank(pct=True)\n",
    "\n",
    "# Looking through the distances that are low for PubMed and high for Wikipedia to see if this is a valuable approach.\n",
    "of_interest = df_pct[(df_pct[\"doc2vec_pubmed\"]<0.1) & (df_pct[\"doc2vec_wiki\"]>0.3)]\n",
    "for row in of_interest.itertuples():\n",
    "    print(\"{} and {}\".format(round(row[4],4),round(row[3],4)))\n",
    "    sentence1 = descriptions[row[1]]\n",
    "    sentence2 = descriptions[row[2]]\n",
    "    print(\"1: {}\\n2: {}\\n\\n\".format(sentence1, sentence2))\n",
    "    \n",
    "# Looking at the distance values of sentence variations of interest found in the previous step. \n",
    "sentences = {\n",
    "    0:\"Susceptible to bacterial infection\",\n",
    "    1:\"Resistant to bacterial infection\",\n",
    "    2:\"Resistant to powdery mildew\",\n",
    "    3:\"susceptible to powdery mildew\",\n",
    "    4:\"Some random control sentence\"\n",
    "}\n",
    "wikipedia_results = pw.pairwise_doc2vec_onegroup(doc2vec_wiki_model,sentences,\"cosine\").edgelist\n",
    "wikipedia_results[\"value\"] = wikipedia_results[\"value\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_wiki\"].values, x, kind=\"rank\")/100)\n",
    "wikipedia_results = pw.remove_self_loops(wikipedia_results)\n",
    "pubmed_results = pw.pairwise_doc2vec_onegroup(doc2vec_pubmed_model,sentences,\"cosine\").edgelist\n",
    "pubmed_results[\"value\"] = pubmed_results[\"value\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_pubmed\"].values, x, kind=\"rank\")/100)\n",
    "pubmed_results = pw.remove_self_loops(pubmed_results)\n",
    "results = pw.merge_edgelists({\"wikipedia\":wikipedia_results,\"pubmed\":pubmed_results})\n",
    "results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Defining a function for getting the similarity between phrases, sentences, or documents.\n",
    "def dist(model, s1, s2):\n",
    "    inferred_vector1 = model.infer_vector(s1.lower().split())\n",
    "    inferred_vector2 = model.infer_vector(s2.lower().split())\n",
    "    distance = cosine(inferred_vector1, inferred_vector2)\n",
    "    return(distance)\n",
    "\n",
    "# Loading a file of handpicked phrase pairs from ontology term labels. \n",
    "pairs = pd.read_csv(\"../data/corpus_related_files/phrase_pairs.csv\")\n",
    "pairs[\"Wikipedia\"] = np.vectorize(dist)(doc2vec_wiki_model, pairs[\"Label 1\"], pairs[\"Label 2\"])\n",
    "pairs[\"PubMed\"] = np.vectorize(dist)(doc2vec_pubmed_model, pairs[\"Label 1\"], pairs[\"Label 2\"])\n",
    "pairs[\"Wikipedia\"] = pairs[\"Wikipedia\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_wiki\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"PubMed\"] = pairs[\"PubMed\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_pubmed\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"Pair\"] = pairs[\"Label 1\"].values+\",\"+pairs[\"Label 2\"].values\n",
    "pairs.to_csv(\"../data/scratch/phrase_pair_handpicked_results.csv\",index=False)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronto\n",
    "\n",
    "# Generating a dataframe that has all the parent-child relationships from the ontologies and their labels.\n",
    "tuples = []\n",
    "pato = pronto.Ontology(\"../ontologies/pato.obo\")\n",
    "po = pronto.Ontology(\"../ontologies/po.obo\")\n",
    "for term in pato:\n",
    "    for parent in term.parents.id:\n",
    "        tuples.append((\"PATO\",term.name,pato[parent].name))\n",
    "for term in po:\n",
    "    for parent in term.parents.id:\n",
    "        tuples.append((\"PO\",term.name,po[parent].name))  \n",
    "\n",
    "# Using that dataframe to see how the generalized distance percentile distributions compare between models.\n",
    "pairs = pd.DataFrame(tuples, columns=[\"Ontology\",\"Label 1\", \"Label 2\"])\n",
    "pairs[\"Wikipedia\"] = np.vectorize(dist)(doc2vec_wiki_model, pairs[\"Label 1\"], pairs[\"Label 2\"])\n",
    "pairs[\"PubMed\"] = np.vectorize(dist)(doc2vec_pubmed_model, pairs[\"Label 1\"], pairs[\"Label 2\"])\n",
    "pairs[\"Wikipedia\"] = pairs[\"Wikipedia\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_wiki\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"PubMed\"] = pairs[\"PubMed\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_pubmed\"].values, x, kind=\"rank\")/100)\n",
    "pairs.to_csv(\"../data/scratch/phrase_pair_generalized_results.csv\",index=False)\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use this dataset to identify sentences from abstracts that contain phenotypic information?\n",
    "The question we want to answer is whether or not these machine learning approaches in combination with the gathered dataset of phenotypic descriptions provides a valuable method for identifying which sentences in abstracts are likely to contain to information related to phenotyping, as this approach could be valuable in a curation pipeline. We will use a partially hands on approach, only evaluating the predicted matches rather than creating a full dataset of annotated abstracts. This means that the result will have to be evaluated as quantifying how many of the return sentences are primarily about a phenotype, partially about a phenotype, or not about a phenotype (three different categories), we cannot actually get an F1 score for this approach because we will not know how many were missed or available as positives in the dataset from which the sentences were drawn. Types of classes could be:\n",
    "1. Specifically mentioning a phenotype (e.g. \"*Plants treated with chemical X exhibited phenotype Y*.\")\n",
    "2. Only generally discussing phenotypes as a topic (e.g. \"*We organized a dataset of Z phenotypes from Arabidopsis thaliana*.\")\n",
    "3. Not talking about phenotypes (e.g. \"*Arabidopsis thaliana is one of the most popular experimental plants.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    from     value                                           sentence\n",
      "28    28  0.037570  Landsberg erecta cell suspension culture has u...\n",
      "26    26  0.039579  The stay-green, sugar-insensitive phenotype of...\n",
      "6      6  0.058020  Dioxins are highly toxic persistent organic po...\n",
      "25    25  0.058554  Regardless, the cell suspension culture still ...\n",
      "30    30  0.058685  Therefore, caution must be exercised in the in...\n",
      "0.09931780862815347\n",
      "0.10638297872340426\n"
     ]
    }
   ],
   "source": [
    "# Reading in the tagged dataset file of sentence that do or do not describe phenotypes.\n",
    "tagged_dataset = pd.read_csv(\"~/Desktop/arabidopsis_sentence_dataset.csv\")\n",
    "sentence_dict = {i:sentence for i,sentence in enumerate(tagged_dataset[\"sentence\"].values)}\n",
    "tags_dict = {i:tag for i,tag in enumerate(tagged_dataset[\"tag\"].values)}\n",
    "wikipedia_results = pw.pairwise_doc2vec_twogroup(doc2vec_wiki_model,sentence_dict,descriptions,\"cosine\").edgelist\n",
    "\n",
    "\n",
    "# Evaluting each sentence either by their mean distance to the phenotypes or their minimum distance.\n",
    "results = pd.DataFrame(wikipedia_results.groupby([\"from\"])[\"value\"].min())\n",
    "results = results.reset_index(inplace=False)\n",
    "results = results.sort_values(by=[\"value\"])\n",
    "results[\"sentence\"] = results[\"from\"].map(lambda x: sentences[x])\n",
    "print(results.head())\n",
    "\n",
    "\n",
    "# Generating the lists of true values and predictions and metrics.\n",
    "y_true = [tags_dict[i] for i in results[\"from\"].values]\n",
    "y_prob = [1.000-v for v in results[\"value\"].values]\n",
    "n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "baseline = Counter(y_true)[1]/len(y_true) \n",
    "area = auc(recall, precision)\n",
    "auc_to_baseline_auc_ratio = area/baseline\n",
    "print(area)\n",
    "print(baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
