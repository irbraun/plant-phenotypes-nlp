{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Dataset\n",
    "The purpose of this notebook is to look closer at the dataset of genes, natural language descriptions, and ontology term annotations that are used in this work. As included in the preprocessing notebooks, these data are drawn from files from either publications supplements like Oellrich, Walls et al. (2015) or model species databases such as TAIR, MaizeGDB, and SGN. The datasets are already loaded and merged using classes available through the oats package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 6.1086320877075195 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, flatten, to_hms\n",
    "from oats.utils.utils import function_wrapper_with_duration, remove_duplicates_retain_order\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import annotate_using_noble_coder, term_enrichment\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocab_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocab_connected_components, reduce_vocab_linares_pontes, token_enrichment\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the files that are used for this notebook.\n",
    "plant_dataset_path = \"../../plant-data/genes_texts_annots.csv\"\n",
    "\n",
    "# Paths to files with mappings to groups.\n",
    "kegg_pathways_path = \"../../plant-data/reshaped_data/kegg_pathways.csv\" \n",
    "plantcyc_pathways_path = \"../../plant-data/reshaped_data/plantcyc_pathways.csv\" \n",
    "lloyd_meinke_subsets_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets.csv\" \n",
    "lloyd_meinke_classes_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes.csv\" \n",
    "\n",
    "# Paths to files that map group identifers to longer group names.\n",
    "kegg_pathways_names_path = \"../../plant-data/reshaped_data/kegg_pathways_name_map.csv\"\n",
    "plantcyc_pathways_names_path = \"../../plant-data/reshaped_data/plantcyc_pathways_name_map.csv\"\n",
    "lloyd_meinke_subsets_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets_name_map.csv\"\n",
    "lloyd_meinke_classes_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes_name_map.csv\"\n",
    "\n",
    "# Path to file with plant ortholog mappings.\n",
    "ortholog_file_path = \"../../plant-data/databases/panther/PlantGenomeOrthologs_IRB_Modified.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and name an output directory according to when the notebooks was run.\n",
    "OUTPUT_NAME = \"composition\"\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",\"{}_{}_{}\".format(OUTPUT_NAME,datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'),random.randrange(1000,9999)))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>5066</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>6700</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                     5066                 2975\n",
       "1     gmx                       30                   23\n",
       "2     mtr                       37                   36\n",
       "3     osa                       92                   85\n",
       "4     sly                       70                   70\n",
       "5     zma                     1405                  810\n",
       "6   total                     6700                 3999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in and describing the dataset of plant genes.\n",
    "plant_dataset = Dataset(plant_dataset_path)\n",
    "plant_dataset.filter_has_description()\n",
    "plant_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's there for each species?\n",
    "The previously loaded dataset contains all of the genes that across six plant species that have natural language description data for phenotype(s) related to that gene. Each gene can have multiple descriptions annotated to it, which were combined or concatenated when the datasets from multiple sources were merged in creating the pickled datasets. Arabidopsis has the highest number of genes that satisfy this criteria, followed by maize, and then followed by the other four species which have a relatively low number of genes that satisfy this criteria, atleast given the sources used for this work. Note that the number of unique descriptions is lower than the number of genes in call cases, because multiple genes can have the same phenotype description associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_sents</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>5066</td>\n",
       "      <td>2975</td>\n",
       "      <td>26043</td>\n",
       "      <td>232445</td>\n",
       "      <td>7085</td>\n",
       "      <td>5116</td>\n",
       "      <td>232445</td>\n",
       "      <td>6561</td>\n",
       "      <td>4864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "      <td>5475</td>\n",
       "      <td>48983</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>48983</td>\n",
       "      <td>1722</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>478</td>\n",
       "      <td>3689</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3689</td>\n",
       "      <td>760</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>359</td>\n",
       "      <td>1678</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1678</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>263</td>\n",
       "      <td>2447</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2447</td>\n",
       "      <td>671</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>222</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>6700</td>\n",
       "      <td>3999</td>\n",
       "      <td>32680</td>\n",
       "      <td>289464</td>\n",
       "      <td>8043</td>\n",
       "      <td>5802</td>\n",
       "      <td>289464</td>\n",
       "      <td>7443</td>\n",
       "      <td>5703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions  total_sents  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species\n",
       "0     ath                     5066                 2975        26043       232445          7085          5116        232445           6561                      4864\n",
       "5     zma                     1405                  810         5475        48983          1846          1317         48983           1722                       503\n",
       "3     osa                       92                   85          478         3689           826           586          3689            760                        99\n",
       "4     sly                       70                   70          359         1678           577           438          1678            552                        99\n",
       "2     mtr                       37                   36          263         2447           718           516          2447            671                       126\n",
       "1     gmx                       30                   23           62          222            81            68           222             78                        12\n",
       "6   total                     6700                 3999        32680       289464          8043          5802        289464           7443                      5703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = plant_dataset\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatize_doc = lambda d: [wnl.lemmatize(x) for x in simple_preprocess(d)]\n",
    "\n",
    "dists = defaultdict(list)\n",
    "\n",
    "sent_lists = {}\n",
    "token_lists = {}\n",
    "stems_lists = {}\n",
    "lemma_lists = {}\n",
    "\n",
    "\n",
    "# For each individual species.\n",
    "for species in data.get_species():\n",
    "    df = data.to_pandas()\n",
    "    subset = df[df[\"species\"]==species]\n",
    "    sentences = [sent_tokenize(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_stemmed = [preprocess_string(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"descriptions\"].values]\n",
    "    sent_lists[species] = flatten(sentences)\n",
    "    token_lists[species] = flatten(descriptions_not_stemmed)\n",
    "    stems_lists[species] = flatten(descriptions_stemmed)    \n",
    "    lemma_lists[species] = flatten(descriptions_lemmatized)\n",
    "    \n",
    "    # What about the distributions of words per gene and sentences per gene?\n",
    "    dists[\"species\"].extend([species]*subset.shape[0])\n",
    "    dists[\"num_words\"].extend([len(word_tokenize(x)) for x in subset[\"descriptions\"].values])\n",
    "    dists[\"num_sents\"].extend([len(sent_tokenize(x)) for x in subset[\"descriptions\"].values])\n",
    "    \n",
    "# For the entire dataset including all of the species.\n",
    "df = data.to_pandas()\n",
    "subset = df\n",
    "sentences = [sent_tokenize(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_stemmed = [preprocess_string(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"descriptions\"].values]\n",
    "sent_lists[\"total\"] = flatten(sentences)\n",
    "token_lists[\"total\"] = flatten(descriptions_not_stemmed)\n",
    "stems_lists[\"total\"] = flatten(descriptions_stemmed)    \n",
    "lemma_lists[\"total\"] = flatten(descriptions_lemmatized)\n",
    "\n",
    "# What about lemmas that are uniquely used for a particular species?\n",
    "lemma_sets_unique_to_species = {}\n",
    "for species in data.get_species():\n",
    "    other_species = [s for s in data.get_species() if s != species]\n",
    "    lemmas_used_in_other_species = set(flatten([lemma_lists[s] for s in other_species]))\n",
    "    unique_lemmas = set(lemma_lists[species]).difference(lemmas_used_in_other_species)\n",
    "    lemma_sets_unique_to_species[species] = unique_lemmas\n",
    "lemma_sets_unique_to_species[\"total\"] = flatten([list(s) for s in lemma_sets_unique_to_species.values()])\n",
    "\n",
    "    \n",
    "# Create a dataframe to contain the summarizing information about this dataset, and sort it by number of genes.\n",
    "# Unique gene identifiers is just the total number of genes, this column name should be changed in the class...\n",
    "df = data.describe() \n",
    "condition = (df.species==\"total\")\n",
    "excluded = df[condition]\n",
    "included = df[~condition]\n",
    "df_sorted = included.sort_values(by=\"unique_gene_identifiers\", ascending=False)\n",
    "df = pd.concat([df_sorted,excluded])\n",
    "\n",
    "# Add columns summarizing information about the text descriptions in the dataset.\n",
    "df[\"total_sents\"] = df[\"species\"].map(lambda x: len(sent_lists[x]))\n",
    "df[\"total_words\"] = df[\"species\"].map(lambda x: len(token_lists[x]))\n",
    "df[\"unique_words\"] = df[\"species\"].map(lambda x: len(set(token_lists[x])))\n",
    "df[\"unique_stems\"] = df[\"species\"].map(lambda x: len(set(stems_lists[x])))\n",
    "df[\"total_lemmas\"] = df[\"species\"].map(lambda x: len(lemma_lists[x]))\n",
    "df[\"unique_lemmas\"] = df[\"species\"].map(lambda x: len(set(lemma_lists[x])))\n",
    "df[\"unique_lemmas_to_species\"] = df[\"species\"].map(lambda x: len(lemma_sets_unique_to_species[x]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zma</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zma</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zma</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zma</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zma</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>105</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zma</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zma</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>zma</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zma</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>zma</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zma</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zma</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zma</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zma</td>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zma</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zma</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zma</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zma</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>zma</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  num_words  num_sents\n",
       "0      zma         14          1\n",
       "1      zma        115          7\n",
       "2      zma          6          2\n",
       "3      zma         13          3\n",
       "4      zma         33          3\n",
       "5      zma        105          9\n",
       "6      zma        100          7\n",
       "7      zma         43          3\n",
       "8      zma         43          3\n",
       "9      zma        115          7\n",
       "10     zma         90          7\n",
       "11     zma         13          3\n",
       "12     zma         14          1\n",
       "13     zma         13          3\n",
       "14     zma         86         10\n",
       "15     zma         25          5\n",
       "16     zma         14          1\n",
       "17     zma         14          1\n",
       "18     zma         14          1\n",
       "19     zma         39          6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_distributions = pd.DataFrame(dists)\n",
    "text_distributions.to_csv(os.path.join(OUTPUT_DIR, \"word_sent_distributions.csv\"), index=False)\n",
    "text_distributions.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ath</th>\n",
       "      <td>25742</td>\n",
       "      <td>21371</td>\n",
       "      <td>26042</td>\n",
       "      <td>15418</td>\n",
       "      <td>18869</td>\n",
       "      <td>53180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmx</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtr</th>\n",
       "      <td>217</td>\n",
       "      <td>296</td>\n",
       "      <td>307</td>\n",
       "      <td>178</td>\n",
       "      <td>250</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>osa</th>\n",
       "      <td>191</td>\n",
       "      <td>457</td>\n",
       "      <td>558</td>\n",
       "      <td>319</td>\n",
       "      <td>324</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sly</th>\n",
       "      <td>150</td>\n",
       "      <td>243</td>\n",
       "      <td>260</td>\n",
       "      <td>136</td>\n",
       "      <td>225</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zma</th>\n",
       "      <td>4289</td>\n",
       "      <td>6193</td>\n",
       "      <td>5700</td>\n",
       "      <td>1921</td>\n",
       "      <td>7983</td>\n",
       "      <td>7368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1      3      4      6      2      5\n",
       "ath  25742  21371  26042  15418  18869  53180\n",
       "gmx     15     14     15     86     28     49\n",
       "mtr    217    296    307    178    250    443\n",
       "osa    191    457    558    319    324    742\n",
       "sly    150    243    260    136    225    324\n",
       "zma   4289   6193   5700   1921   7983   7368"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "species_to_num_to_quantity = {}\n",
    "for species in data.get_species():\n",
    "    how_many_species = lambda token: sum([(token in stems_lists[s]) for s in data.get_species()])\n",
    "    this_vocab = [token for token in stems_lists[species]]\n",
    "    distribution = [how_many_species(token) for token in this_vocab]\n",
    "    species_to_num_to_quantity[species] = dict(Counter(distribution))\n",
    "table = pd.DataFrame(species_to_num_to_quantity).transpose()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>25742</td>\n",
       "      <td>21371</td>\n",
       "      <td>26042</td>\n",
       "      <td>15418</td>\n",
       "      <td>18869</td>\n",
       "      <td>53180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>217</td>\n",
       "      <td>296</td>\n",
       "      <td>307</td>\n",
       "      <td>178</td>\n",
       "      <td>250</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>191</td>\n",
       "      <td>457</td>\n",
       "      <td>558</td>\n",
       "      <td>319</td>\n",
       "      <td>324</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>150</td>\n",
       "      <td>243</td>\n",
       "      <td>260</td>\n",
       "      <td>136</td>\n",
       "      <td>225</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>4289</td>\n",
       "      <td>6193</td>\n",
       "      <td>5700</td>\n",
       "      <td>1921</td>\n",
       "      <td>7983</td>\n",
       "      <td>7368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species      1      3      4      6      2      5\n",
       "0     ath  25742  21371  26042  15418  18869  53180\n",
       "1     gmx     15     14     15     86     28     49\n",
       "2     mtr    217    296    307    178    250    443\n",
       "3     osa    191    457    558    319    324    742\n",
       "4     sly    150    243    260    136    225    324\n",
       "5     zma   4289   6193   5700   1921   7983   7368"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(species_to_num_to_quantity).transpose().reset_index()\n",
    "table.rename({\"index\":\"species\"}, axis=\"columns\", inplace=True)\n",
    "table.to_csv(os.path.join(OUTPUT_DIR, \"words_shared_by_species.csv\"), index=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>others</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>5</td>\n",
       "      <td>15418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gmx</td>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>zma</td>\n",
       "      <td>5</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sly</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>osa</td>\n",
       "      <td>5</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mtr</td>\n",
       "      <td>5</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sly</td>\n",
       "      <td>4</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>osa</td>\n",
       "      <td>4</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mtr</td>\n",
       "      <td>4</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gmx</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ath</td>\n",
       "      <td>4</td>\n",
       "      <td>53180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>zma</td>\n",
       "      <td>4</td>\n",
       "      <td>7368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>3</td>\n",
       "      <td>26042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mtr</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>osa</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sly</td>\n",
       "      <td>3</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zma</td>\n",
       "      <td>3</td>\n",
       "      <td>5700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gmx</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>2</td>\n",
       "      <td>21371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zma</td>\n",
       "      <td>2</td>\n",
       "      <td>6193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sly</td>\n",
       "      <td>2</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>osa</td>\n",
       "      <td>2</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mtr</td>\n",
       "      <td>2</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gmx</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>osa</td>\n",
       "      <td>1</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>zma</td>\n",
       "      <td>1</td>\n",
       "      <td>7983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sly</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ath</td>\n",
       "      <td>1</td>\n",
       "      <td>18869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mtr</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>gmx</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>0</td>\n",
       "      <td>4289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>0</td>\n",
       "      <td>25742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species others  quantity\n",
       "18     ath      5     15418\n",
       "19     gmx      5        86\n",
       "23     zma      5      1921\n",
       "22     sly      5       136\n",
       "21     osa      5       319\n",
       "20     mtr      5       178\n",
       "34     sly      4       324\n",
       "33     osa      4       742\n",
       "32     mtr      4       443\n",
       "31     gmx      4        49\n",
       "30     ath      4     53180\n",
       "35     zma      4      7368\n",
       "12     ath      3     26042\n",
       "14     mtr      3       307\n",
       "15     osa      3       558\n",
       "16     sly      3       260\n",
       "17     zma      3      5700\n",
       "13     gmx      3        15\n",
       "6      ath      2     21371\n",
       "11     zma      2      6193\n",
       "10     sly      2       243\n",
       "9      osa      2       457\n",
       "8      mtr      2       296\n",
       "7      gmx      2        14\n",
       "27     osa      1       324\n",
       "29     zma      1      7983\n",
       "28     sly      1       225\n",
       "24     ath      1     18869\n",
       "26     mtr      1       250\n",
       "25     gmx      1        28\n",
       "1      gmx      0        15\n",
       "5      zma      0      4289\n",
       "4      sly      0       150\n",
       "3      osa      0       191\n",
       "2      mtr      0       217\n",
       "0      ath      0     25742"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueness_df = table.melt(id_vars=[\"species\"], var_name=\"others\", value_name=\"quantity\")\n",
    "uniqueness_df[\"others\"] = uniqueness_df[\"others\"]-1\n",
    "uniqueness_df.sort_values(by=\"others\", inplace=True, ascending=False)\n",
    "uniqueness_df.to_csv(os.path.join(OUTPUT_DIR, \"words_shared_by_species_melted.csv\"), index=False)\n",
    "uniqueness_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the ontology term annotations for each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_sents</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "      <th>go</th>\n",
       "      <th>po</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>5066</td>\n",
       "      <td>2975</td>\n",
       "      <td>26043</td>\n",
       "      <td>232445</td>\n",
       "      <td>7085</td>\n",
       "      <td>5116</td>\n",
       "      <td>232445</td>\n",
       "      <td>6561</td>\n",
       "      <td>4864</td>\n",
       "      <td>4691</td>\n",
       "      <td>3220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "      <td>5475</td>\n",
       "      <td>48983</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>48983</td>\n",
       "      <td>1722</td>\n",
       "      <td>503</td>\n",
       "      <td>184</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>478</td>\n",
       "      <td>3689</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3689</td>\n",
       "      <td>760</td>\n",
       "      <td>99</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>359</td>\n",
       "      <td>1678</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1678</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>263</td>\n",
       "      <td>2447</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2447</td>\n",
       "      <td>671</td>\n",
       "      <td>126</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>222</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>6700</td>\n",
       "      <td>3999</td>\n",
       "      <td>32680</td>\n",
       "      <td>289464</td>\n",
       "      <td>8043</td>\n",
       "      <td>5802</td>\n",
       "      <td>289464</td>\n",
       "      <td>7443</td>\n",
       "      <td>5703</td>\n",
       "      <td>5002</td>\n",
       "      <td>3547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions  total_sents  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species    go    po\n",
       "0     ath                     5066                 2975        26043       232445          7085          5116        232445           6561                      4864  4691  3220\n",
       "5     zma                     1405                  810         5475        48983          1846          1317         48983           1722                       503   184   111\n",
       "3     osa                       92                   85          478         3689           826           586          3689            760                        99    46    92\n",
       "4     sly                       70                   70          359         1678           577           438          1678            552                        99    23    65\n",
       "2     mtr                       37                   36          263         2447           718           516          2447            671                       126    30    32\n",
       "1     gmx                       30                   23           62          222            81            68           222             78                        12    28    27\n",
       "6   total                     6700                 3999        32680       289464          8043          5802        289464           7443                      5703  5002  3547"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many of the genes in this dataset for each species are mapped to atleast one term from a given ontology?\n",
    "num_mapped_go = {}\n",
    "num_mapped_po = {}\n",
    "for species in data.get_species():\n",
    "    d = data.to_pandas()\n",
    "    subset = d[d[\"species\"]==species]    \n",
    "    num_mapped_po[species] = len([t for t in subset[\"annotations\"].values if \"PO\" in t])\n",
    "    num_mapped_go[species] = len([t for t in subset[\"annotations\"].values if \"GO\" in t])\n",
    "num_mapped_go[\"total\"] = sum(list(num_mapped_go.values()))   \n",
    "num_mapped_po[\"total\"] = sum(list(num_mapped_po.values()))\n",
    "df[\"go\"] = df[\"species\"].map(lambda x: num_mapped_go[x])\n",
    "df[\"po\"] = df[\"species\"].map(lambda x: num_mapped_po[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the biologically relevant groups like biochemical pathways and phenotypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_sents</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "      <th>go</th>\n",
       "      <th>po</th>\n",
       "      <th>kegg</th>\n",
       "      <th>plantcyc</th>\n",
       "      <th>lloyd_meinke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>5066</td>\n",
       "      <td>2975</td>\n",
       "      <td>26043</td>\n",
       "      <td>232445</td>\n",
       "      <td>7085</td>\n",
       "      <td>5116</td>\n",
       "      <td>232445</td>\n",
       "      <td>6561</td>\n",
       "      <td>4864</td>\n",
       "      <td>4691</td>\n",
       "      <td>3220</td>\n",
       "      <td>1084</td>\n",
       "      <td>654</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "      <td>5475</td>\n",
       "      <td>48983</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>48983</td>\n",
       "      <td>1722</td>\n",
       "      <td>503</td>\n",
       "      <td>184</td>\n",
       "      <td>111</td>\n",
       "      <td>157</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>478</td>\n",
       "      <td>3689</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3689</td>\n",
       "      <td>760</td>\n",
       "      <td>99</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>359</td>\n",
       "      <td>1678</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1678</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>263</td>\n",
       "      <td>2447</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2447</td>\n",
       "      <td>671</td>\n",
       "      <td>126</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>222</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>6700</td>\n",
       "      <td>3999</td>\n",
       "      <td>32680</td>\n",
       "      <td>289464</td>\n",
       "      <td>8043</td>\n",
       "      <td>5802</td>\n",
       "      <td>289464</td>\n",
       "      <td>7443</td>\n",
       "      <td>5703</td>\n",
       "      <td>5002</td>\n",
       "      <td>3547</td>\n",
       "      <td>1263</td>\n",
       "      <td>796</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions  total_sents  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species    go    po  kegg  plantcyc  lloyd_meinke\n",
       "0     ath                     5066                 2975        26043       232445          7085          5116        232445           6561                      4864  4691  3220  1084       654          1570\n",
       "5     zma                     1405                  810         5475        48983          1846          1317         48983           1722                       503   184   111   157       133             0\n",
       "3     osa                       92                   85          478         3689           826           586          3689            760                        99    46    92     1         4             0\n",
       "4     sly                       70                   70          359         1678           577           438          1678            552                        99    23    65    18         3             0\n",
       "2     mtr                       37                   36          263         2447           718           516          2447            671                       126    30    32     0         2             0\n",
       "1     gmx                       30                   23           62          222            81            68           222             78                        12    28    27     3         0             0\n",
       "6   total                     6700                 3999        32680       289464          8043          5802        289464           7443                      5703  5002  3547  1263       796          1570"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the groupings that we're interested in mapping to? Uses the paths defined at the top of the notebook.\n",
    "groupings_dict = {\n",
    "    \"kegg\":(kegg_pathways_path, kegg_pathways_names_path),\n",
    "    \"plantcyc\":(plantcyc_pathways_path, plantcyc_pathways_names_path),\n",
    "    \"lloyd_meinke\":(lloyd_meinke_subsets_path, lloyd_meinke_subsets_names_path)\n",
    "}\n",
    "\n",
    "\n",
    "for name,(filename,mapfile) in groupings_dict.items():\n",
    "    groups = Groupings(filename, {row.group_id:row.group_name for row in pd.read_csv(mapfile).itertuples()})\n",
    "    id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(data)\n",
    "    group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "    species_dict = data.get_species_dictionary()\n",
    "    num_mapped = {}\n",
    "    for species in data.get_species():\n",
    "        num_mapped[species] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "    num_mapped[\"total\"] = sum(list(num_mapped.values()))    \n",
    "    df[name] = df[\"species\"].map(lambda x: num_mapped[x])  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the other biologically relevant information like orthologous genes and protein interactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_sents</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "      <th>go</th>\n",
       "      <th>po</th>\n",
       "      <th>kegg</th>\n",
       "      <th>plantcyc</th>\n",
       "      <th>lloyd_meinke</th>\n",
       "      <th>panther</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>5066</td>\n",
       "      <td>2975</td>\n",
       "      <td>26043</td>\n",
       "      <td>232445</td>\n",
       "      <td>7085</td>\n",
       "      <td>5116</td>\n",
       "      <td>232445</td>\n",
       "      <td>6561</td>\n",
       "      <td>4864</td>\n",
       "      <td>4691</td>\n",
       "      <td>3220</td>\n",
       "      <td>1084</td>\n",
       "      <td>654</td>\n",
       "      <td>1570</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "      <td>5475</td>\n",
       "      <td>48983</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>48983</td>\n",
       "      <td>1722</td>\n",
       "      <td>503</td>\n",
       "      <td>184</td>\n",
       "      <td>111</td>\n",
       "      <td>157</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>478</td>\n",
       "      <td>3689</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3689</td>\n",
       "      <td>760</td>\n",
       "      <td>99</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>359</td>\n",
       "      <td>1678</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1678</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>263</td>\n",
       "      <td>2447</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2447</td>\n",
       "      <td>671</td>\n",
       "      <td>126</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>222</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>6700</td>\n",
       "      <td>3999</td>\n",
       "      <td>32680</td>\n",
       "      <td>289464</td>\n",
       "      <td>8043</td>\n",
       "      <td>5802</td>\n",
       "      <td>289464</td>\n",
       "      <td>7443</td>\n",
       "      <td>5703</td>\n",
       "      <td>5002</td>\n",
       "      <td>3547</td>\n",
       "      <td>1263</td>\n",
       "      <td>796</td>\n",
       "      <td>1570</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions  total_sents  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species    go    po  kegg  plantcyc  lloyd_meinke  panther\n",
       "0     ath                     5066                 2975        26043       232445          7085          5116        232445           6561                      4864  4691  3220  1084       654          1570      317\n",
       "5     zma                     1405                  810         5475        48983          1846          1317         48983           1722                       503   184   111   157       133             0      443\n",
       "3     osa                       92                   85          478         3689           826           586          3689            760                        99    46    92     1         4             0       86\n",
       "4     sly                       70                   70          359         1678           577           438          1678            552                        99    23    65    18         3             0       11\n",
       "2     mtr                       37                   36          263         2447           718           516          2447            671                       126    30    32     0         2             0        0\n",
       "1     gmx                       30                   23           62          222            81            68           222             78                        12    28    27     3         0             0        1\n",
       "6   total                     6700                 3999        32680       289464          8043          5802        289464           7443                      5703  5002  3547  1263       796          1570      858"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PantherDB for plant orthologs.\n",
    "ortholog_edgelist = AnyInteractions(data.get_name_to_id_dictionary(), ortholog_file_path)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in ortholog_edgelist.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"panther\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b80e50f32324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0mgenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gene_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstring_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinInteractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaming_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minteraction_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mspecies_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_species_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnum_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oats/oats/biology/relationships.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, id_to_gene_dict, name_mapping_file, *string_data_files)\u001b[0m\n\u001b[1;32m     33\u001b[0m \t\t\"\"\"\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_mapping_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_interaction_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_gene_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_data_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oats/oats/biology/relationships.py\u001b[0m in \u001b[0;36m_process_interaction_files\u001b[0;34m(self, id_to_gene_dict, string_data_files)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# Read in the tables from the STRING database and do any necessary preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_data_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oats/oats/biology/relationships.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# Read in the tables from the STRING database and do any necessary preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring_data_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# STRING DB for protein-protein interactions.\n",
    "naming_file = \"../../plant-data/databases/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../../plant-data/databases/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis\n",
    "    \"../../plant-data/databases/string/4577.protein.links.detailed.v11.0.txt\", # Maize\n",
    "    \"../../plant-data/databases/string/4530.protein.links.detailed.v11.0.txt\", # Tomato \n",
    "    \"../../plant-data/databases/string/4081.protein.links.detailed.v11.0.txt\", # Medicago\n",
    "    \"../../plant-data/databases/string/3880.protein.links.detailed.v11.0.txt\", # Rice \n",
    "    \"../../plant-data/databases/string/3847.protein.links.detailed.v11.0.txt\", # Soybean\n",
    "    \"../../plant-data/databases/string/9606.protein.links.detailed.v11.0.txt\", # Human\n",
    "]\n",
    "genes = data.get_gene_dictionary()\n",
    "string_data = ProteinInteractions(genes, naming_file, *interaction_files)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in string_data.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"stringdb\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write that dataframe with all the information about datast to a file.\n",
    "df.to_csv(os.path.join(OUTPUT_DIR,\"full_dataset_composition.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the vocabularies used for different species compare?\n",
    "One of the things we are interested in is discovering or recovering phenotype similarity between different species in order to identify phenologs (phenotypes between species that share some underlying genetic cause). For this reason, we are interested in how the vocabularies used to describe phenotypes between different species vary, because this will impact how feasible it is to use a dataset like this to identify phenologs. Because the Arabidopsis and maize datasets are the largest in this case, we will compare the vocabularies used in describing the phenotypes associated with the genes from these species in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lemmas as the vocabulary components.\n",
    "vocabs = {s:set(lemma_list) for s,lemma_list in lemma_lists.items()}\n",
    "fdist_zma = FreqDist(lemma_lists[\"zma\"])\n",
    "fdist_ath = FreqDist(lemma_lists[\"ath\"])\n",
    "\n",
    "# Using word stems as the vocabulary components.\n",
    "#vocabs = {s:set(stems_list) for s,stems_list in stems_lists.items()}\n",
    "#fdist_zma = FreqDist(stems_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(stems_lists[\"ath\"])\n",
    "\n",
    "# Using tokens (full words) as the vocabulary components.\n",
    "#vocabs = {s:set(token_list) for s,token_list in token_lists.items()}\n",
    "#fdist_zma = FreqDist(token_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(token_lists[\"ath\"])\n",
    "\n",
    "union_vocab = vocabs[\"zma\"].union(vocabs[\"ath\"])\n",
    "table = pd.DataFrame({\"token\":list(union_vocab)})\n",
    "stops = set(stopwords.words('english'))\n",
    "table = table[~table.token.isin(stops)]\n",
    "table[\"part_of_speech\"] = table[\"token\"].map(lambda x: nltk.pos_tag([x])[0][1][:2])\n",
    "table[\"ath_freq\"] = table[\"token\"].map(lambda x: fdist_ath[x])\n",
    "table[\"ath_rate\"] = table[\"ath_freq\"]*100/len(token_lists[\"ath\"])\n",
    "table[\"zma_freq\"] = table[\"token\"].map(lambda x: fdist_zma[x])\n",
    "table[\"zma_rate\"] = table[\"zma_freq\"]*100/len(token_lists[\"zma\"])\n",
    "table[\"diff\"] = table[\"ath_rate\"]-table[\"zma_rate\"]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"token_frequencies.csv\"), index=False)\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for Arabidopsis than maize descriptions in this dataset?\n",
    "table.sort_values(by=\"diff\", ascending=False, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for maize than Arabidopsis descriptions in this dataset?\n",
    "table.sort_values(by=\"diff\", ascending=True, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the mean absolute value of the rate differences different between the different parts of speech?\n",
    "table[\"abs_diff\"] = abs(table[\"diff\"])\n",
    "pos_table = table.groupby(\"part_of_speech\").mean()\n",
    "pos_table.sort_values(by=\"abs_diff\", inplace=True, ascending=False)\n",
    "pos_table = pos_table[[\"abs_diff\"]]\n",
    "pos_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Working on the Venn Diagram for this part, unused currently.\n",
    "#print(table.shape)\n",
    "#zma_only = table[table[\"ath_rate\"]==0]\n",
    "#ath_only = table[table[\"zma_rate\"]==0]\n",
    "#print(zma_only.shape)\n",
    "#print(ath_only.shape)\n",
    "#print(ath_only.shape[0]+zma_only.shape[0])\n",
    "#ath_only.head(10)\n",
    "# We need to create a mapping between stems and the words that were present for them.\n",
    "# This is because what we want is the stems that are exclusive to a species.\n",
    "# but then the words that are actually there for those stems, so that we can count their parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Term and Word Enrichment for Groups of Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset of phenotype descriptions and ontology annotations.\n",
    "plant_dataset = Dataset(plant_dataset_path)\n",
    "data = plant_dataset\n",
    "data.filter_has_description()\n",
    "#data.filter_has_annotation(\"GO\")\n",
    "data.filter_has_annotation(\"PO\")\n",
    "d = data.get_description_dictionary()\n",
    "texts = {i:\" \".join(simple_preprocess(t)) for i,t in d.items()}\n",
    "len(texts)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ontology objects for all the biological ontologies being used.\n",
    "go_pickle_path = \"../ontologies/go.pickle\"                                                                \n",
    "po_pickle_path = \"../ontologies/po.pickle\"                                                             \n",
    "pato_pickle_path = \"../ontologies/pato.pickle\"\n",
    "pato = load_from_pickle(pato_pickle_path)\n",
    "po = load_from_pickle(po_pickle_path)\n",
    "go = load_from_pickle(go_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_go_annotations = data.get_annotations_dictionary(\"GO\")\n",
    "curated_po_annotations = data.get_annotations_dictionary(\"PO\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which GO terms are used to annotate the most genes in this dataset?\n",
    "term_id_to_ids = defaultdict(list)\n",
    "for i,term_id_list in curated_go_annotations.items():\n",
    "    for term_id in term_id_list:\n",
    "        term_id_to_ids[term_id].append(i)\n",
    "term_id_to_num_ids = {k:len(v) for k,v in term_id_to_ids.items()}\n",
    "terms_df = pd.DataFrame(term_id_to_num_ids.items(), columns=[\"term_id\", \"freq\"])\n",
    "\n",
    "def get_term_name(ont,i):\n",
    "    try:\n",
    "        return(ont[i].name)\n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "terms_df[\"term_name\"] = terms_df[\"term_id\"].map(lambda x: get_term_name(go,x))\n",
    "terms_df.sort_values(by=\"freq\", ascending=False, inplace=True)\n",
    "terms_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the group be ones that have that GO term anntation.\n",
    "#go_term_id_of_interest = \"GO:0009640\"\n",
    "#gene_ids_in_this_pathway = [k for k,v in curated_go_annotations.items() if go_term_id_of_interest in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mappings from this dataset to PlantCyc information.\n",
    "#pmn_pathways_filename = \"../data/pickles/groupings_from_pmn_pathways.pickle\"                        \n",
    "#groups = load_from_pickle(pmn_pathways_filename)\n",
    "#id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(data)\n",
    "\n",
    "\n",
    "# Reading in the dataset of groupings for pathways in PlantCyc.\n",
    "plantcyc_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(plantcyc_pathways_names_path).itertuples()}\n",
    "plantcyc_grouping = Groupings(plantcyc_pathways_path, plantcyc_name_mapping)\n",
    "id_to_group_ids, group_id_to_ids = plantcyc_grouping.get_groupings_for_dataset(data)\n",
    "\n",
    "# Look at which pathways are best represented in this dataset.\n",
    "pathways_sorted = sorted(group_id_to_ids.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "pathways_sorted_lengths = [(i,len(l)) for (i,l) in pathways_sorted]\n",
    "pathways_df = pd.DataFrame(pathways_sorted_lengths, columns=[\"pathway_id\",\"num_genes\"])\n",
    "pathways_df[\"pathway_name\"] = pathways_df[\"pathway_id\"].map(lambda x: plantcyc_grouping.get_long_name(x))\n",
    "pathways_df = pathways_df[[\"pathway_name\",\"pathway_id\",\"num_genes\"]]\n",
    "pathways_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some example pathway to use.\n",
    "#pathway_id = \"PWY-361\"\n",
    "pathway_id = \"PWY-581\"\n",
    "#pathway_id = \"PWY-1121\"\n",
    "pathway_id = \"PWY-695\"\n",
    "gene_ids_in_this_pathway = group_id_to_ids[pathway_id]\n",
    "gene_ids_in_this_pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = term_enrichment(curated_po_annotations, gene_ids_in_this_pathway, po).head(20)\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "results[\"info_content\"] = results[\"term_id\"].map(lambda x: po.ic(x))\n",
    "results.sort_values(by=\"info_content\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "# ns   P > 0.05\n",
    "# *    P ≤ 0.05\n",
    "# **   P ≤ 0.01\n",
    "# ***  P ≤ 0.001\n",
    "# **** P ≤ 0.0001\n",
    "\n",
    "# This lambda won't work is passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results.itertuples():\n",
    "    wordcloud[\"Weight\"].append(int(1/row.p_value_adj))\n",
    "    wordcloud[\"Word\"].append(\"{} ({})\".format(row.term_id,row.term_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = term_enrichment(curated_go_annotations, gene_ids_in_this_pathway, go).head(20)\n",
    "\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "\n",
    "\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "\n",
    "results[\"info_content\"] = results[\"term_id\"].map(lambda x: go.ic(x))\n",
    "results.sort_values(by=\"info_content\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "# This lambda won't work is passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = token_enrichment(texts, gene_ids_in_this_pathway).head(20)\n",
    "\n",
    "\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "\n",
    "\n",
    "# This lambda won't work if passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results.itertuples():\n",
    "    wordcloud[\"Weight\"].append(int(1/row.p_value_adj))\n",
    "    wordcloud[\"Word\"].append(row.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wordcloud).to_csv(os.path.join(OUTPUT_DIR, \"{}_word_cloud.csv\".format(pathway_id)), index=False)\n",
    "pd.DataFrame(wordcloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
