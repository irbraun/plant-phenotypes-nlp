{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Biochemical Pathways and Groups in Phenotypic Description Data\n",
    "The purpose of this notebook is to determine how well networks generated using phenotypic description based similarity measurements through a variety of embedding methods correlate with information about gene membership in biochemical pathways, or additionally in a arbitrary grouping such as functional categories as well. The theory this is based on is that because phenotypes are at some level the consequence of molecular or other lower level processes, two genes which are involved in a common process or pathway are more likely to impact a shared phenotype or at least two phenotypes that share more characteristics than what might be expected due to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/irbraun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/irbraun/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.datasets.dataset import Dataset\n",
    "from oats.datasets.groupings import Groupings\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.datasets.string import String\n",
    "from oats.datasets.known import Known\n",
    "from oats.annotation.annotation import annotate_using_rabin_karp, annotate_using_noble_coder\n",
    "from oats.graphs.pairwise import pairwise_doc2vec_onegroup, pairwise_counting_onegroup, pairwise_annotations_onegroup\n",
    "from oats.graphs.pairwise import merge_edgelists, subset_edgelist_with_ids, pairwise_word2vec_onegroup\n",
    "from oats.graphs.pairwise import make_undirected\n",
    "from oats.graphs.pairwise import remove_self_loops\n",
    "from oats.graphs.indexed import IndexedGraph\n",
    "from oats.graphs.models import train_logistic_regression_model, apply_logistic_regression_model\n",
    "from oats.graphs.models import train_random_forest_model, apply_random_forest_model\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, build_vocabulary_from_tokens\n",
    "from oats.utils.utils import function_wrapper_with_duration\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested dictionary to summarize output with shape dict[method][(tag,metric)] --> value\n",
    "# The dictionary is organized this way for printing within this notebook and saving as a csv file.\n",
    "TAG = \"biochemical pathways\"\n",
    "TABLE = defaultdict(dict)\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Defining all of the files and paths used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = \"../data/pickles/ppn_dataset.pickle\"                        # The full dataset object.\n",
    "groupings_filename = \"../data/pickles/pmn_pathways.pickle\"                     # The groupings object.\n",
    "background_corpus_filename = \"../data/corpus_related_files/background.txt\"     # Text file with background content.\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/phenotypes.txt\"     # Text file with specific content.\n",
    "doc2vec_model_filename = \"../gensim/enwiki_dbow/doc2vec.bin\"                   # File holding saved Doc2Vec model.\n",
    "word2vec_model_filename = \"../gensim/wiki_sg/word2vec.bin\"                     # File holding saved Word2Vec model.\n",
    "ontology_filename = \"../ontologies/mo.obo\"                                     # Ontology file in OBO format.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"                          # File for NOBLE Coder annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading in dataset and subsetting based on membership in biochemical pathways\n",
    "The dataset can be loaded from an existing dataset object that was saved as a pickle, or csv files can be loaded directly into a new dataset object. Subsetting here is done so that only genes which are mapped to atleast one of the categories from whatever groupings are being used (such as biochemical pathways) are retained. Any other filtering to reduce the number of genes present in the dataset can be done at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 2683\n",
      "Number of unique IDs:            2683\n",
      "Number of unique descriptions:   1794\n",
      "Number of unique gene name sets: 2683\n",
      "Number of species represented:   6\n",
      "Number of rows in the dataframe: 2683\n",
      "Number of unique IDs:            2683\n",
      "Number of unique descriptions:   1794\n",
      "Number of unique gene name sets: 2683\n",
      "Number of species represented:   6\n"
     ]
    }
   ],
   "source": [
    "# Reading in the entire dataset, subsetting for species and desired annotation types.\n",
    "dataset = load_from_pickle(dataset_filename)\n",
    "dataset.describe()\n",
    "dataset.collapse_by_all_gene_names()\n",
    "dataset.filter_has_description()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups present for each species\n",
      "  ath: 627\n",
      "  zma: 565\n",
      "  mtr: 520\n",
      "  osa: 569\n",
      "  gmx: 618\n",
      "  sly: 524\n",
      "Number of genes names mapped to any group for each species\n",
      "  ath: 9959\n",
      "  zma: 14319\n",
      "  mtr: 14100\n",
      "  osa: 12156\n",
      "  gmx: 20677\n",
      "  sly: 13232\n"
     ]
    }
   ],
   "source": [
    "# Subsetting the dataset to include only those genes that map to at least one group from some classification source.\n",
    "groups = load_from_pickle(groupings_filename)\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "groups.describe()\n",
    "groups.write_to_csv(\"~/Desktop/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "summary = defaultdict(dict)\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "for species in dataset.get_species():\n",
    "    summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "    summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table.loc[\"total\"]= table.sum()\n",
    "table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_summary.csv\"))\n",
    "\n",
    "\n",
    "# Generate a table describing how many genes from each species map to which particular group.\n",
    "summary = defaultdict(dict)\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "    summary[group_id][\"total\"] = len([x for x in ids])\n",
    "table = pd.DataFrame(summary).transpose()\n",
    "table = table.sort_values(by=\"total\", ascending=False)\n",
    "table = table.reset_index(inplace=False)\n",
    "table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "table.loc[\"total\"] = table.sum()\n",
    "table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"mappings_by_group.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe: 500\n",
      "Number of unique IDs:            500\n",
      "Number of unique descriptions:   392\n",
      "Number of unique gene name sets: 500\n",
      "Number of species represented:   5\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset based on whether or not the genes were successfully mapped to a group.\n",
    "dataset.filter_with_ids(group_mapped_ids)\n",
    "dataset.filter_random_k(500)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the mappings in each direction again now that the dataset has been subset.\n",
    "id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing of text descriptions, generating vocabularies, and other optional steps\n",
    "Add some description for the different things that are done here. Some of these take a long time to run only load or run the parts which are used to generate the pairwise similarity networks in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a vocabulary by looking at what words are overrepresented in domain specific text.\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "tokens = get_overrepresented_tokens(phenotypes_corpus, background_corpus, max_features=2000)\n",
    "vocabulary_from_text = build_vocabulary_from_tokens(tokens)\n",
    "\n",
    "# Constructing a vocabulary by assuming all words present in a given ontology are important.\n",
    "ontology = Ontology(ontology_filename)\n",
    "vocabulary_from_ontology = build_vocabulary_from_tokens(ontology.get_tokens())\n",
    "\n",
    "# Loading other necessary objects or models and annotating text with ontology terms.\n",
    "doc2vec_model = gensim.models.Doc2Vec.load(doc2vec_model_filename)\n",
    "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_filename)\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "annotations_from_dataset = dataset.get_annotations_dictionary()\n",
    "#annotations_rabin_karp = annotate_using_rabin_karp(descriptions, ontology)\n",
    "annotations_noblecoder_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=1)\n",
    "annotations_noblecoder_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"mo\", precise=0)\n",
    "\n",
    "# Generating random descriptions that are drawn from same token population and retain original lengths.\n",
    "#tokens = [w for w in itertools.chain.from_iterable(word_tokenize(desc) for desc in descriptions.values())]\n",
    "#scrambled_descriptions = {k:\" \".join(np.random.choice(tokens,len(word_tokenize(v)))) for k,v in descriptions.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generating vector representations, pairwise similarity matrices, and edgelists\n",
    "This includes things like whether or not to do capitilization removal, lemmatization, stemming, etc. on the descriptions present in the dataset. This could also include things like scrambling the contexts of each description to establish a baseline performance measure. This could also include things like reducing the vocabulary size through the preprocessing methods given here but also through additional means such as provided a reduced (more specialized) vocabulary dictionary to the vectorizing functions so that only those words which are most likely to have meaning have positions with those vectors. Should also test other vectorization methods such as term-frequency inverse-document-frequency for weighting. Can also change how the feature selection is done for those vectors by altering whether the *n*-grams are based on word or characters, and what the range of *n* is. Also should add the thing about combining the term annotations are text into a single bag-of-words vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the pairwise edgelists for each method.\n",
    "#graphs = {} # A mapping between method names and graph objects containing edgelists and vector mappings.\n",
    "#graphs[\"doc2vec\"] = pairwise_doc2vec_onegroup(doc2vec_model, descriptions, metric=\"cosine\")\n",
    "#graphs[\"bagofwords\"] = pairwise_counting_onegroup(descriptions, binary=False, metric=\"cosine\", max_features=5000) \n",
    "#graphs[\"ngram_word\"] = pairwise_counting_onegroup(descriptions, metric=\"cosine\", binary=False, analyzer=\"word\", ngram_range=(1,2), max_features=5000)\n",
    "#graphs[\"ngram_char\"] = pairwise_counting_onegroup(descriptions, metric=\"cosine\", binary=False, analyzer=\"char\", ngram_range=(4,6), max_features=5000)\n",
    "#graphs[\"setofwords\"] = pairwise_counting_onegroup(descriptions, binary=True, metric=\"jaccard\")\n",
    "#graphs[\"annot_rabk\"] = pairwise_annotations_onegroup(annotations_rabin_karp, ontology, binary=True, metric=\"cosine\")\n",
    "#graphs[\"annot_ncpr\"] = pairwise_annotations_onegroup(annotations_noblecoder_precise, ontology, binary=True, metric=\"cosine\")\n",
    "#graphs[\"annot_ncpa\"] = pairwise_annotations_onegroup(annotations_noblecoder_partial, ontology, binary=True, metric=\"cosine\")\n",
    "#graphs[\"wrd2vec_avg\"] = pairwise_word2vec_onegroup(word2vec_model, descriptions, metric=\"cosine\", method=\"mean\")\n",
    "#graphs[\"wrd2vec_max\"] = pairwise_word2vec_onegroup(word2vec_model, descriptions, metric=\"cosine\", method=\"max\")\n",
    "#graphs[\"bag_pubmed\"] = pairwise_counting_onegroup(descriptions, binary=False, metric=\"cosine\", max_features=5000, vocabulary=vocabulary_from_text) \n",
    "#graphs[\"set_pubmed\"] = pairwise_counting_onegroup(descriptions, binary=True, metric=\"cosine\", max_features=5000, vocabulary=vocabulary_from_text)\n",
    "#graphs[\"bag_ontvoc\"] = pairwise_counting_onegroup(descriptions, binary=False, metric=\"cosine\", max_features=5000, vocabulary=vocabulary_from_ontology) \n",
    "#graphs[\"set_ontvoc\"] = pairwise_counting_onegroup(descriptions, binary=True, metric=\"cosine\", max_features=5000, vocabulary=vocabulary_from_ontology)\n",
    "#print(\"{} methods completed\".format(len(graphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the pairwise edgelists for each method, multiprocessing verion.\n",
    "name_function_args_tuples = [\n",
    "    (\"doc2vec\", pairwise_doc2vec_onegroup, {\"model\":doc2vec_model, \"object_dict\":descriptions, \"metric\":\"cosine\"}),\n",
    "    (\"bagofwords\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"max_features\":5000}),\n",
    "    (\"ngram_word\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"max_features\":5000}),\n",
    "    (\"ngram_char\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"char\", \"ngram_range\":(4,6), \"max_features\":5000}),\n",
    "    (\"setofwords\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"jaccard\", \"binary\":True, \"max_features\":5000}),\n",
    "    (\"annot_ncpr\", pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_precise, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\"}),\n",
    "    (\"annot_ncpa\", pairwise_annotations_onegroup, {\"annotations_dict\":annotations_noblecoder_partial, \"ontology\":ontology, \"binary\":True, \"metric\":\"jaccard\"}),\n",
    "    (\"word2vec_avg\", pairwise_word2vec_onegroup, {\"model\":word2vec_model, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}),\n",
    "    (\"word2vec_max\", pairwise_word2vec_onegroup, {\"model\":word2vec_model, \"object_dict\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}),\n",
    "    (\"bag_pubmed\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"max_features\":5000, \"vocabulary\":vocabulary_from_text}),\n",
    "    (\"set_pubmed\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":True, \"max_features\":5000, \"vocabulary\":vocabulary_from_text}),\n",
    "    (\"bag_ontvoc\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":False, \"max_features\":5000, \"vocabulary\":vocabulary_from_ontology}),\n",
    "    (\"set_ontvoc\", pairwise_counting_onegroup, {\"object_dict\":descriptions, \"metric\":\"cosine\", \"binary\":True, \"max_features\":5000, \"vocabulary\":vocabulary_from_ontology}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/anaconda2/envs/oats/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-301e2e62c666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_wrapper_with_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdurations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-301e2e62c666>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_wrapper_with_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdurations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_function_args_tuples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/oats/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate all of the similarity matrices in parallel.\n",
    "start_time_mp = time.perf_counter()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = [pool.apply_async(function_wrapper_with_duration, args=(func, args)) for (name,func,args) in name_function_args_tuples]\n",
    "results = [result.get() for result in results]\n",
    "graphs = {tup[0]:result[0] for tup,result in zip(name_function_args_tuples,results)}\n",
    "durations = {tup[0]:result[1] for tup,result in zip(name_function_args_tuples,results)}\n",
    "pool.close()\n",
    "pool.join()    \n",
    "total_time_mp = time.perf_counter()-start_time_mp\n",
    "\n",
    "# Reporting how long each matrix took to build and how much time parallel processing saved.\n",
    "print(\"Durations of generating each pairwise similarity matrix (hh:mm:ss)\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "savings = total_time_mp/sum(durations.values())\n",
    "for (name,duration) in durations.items():\n",
    "    print(\"{:15} {}\".format(name, to_hms(duration)))\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"{:15} {}\".format(\"total\", to_hms(sum(durations.values()))))\n",
    "print(\"{:15} {} ({:.2%} of single thread time)\".format(\"multiprocess\", to_hms(total_time_mp), savings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphs = {tup[0]:tup[1](**tup[2]) for tup in name_function_args_tuples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all of the edgelist dataframes together.\n",
    "methods = list(graphs.keys())\n",
    "edgelists = {k:v.edgelist for k,v in graphs.items()}\n",
    "df = merge_edgelists(edgelists, default_value=0.000)\n",
    "df = remove_self_loops(df)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merging the edgelists with information about biochemical pathway membership and species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column indicating whether or not the two genes have atleast one pathway in common.\n",
    "df[\"common\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "print(Counter(df[\"common\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column indicating whether or not the two genes are from the same species.\n",
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)*1\n",
    "print(Counter(df[\"same\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training logistic regression or random forest models to combine multiple methods\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating indicating how likely is it that two genes share atleast one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "method = \"logistic_regression\"\n",
    "splits = 12\n",
    "kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "df[method] = pd.Series()\n",
    "for train,test in kf.split(df):\n",
    "    lr_model = train_logistic_regression_model(df=df.iloc[train], predictor_columns=methods, target_column=\"common\")\n",
    "    df[method].iloc[test] = apply_logistic_regression_model(df=df.iloc[test], predictor_columns=methods, model=lr_model)\n",
    "df[method] = 1-df[method]\n",
    "methods.append(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively create models for combining output values from multiple semantic similarity methods.\n",
    "method = \"random_forest\"\n",
    "splits = 2\n",
    "kf = KFold(n_splits=splits, random_state=14271, shuffle=True)\n",
    "df[method] = pd.Series()\n",
    "for train,test in kf.split(df):\n",
    "    rf_model = train_random_forest_model(df=df.iloc[train], predictor_columns=methods, target_column=\"common\")\n",
    "    df[method].iloc[test] = apply_random_forest_model(df=df.iloc[test],predictor_columns=methods, model=rf_model)\n",
    "df[method] = 1-df[method]\n",
    "methods.append(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Do the edges joining genes that share atleast one pathways come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation betwene the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "ppi_pos_dict = {name:(df[df[\"common\"] > 0.00][name].values) for name in methods}\n",
    "ppi_neg_dict = {name:(df[df[\"common\"] == 0.00][name].values) for name in methods}\n",
    "for name in methods:\n",
    "    stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "    pos_mean = np.average(ppi_pos_dict[name])\n",
    "    neg_mean = np.average(ppi_neg_dict[name])\n",
    "    pos_n = len(ppi_pos_dict[name])\n",
    "    neg_n = len(ppi_neg_dict[name])\n",
    "    TABLE[name].update({(TAG,\"mean_1\"):pos_mean, (TAG,\"mean_0\"):neg_mean, (TAG,\"n_1\"):pos_n, (TAG,\"n_0\"):neg_n})\n",
    "    TABLE[name].update({(TAG,\"ks\"):stat, (TAG,\"ks_pval\"):p})\n",
    "\n",
    "# Show the kernel estimates for each distribution of weights for each method.\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for name,ax in zip(methods,axs.flatten()):\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"value\")\n",
    "    ax.set_ylabel(\"density\")\n",
    "    sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "    sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"kernel_density.png\"), dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Ranking each biochemical pathways by its phenotypic visibility\n",
    "The purpose of this method is to look at which of the biochemical pathways (or whatever other arbitrary groupings) are the most easily recaptured using analysis of the text descriptions corresponding to the genes present in those biochemical pathways. This is done here by finding the average distance score for all the pairs of genes within each particular pathway or group for each method, then converting these values to ranks for ranking the pathways within a given method, then averaging the ranks across all methods. This is not necessarily extremely meaningful, if the pathway was involved in many phenotypes the pairwise similarity would not necessarily be very high, although the number of similar pairs might be higher than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the average within-pathway phenotype similarity values for each method for each particular pathway.\n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "group_ids = list(group_id_to_ids.keys())\n",
    "graph = IndexedGraph(df)\n",
    "within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "all_weights_dict = {}\n",
    "for method in methods:\n",
    "    all_weights_dict[method] = df[method].values\n",
    "    for group in group_ids:\n",
    "        within_ids = group_id_to_ids[group]\n",
    "        within_pairs = [(i,j) for i,j in itertools.permutations(within_ids,2)]\n",
    "        within_weights_dict[method][group] = np.mean((graph.get_values(within_pairs, kind=method)))\n",
    "heatmap_data = pd.DataFrame(within_weights_dict)\n",
    "heatmap_data = heatmap_data.dropna(axis=0, inplace=False)\n",
    "heatmap_data = heatmap_data.round(4).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the average rank across methods of each pathway in terms of how low the mean within-group distance values were.\n",
    "ranks = pd.DataFrame(within_weights_dict).rank()\n",
    "ranks[\"average\"] = ranks.mean(axis=1)\n",
    "ranks.sort_values(by=\"average\", inplace=True)\n",
    "ranks.reset_index(inplace=True)\n",
    "ranks[\"group_id\"] = ranks[\"index\"]\n",
    "ranks[\"full_name\"] = ranks[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "ranks[\"n\"] = ranks[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "ranks = ranks[[\"group_id\", \"n\", \"average\",\"full_name\",]]\n",
    "ranks.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Predicting whether two genes belong to a common biochemical pathway\n",
    "The purpose of this section is to see if whether or not two genes share atleast one common pathway can be predicted from the similarity scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_dict = {name:df[\"common\"] for name in methods}\n",
    "y_prob_dict = {name:(1 - df[name].values) for name in methods}\n",
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true, y_prob = y_true_dict[method], y_prob_dict[method]\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"auc\"):area, (TAG,\"baseline\"):baseline, (TAG,\"ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_shared.png\"), dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Are genes in the same biochemical pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "# graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "# i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "# in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "# connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "df = make_undirected(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "# each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "# recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "k = 10      # The threshold of interest for gene ranks.\n",
    "n = 1000    # Number of Monte Carlo simulation iterations to complete.\n",
    "df[list(methods)] = df.groupby(\"from\")[list(methods)].rank()\n",
    "ys = df[df[\"common\"]==1][list(methods)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "ymax = sum(df.groupby(\"from\")[\"common\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "# Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "# edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "ysims = [sum(df.groupby(\"from\")[\"common\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "for method in methods:\n",
    "    pvalue = len([ysim for ysim in ysims if ysim>=ys[method]])/float(n)\n",
    "    TABLE[method][(TAG,\"y\")] = ys[method]\n",
    "    TABLE[method][(TAG,\"y_max\")] = ymax\n",
    "    TABLE[method][(TAG,\"y_pval\")] = pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Predicting biochemical pathway membership based on representative mean vectors.\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes asociated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "methods = list(vector_dicts.keys())\n",
    "metric_dict = {t[0]:(spatial.distance.jaccard if t[2][\"metric\"]==\"jaccard\" else spatial.distance.cosine) for t in name_function_args_tuples}\n",
    "        \n",
    "    \n",
    "group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for method in methods:\n",
    "    for group in valid_group_ids:\n",
    "        ids = group_id_to_ids[group]\n",
    "        for identifier in valid_ids:\n",
    "            # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "            vectors = np.array([vector_dicts[method][some_id] for some_id in ids if not some_id==identifier])\n",
    "            mean_vector = vectors.mean(axis=0)\n",
    "            this_vector = vector_dicts[method][identifier]\n",
    "            pred_dict[method][identifier][group] = 1-metric_dict[method](mean_vector, this_vector)\n",
    "            true_dict[method][identifier][group] = (identifier in group_id_to_ids[group])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true = pd.DataFrame(true_dict[method]).as_matrix().flatten()\n",
    "    y_prob = pd.DataFrame(pred_dict[method]).as_matrix().flatten()\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"mean_auc\"):area, (TAG,\"mean_baseline\"):baseline, (TAG,\"mean_ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_mean_classifier.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Predicting biochemical pathway membership based on mean similarity values.\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representationt of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have atleast two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 KNN classification for groups or pathways\n",
    "Only interested in groups that contain more than one gene, and the genes that map to those groups. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but KNN could never predict this becuase when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be candidates for the K nearest neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "\n",
    "# Leave one out predictions.\n",
    "pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for method in methods:\n",
    "    for i in valid_ids:\n",
    "        ids = [identifier for identifier in valid_ids if identifier!=i]\n",
    "        # Make the labels just the ID's for that gene instead of pathways or groups, transform votes later.\n",
    "        X = [vector_dicts[method][identifier] for identifier in ids]\n",
    "        y = [identifier for identifier in ids]\n",
    "        neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "        neigh.fit(X,y)\n",
    "        probs = neigh.predict_proba([vector_dicts[method][i]])\n",
    "        # The classes are given in lexicographic order from the predict_proba method.\n",
    "        # 1. Create a 2D binary array with a row for each group and a column for each gene.\n",
    "        # 2. Multiply each row of that 2D binary array with the 1D array that has the probability for each gene.\n",
    "        # 3. Find the sum of each row to get the score specific to each group for this particular gene.\n",
    "        # 4. Append to the arrays for the true binary classifications and the predictions for each group. \n",
    "        ids.sort()\n",
    "        binary_arr = np.array([[(group_id in id_to_group_ids[x])*1 for x in ids] for group_id in valid_group_ids]) \n",
    "        group_probs_by_gene = probs*binary_arr\n",
    "        group_probs = np.sum(group_probs_by_gene, axis=1)\n",
    "        for group_index,group_id in enumerate(valid_group_ids):\n",
    "            pred_dict[method][i][group_id] = group_probs[group_index]\n",
    "            true_dict[method][i][group_id] = (i in group_id_to_ids[group_id])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "for method,ax in zip(methods, axs.flatten()):\n",
    "    \n",
    "    # Obtaining the values and metrics.\n",
    "    y_true = pd.DataFrame(true_dict[method]).as_matrix().flatten()\n",
    "    y_prob = pd.DataFrame(pred_dict[method]).as_matrix().flatten()\n",
    "    n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    baseline = Counter(y_true)[1]/len(y_true) \n",
    "    area = auc(recall, precision)\n",
    "    auc_to_baseline_auc_ratio = area/baseline\n",
    "    TABLE[method].update({(TAG,\"knn_auc\"):area, (TAG,\"knn_baseline\"):baseline, (TAG,\"knn_ratio\"):auc_to_baseline_auc_ratio})\n",
    "\n",
    "    # Producing the precision recall curve.\n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "    ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "    ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(method, baseline))\n",
    "    \n",
    "fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(os.path.join(OUTPUT_DIR,\"prcurve_knn_classifier.png\"), dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Summarizing the results for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(TABLE).transpose()\n",
    "results.to_csv(os.path.join(OUTPUT_DIR,\"full_table.csv\"))\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
