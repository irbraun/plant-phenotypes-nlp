{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Dataset\n",
    "The purpose of this notebook is to look closer at the dataset of genes, natural language descriptions, and ontology term annotations that are used in this work. As included in the preprocessing notebooks, these data are drawn from files from either publications supplements like Oellrich, Walls et al. (2015) or model species databases such as TAIR, MaizeGDB, and SGN. The datasets are already loaded and merged using classe available through the oats package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, merge_list_dicts, flatten, to_hms\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>628658</td>\n",
       "      <td>9110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>156</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>342</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>772</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>786</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>6526</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>637240</td>\n",
       "      <td>11015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_genes  unique_descriptions\n",
       "0     ath     628658                 9110\n",
       "1     gmx        156                   49\n",
       "2     mtr        342                  155\n",
       "3     osa        772                  389\n",
       "4     sly        786                  314\n",
       "5     zma       6526                  998\n",
       "6   total     637240                11015"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_pickle(\"../data/pickles/gene_phenotype_dataset_all_text_and_annotations_unmerged.pickle\")\n",
    "data.to_pandas().head()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6364</td>\n",
       "      <td>3813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1406</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7999</td>\n",
       "      <td>4839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_genes  unique_descriptions\n",
       "0     ath       6364                 3813\n",
       "1     gmx         30                   24\n",
       "2     mtr         37                   36\n",
       "3     osa         92                   85\n",
       "4     sly         70                   70\n",
       "5     zma       1406                  811\n",
       "6   total       7999                 4839"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_pickle(\"../data/pickles/gene_phenotype_dataset_all_text_and_annotations.pickle\")\n",
    "data.filter_has_description()\n",
    "data.to_pandas().head()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's there for each species?\n",
    "The previously loaded dataset contains all of the genes that across six plant species that have natural language description data for phenotype(s) related to that gene. Each gene can have multiple descriptions annotated to it, which were combined or concatenated when the datasets from multiple sources were merged in creating the pickled datasets. Arabidopsis has the highest number of genes that satisfy this criteria, followed by maize, and then followed by the other four species which have a relatively low number of genes that satisfy this criteria, atleast given the sources used for this work. Note that the number of unique descriptions is lower than the number of genes in call cases, because multiple genes can have the same phenotype description associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6364</td>\n",
       "      <td>3813</td>\n",
       "      <td>264189</td>\n",
       "      <td>7085</td>\n",
       "      <td>5116</td>\n",
       "      <td>264189</td>\n",
       "      <td>6561</td>\n",
       "      <td>4864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1406</td>\n",
       "      <td>811</td>\n",
       "      <td>50029</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>50029</td>\n",
       "      <td>1722</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>3887</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3887</td>\n",
       "      <td>760</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>1810</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1810</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>2672</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2672</td>\n",
       "      <td>671</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>233</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>233</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7999</td>\n",
       "      <td>4839</td>\n",
       "      <td>322820</td>\n",
       "      <td>8043</td>\n",
       "      <td>5802</td>\n",
       "      <td>322820</td>\n",
       "      <td>7443</td>\n",
       "      <td>5703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_genes  unique_descriptions  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species\n",
       "0     ath       6364                 3813       264189          7085          5116        264189           6561                      4864\n",
       "5     zma       1406                  811        50029          1846          1317         50029           1722                       503\n",
       "3     osa         92                   85         3887           826           586          3887            760                        99\n",
       "4     sly         70                   70         1810           577           438          1810            552                        99\n",
       "2     mtr         37                   36         2672           718           516          2672            671                       126\n",
       "1     gmx         30                   24          233            81            68           233             78                        12\n",
       "6   total       7999                 4839       322820          8043          5802        322820           7443                      5703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatize_doc = lambda d: [wnl.lemmatize(x) for x in simple_preprocess(d)]\n",
    "\n",
    "\n",
    "dists = defaultdict(list)\n",
    "\n",
    "\n",
    "token_lists = {}\n",
    "stems_lists = {}\n",
    "lemma_lists = {}\n",
    "\n",
    "# For each individual species.\n",
    "for species in data.get_species():\n",
    "    df = data.to_pandas()\n",
    "    subset = df[df[\"species\"]==species]\n",
    "    descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"description\"].values]\n",
    "    descriptions_stemmed = [preprocess_string(d) for d in subset[\"description\"].values]\n",
    "    descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"description\"].values]\n",
    "    token_lists[species] = flatten(descriptions_not_stemmed)\n",
    "    stems_lists[species] = flatten(descriptions_stemmed)    \n",
    "    lemma_lists[species] = flatten(descriptions_lemmatized)\n",
    "    \n",
    "    # What about the distributions of words per gene and sentences per gene?\n",
    "    dists[\"species\"].extend([species]*subset.shape[0])\n",
    "    dists[\"num_words\"].extend([len(word_tokenize(x)) for x in subset[\"description\"].values])\n",
    "    dists[\"num_sents\"].extend([len(sent_tokenize(x)) for x in subset[\"description\"].values])\n",
    "    \n",
    "    # What about the number of ontology annotations?\n",
    "    po = len([t for t in subset[\"term_ids\"].values if \"PO\" in t])\n",
    "    go = len([t for t in subset[\"term_ids\"].values if \"GO\" in t])\n",
    "    \n",
    "# For the entire dataset including all of the species.\n",
    "df = data.to_pandas()\n",
    "subset = df\n",
    "descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"description\"].values]\n",
    "descriptions_stemmed = [preprocess_string(d) for d in subset[\"description\"].values]\n",
    "descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"description\"].values]\n",
    "token_lists[\"total\"] = flatten(descriptions_not_stemmed)\n",
    "stems_lists[\"total\"] = flatten(descriptions_stemmed)    \n",
    "lemma_lists[\"total\"] = flatten(descriptions_lemmatized)\n",
    "\n",
    "# What about lemmas that are uniquely used for a particular species?\n",
    "lemma_sets_unique_to_species = {}\n",
    "for species in data.get_species():\n",
    "    other_species = [s for s in data.get_species() if s != species]\n",
    "    lemmas_used_in_other_species = set(flatten([lemma_lists[s] for s in other_species]))\n",
    "    unique_lemmas = set(lemma_lists[species]).difference(lemmas_used_in_other_species)\n",
    "    lemma_sets_unique_to_species[species] = unique_lemmas\n",
    "lemma_sets_unique_to_species[\"total\"] = flatten([list(s) for s in lemma_sets_unique_to_species.values()])\n",
    "\n",
    "    \n",
    "# Create a dataframe to contain the summarizing information about this dataset, and sort it by number of genes.\n",
    "df = data.describe() \n",
    "condition = (df.species==\"total\")\n",
    "excluded = df[condition]\n",
    "included = df[~condition]\n",
    "df_sorted = included.sort_values(by=\"num_genes\", ascending=False)\n",
    "df = pd.concat([df_sorted,excluded])\n",
    "\n",
    "# Add columns summarizing information about the text descriptions in the dataset.\n",
    "df[\"total_words\"] = df[\"species\"].map(lambda x: len(token_lists[x]))\n",
    "df[\"unique_words\"] = df[\"species\"].map(lambda x: len(set(token_lists[x])))\n",
    "df[\"unique_stems\"] = df[\"species\"].map(lambda x: len(set(stems_lists[x])))\n",
    "df[\"total_lemmas\"] = df[\"species\"].map(lambda x: len(lemma_lists[x]))\n",
    "df[\"unique_lemmas\"] = df[\"species\"].map(lambda x: len(set(lemma_lists[x])))\n",
    "df[\"unique_lemmas_to_species\"] = df[\"species\"].map(lambda x: len(lemma_sets_unique_to_species[x]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  num_words  num_sents\n",
       "0     ath          8          1\n",
       "1     ath          6          1\n",
       "2     ath          5          1\n",
       "3     ath         44          7\n",
       "4     ath         55          2\n",
       "5     ath          3          1\n",
       "6     ath         15          2\n",
       "7     ath         67          5\n",
       "8     ath         18          3\n",
       "9     ath         78         10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_distributions = pd.DataFrame(dists)\n",
    "text_distributions.to_csv(\"~/Desktop/b.csv\", index=False)\n",
    "text_distributions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the ontology term annotations for each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the genes in this dataset for each species are mapped to atleast one term from a given ontology?\n",
    "num_mapped_go = {}\n",
    "num_mapped_po = {}\n",
    "for species in data.get_species():\n",
    "    d = data.to_pandas()\n",
    "    subset = d[d[\"species\"]==species]    \n",
    "    num_mapped_po[species] = len([t for t in subset[\"term_ids\"].values if \"PO\" in t])\n",
    "    num_mapped_go[species] = len([t for t in subset[\"term_ids\"].values if \"GO\" in t])\n",
    "num_mapped_go[\"total\"] = sum(list(num_mapped_go.values()))   \n",
    "num_mapped_po[\"total\"] = sum(list(num_mapped_po.values()))\n",
    "df[\"go\"] = df[\"species\"].map(lambda x: num_mapped_go[x])\n",
    "df[\"po\"] = df[\"species\"].map(lambda x: num_mapped_po[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the biologically relevant groups like biochemical pathways and phenotypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the groupings that we're interested in mapping to?\n",
    "kegg_pathways_filename = \"../data/pickles/groupings_from_kegg_pathways.pickle\" \n",
    "pmn_pathways_filename = \"../data/pickles/groupings_from_pmn_pathways.pickle\"                        \n",
    "lloyd_subsets_filename = \"../data/pickles/groupings_from_lloyd_subsets.pickle\"                     \n",
    "groupings_dict = {\"kegg\":kegg_pathways_filename,\"plantcyc\":pmn_pathways_filename,\"lloyd\":lloyd_subsets_filename}\n",
    "\n",
    "for name,filename in groupings_dict.items():\n",
    "    groups = load_from_pickle(filename)\n",
    "    id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(data)\n",
    "    group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "    species_dict = data.get_species_dictionary()\n",
    "    num_mapped = {}\n",
    "    for species in data.get_species():\n",
    "        num_mapped[species] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "    num_mapped[\"total\"] = sum(list(num_mapped.values()))    \n",
    "    df[name] = df[\"species\"].map(lambda x: num_mapped[x])  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the other biologically relevant information like orthologous genes and protein interactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PantherDB for plant orthologs.\n",
    "ortholog_file_path = \"../data/orthology_related_files/pantherdb/PlantGenomeOrthologs_IRB_Modified.txt\"\n",
    "ortholog_edgelist = AnyInteractions(data.get_name_to_id_dictionary(), ortholog_file_path)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in ortholog_edgelist.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"panther\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRING DB for protein-protein interactions.\n",
    "naming_file = \"../data/group_related_files/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../data/group_related_files/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis thaliana\n",
    "    \"../data/group_related_files/string/4577.protein.links.detailed.v11.0.txt\", # maize\n",
    "    \"../data/group_related_files/string/4530.protein.links.detailed.v11.0.txt\", # tomato \n",
    "    \"../data/group_related_files/string/4081.protein.links.detailed.v11.0.txt\", # medicago\n",
    "    \"../data/group_related_files/string/3880.protein.links.detailed.v11.0.txt\", # rice \n",
    "    \"../data/group_related_files/string/3847.protein.links.detailed.v11.0.txt\", # soybean\n",
    "]\n",
    "genes = data.get_gene_dictionary()\n",
    "string_data = ProteinInteractions(genes, naming_file, *interaction_files)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in string_data.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"stringdb\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the vocabularies used for different species compare?\n",
    "One of the things we are interested in is discovering or recovering phenotype similarity between different species in order to identify phenologs (phenotypes between species that share some underlying genetic cause). For this reason, we are interested in how the vocabularies used to describe phenotypes between different species vary, because this will impact how feasible it is to use a dataset like this to identify phenologs. Because the Arabidopsis and maize datasets are the largest in this case, we will compare the vocabularies used in describing the phenotypes associated with the genes from these species in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "vocabs = {s:set(lemma_list) for s,lemma_list in lemma_lists.items()}\n",
    "fdist_zma = FreqDist(lemma_lists[\"zma\"])\n",
    "fdist_ath = FreqDist(lemma_lists[\"ath\"])\n",
    "\n",
    "#vocabs = {s:set(stems_list) for s,stems_list in stems_lists.items()}\n",
    "#fdist_zma = FreqDist(stems_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(stems_lists[\"ath\"])\n",
    "\n",
    "#vocabs = {s:set(token_list) for s,token_list in token_lists.items()}\n",
    "#fdist_zma = FreqDist(token_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(token_lists[\"ath\"])\n",
    "\n",
    "\n",
    "union_vocab = vocabs[\"zma\"].union(vocabs[\"ath\"])\n",
    "table = pd.DataFrame({\"token\":list(union_vocab)})\n",
    "stops = set(stopwords.words('english'))\n",
    "table = table[~table.token.isin(stops)]\n",
    "table[\"part_of_speech\"] = table[\"token\"].map(lambda x: nltk.pos_tag([x])[0][1][:2])\n",
    "table[\"ath_freq\"] = table[\"token\"].map(lambda x: fdist_ath[x])\n",
    "table[\"ath_rate\"] = table[\"ath_freq\"]*100/len(token_lists[\"ath\"])\n",
    "table[\"zma_freq\"] = table[\"token\"].map(lambda x: fdist_zma[x])\n",
    "table[\"zma_rate\"] = table[\"zma_freq\"]*100/len(token_lists[\"zma\"])\n",
    "table[\"diff\"] = table[\"ath_rate\"]-table[\"zma_rate\"]\n",
    "table.to_csv(\"~/Desktop/a.csv\")\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for Arabidopsis than maize?\n",
    "table.sort_values(by=\"diff\", ascending=False, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for maize than Arabidopsis?\n",
    "table.sort_values(by=\"diff\", ascending=True, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the mean absolute value of the rate differences different between the different parts of speech?\n",
    "table[\"abs_diff\"] = abs(table[\"diff\"])\n",
    "pos_table = table.groupby(\"part_of_speech\").mean()\n",
    "pos_table.sort_values(by=\"abs_diff\", inplace=True, ascending=False)\n",
    "pos_table = pos_table[[\"abs_diff\"]]\n",
    "pos_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(table.shape)\n",
    "zma_only = table[table[\"ath_rate\"]==0]\n",
    "ath_only = table[table[\"zma_rate\"]==0]\n",
    "print(zma_only.shape)\n",
    "print(ath_only.shape)\n",
    "print(ath_only.shape[0]+zma_only.shape[0])\n",
    "ath_only.head(10)\n",
    "# We need to create a mapping between stems and the words that were present for them.\n",
    "# This is because what we want is the stems that are exclusive to a species.\n",
    "# but then the words that are actually there for those stems, so that we can count their parts of speech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
