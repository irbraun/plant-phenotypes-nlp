{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can machine learning approaches learn relationships between concepts that are in ontologies?\n",
    "If the neural network document encoding models (Doc2Vec) are being successfully trained, then they should be able to recapture some of the domain-specific information that is written into relationships present in biological ontologies. Specifically, two concepts which have a parent-child relationship in PATO or PO can be considered to be highly similar in this context. We compare the distances between the labels for these pairs of terms as inferred by both the general Doc2Vec model trained on the English Wikipedia corpus, as well as our own models trained specifically on abstracts from PubMed that are specific to plant phenotypes. Here we generate figures to compare the results for a specific set of handpicked phrase or term pairs, as well as a second figure over all pairs parsed from the hierarchies in each ontology to check whether the result generalizes to the ontologies as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Convert the distance values in the dataframe to be percentiles.\n",
    "df_pct = df.copy()\n",
    "df_pct[methods] = df[methods].rank(pct=True)\n",
    "\n",
    "# Looking through the distances that are low for PubMed and high for Wikipedia to see if this is a valuable approach.\n",
    "of_interest = df_pct[(df_pct[\"doc2vec_pubmed\"]<0.1) & (df_pct[\"doc2vec_wiki\"]>0.3)]\n",
    "for row in of_interest.itertuples():\n",
    "    print(\"{} and {}\".format(round(row[4],4),round(row[3],4)))\n",
    "    sentence1 = descriptions[row[1]]\n",
    "    sentence2 = descriptions[row[2]]\n",
    "    print(\"1: {}\\n2: {}\\n\\n\".format(sentence1, sentence2))\n",
    "    \n",
    "# Looking at the distance values of sentence variations of interest found in the previous step. \n",
    "sentences = {\n",
    "    0:\"Susceptible to bacterial infection\",\n",
    "    1:\"Resistant to bacterial infection\",\n",
    "    2:\"Resistant to powdery mildew\",\n",
    "    3:\"susceptible to powdery mildew\",\n",
    "    4:\"Some random control sentence\"\n",
    "}\n",
    "wikipedia_results = pw.pairwise_doc2vec_onegroup(doc2vec_wiki_model,sentences,\"cosine\").edgelist\n",
    "wikipedia_results[\"value\"] = wikipedia_results[\"value\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_wiki\"].values, x, kind=\"rank\")/100)\n",
    "wikipedia_results = pw.remove_self_loops(wikipedia_results)\n",
    "pubmed_results = pw.pairwise_doc2vec_onegroup(doc2vec_pubmed_model,sentences,\"cosine\").edgelist\n",
    "pubmed_results[\"value\"] = pubmed_results[\"value\"].map(lambda x: stats.percentileofscore(df[\"doc2vec_pubmed\"].values, x, kind=\"rank\")/100)\n",
    "pubmed_results = pw.remove_self_loops(pubmed_results)\n",
    "results = pw.merge_edgelists({\"wikipedia\":wikipedia_results,\"pubmed\":pubmed_results})\n",
    "results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "# Loading a file of handpicked phrase pairs from ontology term labels. \n",
    "pairs = pd.read_csv(\"../data/corpus_related_files/ontology_knowledge/phrase_pairs.csv\")\n",
    "# Adding the distance values found with each method to the dataframe.\n",
    "# Another valid background distribution for each method could be the distance between all pairs of term labels.\n",
    "pairs[\"Wikipedia\"] = pw.elemwise_doc2vec_twogroup(doc2vec_wiki_model, pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, cosine)\n",
    "pairs[\"PubMed\"] = pw.elemwise_doc2vec_twogroup(doc2vec_pubmed_model, pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, cosine)\n",
    "pairs[\"Jaccard\"] = pw.elemwise_ngrams_twogroup(pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, jaccard)\n",
    "pairs[\"Wikipedia\"] = pairs[\"Wikipedia\"].map(lambda x: stats.percentileofscore(df[\"Doc2Vec Wikipedia:Size=300\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"PubMed\"] = pairs[\"PubMed\"].map(lambda x: stats.percentileofscore(df[\"Doc2Vec PubMed:Size=100\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"Pair\"] = pairs[\"Label 1\"].values+\",\"+pairs[\"Label 2\"].values\n",
    "pairs.to_csv(\"../data/scratch/phrase_pair_handpicked_results.csv\",index=False)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronto\n",
    "\n",
    "# Define the ontologies to be used for this section, using the pronto library to read them.\n",
    "ontologies = {\"PATO\":pronto.Ontology(\"../ontologies/pato.obo\"), \"PO\":pronto.Ontology(\"../ontologies/po.obo\")}\n",
    "tuples = []\n",
    "\n",
    "# Iterate through the ontologies and all parent/child and sibling term label pairs.\n",
    "for ont_name,ont in ontologies.items():\n",
    "    delim = \"[DELIM]\"\n",
    "    sibling_pairs = set()\n",
    "    for term in ont:\n",
    "        for parent in term.parents.id:\n",
    "            tuples.append((ont_name,\"parent_child\",term.name,ont[parent].name))     \n",
    "        sorted_id_pairs = [sorted(pair) for pair in list(itertools.combinations(term.children.id, 2))]\n",
    "        sorted_pairs = [\"{}{}{}\".format(ont[pair[0]].name, delim, ont[pair[1]].name) for pair in sorted_id_pairs]\n",
    "        sibling_pairs.update(sorted_pairs)\n",
    "    for pair in list(sibling_pairs):\n",
    "        pair = pair.split(delim)\n",
    "        tuples.append((ont_name,\"sibling\",pair[0],pair[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using that dataframe to see how the generalized distance percentile distributions compare between models.\n",
    "pairs = pd.DataFrame(tuples, columns=[\"Ontology\", \"Relationship\", \"Label 1\", \"Label 2\"])\n",
    "\n",
    "# Adding the distance values found with each method to the dataframe.\n",
    "# Another valid background distribution for each method could be the distance between all pairs of term labels.\n",
    "pairs[\"Wikipedia\"] = pw.elemwise_doc2vec_twogroup(doc2vec_wiki_model, pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, cosine)\n",
    "pairs[\"PubMed\"] = pw.elemwise_doc2vec_twogroup(doc2vec_pubmed_model, pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, cosine)\n",
    "pairs[\"Jaccard\"] = pw.elemwise_ngrams_twogroup(pairs[\"Label 1\"].values, pairs[\"Label 2\"].values, jaccard)\n",
    "pairs[\"Wikipedia\"] = pairs[\"Wikipedia\"].map(lambda x: stats.percentileofscore(df[\"Wikipedia\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"PubMed\"] = pairs[\"PubMed\"].map(lambda x: stats.percentileofscore(df[\"PubMed\"].values, x, kind=\"rank\")/100)\n",
    "pairs[\"Pair\"] = pairs[\"Label 1\"].values+\",\"+pairs[\"Label 2\"].values\n",
    "pairs.to_csv(\"../data/scratch/phrase_pair_handpicked_results.csv\",index=False)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use this dataset to identify sentences from abstracts that contain phenotypic information?\n",
    "The question we want to answer is whether or not these machine learning approaches in combination with the gathered dataset of phenotypic descriptions provides a valuable method for identifying which sentences in abstracts are likely to contain to information related to phenotyping, as this approach could be valuable in a curation pipeline. We will use a partially hands on approach, only evaluating the predicted matches rather than creating a full dataset of annotated abstracts. This means that the result will have to be evaluated as quantifying how many of the return sentences are primarily about a phenotype, partially about a phenotype, or not about a phenotype (three different categories), we cannot actually get an F1 score for this approach because we will not know how many were missed or available as positives in the dataset from which the sentences were drawn. Types of classes could be:\n",
    "1. Specifically mentioning a phenotype (e.g. \"*Plants treated with chemical X exhibited phenotype Y*.\")\n",
    "2. Only generally discussing phenotypes as a topic (e.g. \"*We organized a dataset of Z phenotypes from Arabidopsis thaliana studies*.\")\n",
    "3. Not talking about phenotypes (e.g. \"*Arabidopsis thaliana is one of the most widely studied plant species.*\")\n",
    "\n",
    "Note that the false positives for this analysis likely include sentences where phenotypes are specifically mentioned, but not in the context of observing those phenotypes in a particular plant, which is what we are interested in here because those are the type of descriptions that should go in a database or dataset or something. This should be taken into account when evaluating the scores, and further researcher for distinguishing between these types of descriptions should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dataset.get_description_dictionary()\n",
    "\n",
    "\n",
    "# Reading in the tagged dataset file of sentence that do or do not describe phenotypes.\n",
    "tagged_dataset = pd.read_csv(\"../data/corpus_related_files/brat_annotations_zma_corpus/untagged_dataset.csv\")\n",
    "tagged_dataset = tagged_dataset[(tagged_dataset[\"tag\"]==0) | (tagged_dataset[\"tag\"]==1)]\n",
    "\n",
    "#print(tagged_dataset)\n",
    "\n",
    "sentence_dict = {i:sentence for i,sentence in enumerate(tagged_dataset[\"sentence\"].values)}\n",
    "tags_dict = {i:tag for i,tag in enumerate(tagged_dataset[\"tag\"].values)}\n",
    "wikipedia_results = pw.pairwise_doc2vec_twogroup(doc2vec_wiki_model,sentence_dict,descriptions,\"cosine\").edgelist\n",
    "#wikipedia_results = pw.pairwise_ngrams_twogroup(sentence_dict,descriptions,\"cosine\").edgelist\n",
    "\n",
    "\n",
    "# Evaluting each sentence either by their mean distance to the phenotypes or their minimum distance.\n",
    "results = pd.DataFrame(wikipedia_results.groupby([\"from\"])[\"value\"].min())\n",
    "results = results.reset_index(inplace=False)\n",
    "results = results.sort_values(by=[\"value\"])\n",
    "results[\"sentence\"] = results[\"from\"].map(lambda x: sentence_dict[x])\n",
    "\n",
    "results.sort_values(by=[\"value\"], inplace=True, ascending=True)\n",
    "\n",
    "print(results.head())\n",
    "\n",
    "\n",
    "# Generating the lists of true values and predictions and metrics.\n",
    "y_true = [tags_dict[i] for i in results[\"from\"].values]\n",
    "y_prob = [1.000-v for v in results[\"value\"].values]\n",
    "n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "baseline = Counter(y_true)[1]/len(y_true) \n",
    "area = auc(recall, precision)\n",
    "auc_to_baseline_auc_ratio = area/baseline\n",
    "print(area)\n",
    "print(baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(200).to_csv(\"~/Desktop/a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum Fß score for different values of ß.  \n",
    "f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "print(np.nanmax(f_beta(precision,recall,1)))\n",
    "print(np.nanmax(f_beta(precision,recall,0.5)))\n",
    "print(np.nanmax(f_beta(precision,recall,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
