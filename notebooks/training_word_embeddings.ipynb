{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.utils.utils import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating datasets of sentences to traing word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths to text datasets.\n",
    "plant_abstracts_corpus_path = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\"\n",
    "plant_phenotype_descriptions_path = \"../../plant-data/genes_texts_annots.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset that combines the dataset of plant phenotype descriptions and scrapped abstracts.\n",
    "corpus = open(plant_abstracts_corpus_path, 'r').read()\n",
    "sentences_from_corpus = sent_tokenize(corpus)\n",
    "phenotype_descriptions = \" \".join(pd.read_csv(plant_phenotype_descriptions_path)[\"descriptions\"].values)\n",
    "times_to_duplicate_phenotype_dataset = 5\n",
    "sentences_from_descriptions = sent_tokenize(phenotype_descriptions)\n",
    "sentences_from_descriptions_duplicated = list(np.repeat(sentences_from_descriptions, times_to_duplicate_phenotype_dataset))\n",
    "sentences_from_corpus_and_descriptions = sentences_from_corpus+sentences_from_descriptions_duplicated\n",
    "random.shuffle(sentences_from_corpus_and_descriptions)\n",
    "random.shuffle(sentences_from_corpus)\n",
    "random.shuffle(sentences_from_descriptions)\n",
    "sentences_from_corpus_and_descriptions = [preprocess_string(s) for s in sentences_from_corpus_and_descriptions]\n",
    "sentences_from_corpus = [preprocess_string(s) for s in sentences_from_corpus]\n",
    "sentences_from_descriptions = [preprocess_string(s) for s in sentences_from_descriptions]\n",
    "assert len(sentences_from_corpus_and_descriptions) == len(sentences_from_corpus)+(times_to_duplicate_phenotype_dataset*len(sentences_from_descriptions))\n",
    "print(len(sentences_from_corpus_and_descriptions))\n",
    "print(len(sentences_from_corpus))\n",
    "print(len(sentences_from_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and saving models with hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining grid search parameters for training word embedding models.\n",
    "training_sentence_sets =  [(sentences_from_corpus_and_descriptions,\"both\"), (sentences_from_corpus,\"abstracts\"), (sentences_from_descriptions,\"dataset\")]\n",
    "training_sentence_sets =  [(sentences_from_corpus_and_descriptions,\"both\")]\n",
    "dimensions = [(x,\"dim{}\".format(str(x))) for x in [50, 100, 150, 200]]\n",
    "num_epochs = [(500,\"500\")]\n",
    "min_alpha = [(0.0001,\"a\")]\n",
    "alpha = [(0.025,\"s\")]\n",
    "min_count = [(x,\"min{}\".format(str(x))) for x in [3,5]]\n",
    "window = [(x,\"window{}\".format(str(x))) for x in [5,8]]\n",
    "hyperparameter_sets = list(product(\n",
    "    training_sentence_sets, \n",
    "    dimensions, \n",
    "    num_epochs, \n",
    "    min_alpha, \n",
    "    alpha, \n",
    "    min_count, \n",
    "    window))\n",
    "print(len(hyperparameter_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "        self.deltas = []\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 1:\n",
    "            delta = loss\n",
    "        else:\n",
    "            delta = loss- self.loss_previous_step\n",
    "        self.loss_previous_step=loss\n",
    "        self.losses.append(loss)\n",
    "        self.epochs.append(self.epoch)\n",
    "        self.epoch += 1\n",
    "        self.deltas.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_word_embedding_model(output_dir, hyperparameters):\n",
    "    \n",
    "    # Producing the path to the output file named according to hyperparameters.\n",
    "    model_filename = \"word2vec_{}.model\".format(\"_\".join([x[1] for x in hyperparameters]))\n",
    "    output_path = os.path.join(output_dir, model_filename)\n",
    "    \n",
    "    # Get the hyperparameter values.\n",
    "    sents, dim, epochs, min_a, a, min_count, window = [x[0] for x in hyperparameters]                      \n",
    "    \n",
    "    # Training the word2vec neural network with the current set of hyperparameters. \n",
    "    model = gensim.models.Word2Vec(sg=1, min_count=min_count, window=window, size=dim, workers=4, alpha=a, min_alpha=min_a)\n",
    "    model.build_vocab(sents)\n",
    "    loss_logger = LossLogger()\n",
    "    model.train(sents, epochs=epochs, total_examples=model.corpus_count, compute_loss=True, callbacks=[loss_logger])\n",
    "               \n",
    "    # Saving the model to a file.\n",
    "    model.save(output_path)\n",
    "    print(\"saving {}\".format(model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the model creation function iteratively through the hyperparameter grid search.\n",
    "output_models_directory = \"../models/plants_sg\"\n",
    "for h in hyperparameter_sets:\n",
    "    train_and_save_word_embedding_model(output_models_directory, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using ontologies to generate datasets of closely related domain concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_validation_df_from_ontology(path):\n",
    "    \n",
    "    # Load the ontology and term information.\n",
    "    path = \"../ontologies/po.obo\"\n",
    "    ont = Ontology(path)\n",
    "    term_ids_and_names = [(t.id,t.name) for t in ont.terms() if \"obsolete\" not in t.name]\n",
    "    key_to_annotations = {i:[x[0]] for i,x in enumerate(term_ids_and_names)}\n",
    "    key_to_term_id = {i:x[0] for i,x in enumerate(term_ids_and_names)}\n",
    "    key_to_text_string = {i:x[1] for i,x in enumerate(term_ids_and_names)}\n",
    "    key_to_preprocessed_text_string = {i:\" \".join(preprocess_string(s)) for i,s in key_to_text_string.items()}\n",
    "    \n",
    "    # Get mappings that define which terms are very close to which others ones in the ontology structure.\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    for term in ont.terms():\n",
    "        parents[term.id] = [t.id for t in term.superclasses(with_self=False, distance=1)]\n",
    "        children[term.id] = [t.id for t in term.subclasses(with_self=False, distance=1)]\n",
    "    siblings = {}\n",
    "    for term in ont.terms():\n",
    "        siblings[term.id] = flatten([[t for t in children[parent_id] if t!=term.id] for parent_id in parents[term.id]])\n",
    "    assert len(parents) == len(children)\n",
    "    assert len(parents) == len(siblings)\n",
    "    any_close = {}\n",
    "    for key in parents.keys():\n",
    "        any_close[key] = flatten([parents[key],children[key],siblings[key]])\n",
    "        \n",
    "        \n",
    "    df = pw.with_annotations(key_to_annotations, ont, \"jaccard\", tfidf=False).edgelist\n",
    "    df = df[df[\"from\"]!=df[\"to\"]]\n",
    "    df[\"from_id\"] = df[\"from\"].map(lambda x: key_to_term_id[x])\n",
    "    df[\"to_id\"] = df[\"to\"].map(lambda x: key_to_term_id[x])\n",
    "    df[\"from_text\"] = df[\"from\"].map(lambda x: key_to_text_string[x])\n",
    "    df[\"to_text\"] = df[\"to\"].map(lambda x: key_to_text_string[x])\n",
    "    df[\"close\"] = df.apply(lambda x: x[\"to_id\"] in any_close[x[\"from_id\"]], axis=1)\n",
    "    df[\"token_overlap\"] = df.apply(lambda x: len(set(x[\"from_text\"].split()).intersection(set(x[\"to_text\"].split())))>0, axis=1)\n",
    "    df.head(20)\n",
    "    \n",
    "    positive_df = df[(df[\"token_overlap\"]==False) & (df[\"close\"]==True)]\n",
    "    negative_df = df[(df[\"token_overlap\"]==False) & (df[\"close\"]==False)]\n",
    "    assert negative_df.shape[0]+positive_df.shape[0] == df[df[\"token_overlap\"]==False].shape[0]\n",
    "    num_positive_examples = positive_df.shape[0]\n",
    "    validation_df = pd.concat([positive_df, negative_df.sample(num_positive_examples, random_state=2)])\n",
    "    del df\n",
    "    return(validation_df, key_to_preprocessed_text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pato_validation_df, pato_key_to_preprocessed_text_string = build_validation_df_from_ontology(\"../ontologies/pato.obo\")\n",
    "po_validation_df, po_key_to_preprocessed_text_string = build_validation_df_from_ontology(\"../ontologies/po.obo\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"auxin\",\"leaves\",\"dwarfism\",\"roots\",\"tip\",\"anatomy\",\"abnormal\",\"hair\",\"late\",\"flowering\"]\n",
    "test_words = [preprocess_string(word)[0] for word in test_words]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Validation dataset of paraphrased phenotype sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paraphrase_output_path = \"/Users/irbraun/phenologs-with-oats/data/corpus_related_files/paraphrasing/from_corpus.txt\"\n",
    "f = Path(paraphrase_output_path)\n",
    "if not f.exists():\n",
    "    corpus = open(plant_abstracts_corpus_path, 'r').read()\n",
    "    sentences_from_corpus = sent_tokenize(corpus)\n",
    "    random.shuffle(sentences_from_corpus)\n",
    "    sentences_from_corpus = [s for s in sentences_from_corpus if len(s.split())<=20]\n",
    "    with open(paraphrase_output_path, \"w\") as f:\n",
    "        for s in sentences_from_corpus[:100]:\n",
    "            f.write(s+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating word embedding models on the validation dataset of related concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"../models/plants_sg\"\n",
    "output_path_for_results = \"../models/plants_sg/{}_validation.csv\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "rows = []\n",
    "header = [\"model\",\"pato\",\"po\"]\n",
    "header.extend([\"word\"]*len(test_words))\n",
    "for path in glob.glob(os.path.join(models_dir,\"*.model\")):\n",
    "    \n",
    "    print(\"validating model at {}\".format(path))\n",
    "    model = gensim.models.Word2Vec.load(path)\n",
    "    model_name = os.path.basename(path)\n",
    "    \n",
    "    \n",
    "    if model_name == \"word2vec.model\":\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    row_items = []\n",
    "    row_items.append(model_name)\n",
    "    \n",
    "    # Use the pairwise interface to actually get a set of document vectors and calculate maximum F_1 values.\n",
    "    validation_df = pato_validation_df\n",
    "    result = pw.with_word2vec(model, pato_key_to_preprocessed_text_string, \"cosine\", \"mean\")\n",
    "    validation_df[path] = validation_df.apply(lambda x: result.array[result.id_to_index[x[\"from\"]],result.id_to_index[x[\"to\"]]], axis=1)\n",
    "    \n",
    "    y_true = list(validation_df[\"close\"].values*1)\n",
    "    y_prob = list(1-validation_df[path].values)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f_1_scores = f_beta(precision,recall,beta=1)\n",
    "    f_1_max = np.nanmax(f_1_scores)\n",
    "    row_items.append(f_1_max)\n",
    "    \n",
    "    \n",
    "    # Use the pairwise interface to actually get a set of document vectors and calculate maximum F_1 values.\n",
    "    validation_df = po_validation_df\n",
    "    result = pw.with_word2vec(model, po_key_to_preprocessed_text_string, \"cosine\", \"mean\")\n",
    "    validation_df[path] = validation_df.apply(lambda x: result.array[result.id_to_index[x[\"from\"]],result.id_to_index[x[\"to\"]]], axis=1)\n",
    "    \n",
    "    y_true = list(validation_df[\"close\"].values*1)\n",
    "    y_prob = list(1-validation_df[path].values)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f_1_scores = f_beta(precision,recall,beta=1)\n",
    "    f_1_max = np.nanmax(f_1_scores)\n",
    "    row_items.append(f_1_max)\n",
    "    \n",
    "    \n",
    "    # For the word similarity test.\n",
    "    row_items.extend([\"{}: {}\".format(w, \"; \".join([x[0] for x in model.most_similar(w,topn=10)])) for w in test_words])\n",
    "    \n",
    "    # Adding these results to a list to build a dataframe from.\n",
    "    rows.append(tuple(row_items))\n",
    "    \n",
    "    \n",
    "    \n",
    "# Constructing and saving the results dataframe.\n",
    "pd.DataFrame(rows, columns=header).to_csv(output_path_for_results, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
